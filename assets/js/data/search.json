[
  
  {
    "title": "HashiCorp | Vault Multiple Choice Training Exam",
    "url": "/posts/Vault-Multiple-Choice-Training-Exam/",
    "categories": "HashiCorp, Vault",
    "tags": "hashicorp, vault, exam, certification",
    "date": "2024-03-18 00:00:00 +0100",
    





    
    "snippet": "The Vault Associate 002 exam contains 10 sections. Below, you’ll find some recurring questions that can help you prepare for the exam.Multiple Choice ExamModule 1: Compare Authentication Methods  W...",
    "content": "The Vault Associate 002 exam contains 10 sections. Below, you’ll find some recurring questions that can help you prepare for the exam.Multiple Choice ExamModule 1: Compare Authentication Methods  What is a primary purpose of authentication methods in Vault?          To manage secrets      To revoke leases      To encrypt data      To verify and authorize users        Which authentication method in Vault utilizes AWS IAM roles?          LDAP      Token      AWS IAM      TLS Certificate        In Vault, which authentication method is commonly used for machine-to-machine communication?          Username/Password      Token      AWS IAM      LDAP        What authentication method in Vault requires a signed X.509 certificate from a trusted CA?          LDAP      Token      AWS IAM      TLS Certificate        Which authentication method in Vault provides short-lived tokens for temporary access?          Username/Password      AWS IAM      Token      LDAP        What authentication method in Vault is suitable for human users?          Username/Password      Token      AWS IAM      TLS Certificate        Which authentication method in Vault uses client-side TLS certificates?          Username/Password      Token      AWS IAM      TLS Certificate        Which authentication method in Vault is primarily used for Kubernetes environments?          Token      Username/Password      AWS IAM      Kubernetes        In Vault, what is the purpose of LDAP authentication?          To authenticate using AWS IAM roles      To generate temporary access tokens      To authenticate machines      To integrate with existing LDAP directories        Which authentication method in Vault is recommended for automated processes?          Username/Password      Token      AWS IAM      LDAP      Module 2: Create Vault Policies  What is the primary function of a Vault policy?          To authenticate users      To define access control rules      To manage leases      To generate tokens        How are policies associated with users or tokens in Vault?          By assigning roles      By attaching policies directly      By defining ACLs      By using LDAP groups        What language is commonly used to define Vault policies?          JSON      HCL (HashiCorp Configuration Language)      YAML      XML        What does a Vault policy specify?          Authentication methods      Permissions on paths and operations      TLS certificates      Lease durations        Which Vault CLI command is used to create a new policy?          vault authenticate      vault policy write      vault token create      vault lease create        How are policies evaluated in Vault?          Deny-by-default      Allow-by-default      Deny-never      Allow-never        What happens if a user or token is associated with multiple policies in Vault?          Only the most permissive policy is applied      Only the least permissive policy is applied      Policies are merged, granting the union of permissions      Policies are ignored        In Vault, what is the purpose of a wildcard (*) in a policy path?          To deny all access to the path      To specify a particular operation      To restrict access to certain users      To match any subpath under the specified path        Which of the following is a valid Vault policy statement?          path “/secrets/*” { deny = [“read”] }      path “secret/data/*” { capabilities = [“create”, “read”, “update”] }      path “secrets/” { capabilities = [“read”, “write”] }      path “/secret/” { allow = [“”] }        What is the default behavior of Vault if a user or token has no associated policy?          Access is granted with full permissions      Access is denied      Access is granted with read-only permissions      Access is denied by default      Module 3: Assess Vault Tokens  What is a Vault token?          A password for accessing secrets      An authentication mechanism      An AWS IAM role      A TLS certificate        How are tokens created in Vault?          Manually by administrators      Automatically during authentication      Through LDAP integration      Via AWS IAM roles        Which token type in Vault is meant for long-term use and can be renewed?          Batch Tokens      Service Tokens      Periodic Tokens      Batch-Orphan Tokens        What happens when a Vault token expires?          It is automatically renewed      It becomes invalid and must be recreated      It becomes orphaned      It is revoked        How can token renewals be managed in Vault?          Tokens cannot be renewed      Through AWS IAM integration      Using the vault token renew command      By updating policies        What is the purpose of token leasing in Vault?          To increase the security of tokens      To automatically revoke tokens after a specified period      To limit the number of tokens per user      To rotate encryption keys        Which of the following token types in Vault is not renewable?          Periodic Tokens      Batch Tokens      Service Tokens      Batch-Orphan Tokens        What is the difference between periodic tokens and service tokens?          They are the same, just named differently      Periodic tokens are manually created by administrators      Periodic tokens can be renewed, while service tokens cannot      Service tokens have shorter lease durations        How can token revocation be initiated in Vault?          Automatically upon expiration      By administrators only      By users themselves      Through AWS IAM roles        What is the purpose of token accessors in Vault?          To uniquely identify tokens      To grant access to secrets      To authenticate users      To revoke tokens      Module 4: Manage Vault Leases  What is a lease in Vault?          A time-limited credential or resource allocation      A permanent access token      A TLS certificate      An AWS IAM role        Which Vault operation can be performed on leases?          Create      Renew      Authenticate      Revoke        How are leases associated with secrets in Vault?          Manually by administrators      Automatically when secrets are generated      Through LDAP integration      Via AWS IAM roles        What is the default lease duration for secrets in Vault?          1 hour      1 month      1 day      1 year        Which command is used to revoke a lease in Vault?          vault token revoke      vault lease delete      vault secret revoke      vault lease revoke        What happens when a lease is revoked in Vault?          The associated secret is permanently deleted      Access to the associated secret is immediately revoked      The lease duration is extended      A new lease is created        How can you renew a lease in Vault?          By manually updating the lease duration      By using the vault lease renew command      By revoking the lease and recreating it      By updating policies        What is the purpose of lease IDs in Vault?          To identify users      To uniquely identify leases      To authenticate tokens      To revoke leases        In Vault, what is the difference between a lease and a token?          There is no difference, they are synonymous      Tokens are renewable, leases are not      Tokens are associated with policies, leases are not      Tokens are used for authentication, leases are used for secret access        What happens to a secret when its associated lease expires in Vault?          The secret is permanently deleted      Access to the secret is revoked until manually reactivated      The secret is invalidated and cannot be accessed      The secret is automatically renewed      Module 5: Compare and Configure Vault Secrets Engines  What is a secrets engine in Vault?          A component that stores, generates, or encrypts secrets      A user authentication mechanism      A TLS certificate      An AWS IAM role        Which type of secrets engine in Vault is used for dynamic secrets?          Transit      AWS      Cubbyhole      Database        Which secrets engine is commonly used for generating encryption keys?          AWS      Cubbyhole      Transit      Database        In Vault, which secrets engine is suitable for managing access to cloud resources?          Cubbyhole      Transit      Database      AWS        How can a new secrets engine be enabled in Vault?          By updating policies      By restarting the Vault server      Using the vault secrets enable command      Through LDAP integration        What is the purpose of dynamic secrets in Vault?          To store long-lived secrets      To generate short-lived credentials on demand      To authenticate users      To manage encryption keys        What is a lease in the context of secrets engines in Vault?          A permanent access token      A TLS certificate      An AWS IAM role      A time-limited credential or resource allocation        How does Vault manage revocation of dynamic secrets?          By automatically renewing leases      By using LDAP integration      By revoking the associated lease      By rotating encryption keys        What is the purpose of mounting a secrets engine in Vault?          To revoke all secrets associated with the engine      To make the secrets engine accessible via a specific path      To generate new secrets      To authenticate users        Which secrets engine in Vault is commonly used for generating and storing encryption keys?          Database      Transit      AWS      Transit      Module 6: Utilize Vault CLI  Which Vault CLI command is used to authenticate against Vault?          vault token      vault login      vault auth      vault generate        How can you read a secret from Vault using the CLI?          vault secret read      vault read      vault get      vault access        Which command is used to write a secret to Vault?          vault set      vault write      vault add      vault create        How can you list the enabled secrets engines in Vault?          vault list engines      vault secrets list      vault engines      vault enabled        What command is used to revoke a token in Vault?          vault token revoke      vault revoke      vault token revoke-self      vault revoke-self        How can you authenticate using the Vault CLI with a specific authentication method?          By specifying the method in the vault login command      By using the vault login -method=&lt;method&gt; command      By setting environment variables      By editing Vault configuration files        Which command is used to create a new token in Vault?          vault token generate      vault token create      vault create-token      vault generate-token        How can you display detailed information about a token using the Vault CLI?          vault token info      vault token lookup      vault token details      vault token describe        What command is used to authenticate to Vault using a GitHub token?          vault login -method=github      vault auth github      vault authenticate github      vault login -method=github token=        How can you display the current Vault CLI version?          vault version      vault –version      vault -version      vault get version      Module 7: Utilize Vault UI  Which of the following is NOT a feature of Vault’s UI?          Managing secrets engines      Writing policies in HCL      Managing tokens      Viewing audit logs        What authentication methods can be used with Vault’s UI?          Token only      Username/Password only      Multiple methods, including token and username/password      LDAP only        How can you access Vault’s UI?          By installing a separate package      By running a separate server      By enabling the built-in UI      By using a web browser extension        Which action can you perform through Vault’s UI?          Revoking leases      Managing lease renewals      Creating policies      Accessing the CLI        In Vault’s UI, where can you view audit logs?          Settings      Activity tab      Secrets tab      Policies tab        What permissions are required to access Vault’s UI?          Superuser privileges      Vault administrator privileges      No specific permissions required      Full access to all secrets        How can you customize the appearance of Vault’s UI?          By editing the Vault configuration file      By installing plugins      By adding custom branding and logos      By adjusting browser settings        Which web browsers are supported for accessing Vault’s UI?          Internet Explorer only      Firefox only      Safari only      Chrome, Firefox, and Safari        Can you access Vault’s UI without authentication?          Yes, it is open to the public by default      No, authentication is always required      It depends on Vault’s configuration      Only if using a specific authentication method        What is the purpose of Vault’s UI dashboard?          To manage database secrets      To view system logs      To provide an overview of Vault’s status and activity      To configure access policies      Module 8: Be Aware of the Vault API  What does the Vault API provide?          Programmatic access to Vault’s features      A graphical user interface      Token generation      Audit logging        Which protocol does the Vault API primarily use?          HTTP      HTTP(S)      FTP      SMTP        How are API requests authenticated in Vault?          Through LDAP integration      By using AWS IAM roles      By providing a token      By using a username/password combination        What HTTP method is used to read data from Vault using the API?          POST      GET      PUT      DELETE        Which endpoint is used to interact with secrets in Vault via the API?          /auth      /token      /policy      /data        What is the purpose of response wrapping in Vault’s API?          To securely store secrets      To encrypt API requests      To validate authentication tokens      To protect sensitive data during transit        How can you authenticate to the Vault API using a token?          By including the token in the request body      By using HTTP basic authentication      By setting the X-Vault-Token header      By passing the token as a URL parameter        Which HTTP status code indicates a successful API request in Vault?          200      401      403      204        What is the purpose of using namespaces in Vault’s API?          To manage user accounts      To partition data within Vault      To define access control policies      To create isolated environments for different teams        How can you limit the response size when reading data from Vault’s API?          By specifying the desired response format      By setting a maximum request size in Vault’s configuration      By encrypting the response data      By using pagination parameters in the request      Module 9: Explain Vault Architecture  What is the core component of Vault’s architecture?          Storage backend      API gateway      Authentication engine      Policy engine        What is the purpose of the storage backend in Vault?          Authenticating users      Generating tokens      Storing encrypted data      Managing leases        Which component is responsible for authenticating users and clients in Vault?          Token generator      Policy engine      Storage backend      Authentication engine        How does Vault handle encryption of data at rest?          Through LDAP integration      Using an encryption key stored securely      By rotating AWS IAM roles      Through TLS certificates        What is the role of the API gateway in Vault’s architecture?          Storing secrets      Authenticating users      Exposing Vault’s features via HTTP(S) API      Generating tokens        How does Vault ensure high availability?          Through replication and clustering      By limiting access to a single instance      By disabling encryption      By using a single storage backend        What is the purpose of the transit engine in Vault’s architecture?          To authenticate users      To manage access control policies      To perform encryption and decryption operations      To store and retrieve secrets        What is the advantage of Vault’s pluggable architecture?          It allows for customization and integration with external systems      It reduces the need for authentication      It simplifies data storage      It provides built-in encryption algorithms        What role does the barrier controller play in Vault’s architecture?          Managing authentication tokens      Enforcing access control policies      Encrypting data      Protecting sensitive data during transit and at rest        How does Vault handle secrets during transit between clients and the server?          Through TLS encryption      By storing secrets in plaintext      By rotating encryption keys      Through LDAP integration      Module 10: Explain Encryption as a Service  What is encryption as a service?          A feature of Vault for managing access control      A method for storing secrets in plaintext      A service that provides encryption and decryption operations      A protocol for user authentication        How does encryption as a service differ from traditional encryption methods?          It uses static encryption keys      It abstracts away encryption logic into a separate service      It requires manual key management      It relies on external hardware modules        What are some benefits of encryption as a service?          Increased complexity in application architecture      Simplified encryption integration for applications      Lower security due to reliance on external services      Limited scalability options        Which components are typically involved in encryption as a service?          Key management service, encryption service      Database, API gateway      LDAP server, authentication engine      Token generator, policy engine        How can encryption as a service improve application security?          By storing secrets in plaintext      By ensuring consistent encryption practices across applications      By relying solely on perimeter security measures      By disabling encryption for certain sensitive data types        What role does key rotation play in encryption as a service?          It decreases encryption performance      It increases the risk of data loss      It enhances security by regularly changing encryption keys      It reduces the need for encryption        How can encryption as a service simplify compliance with data protection regulations?          By providing centralized key management and encryption policies      By requiring manual encryption for each data record      By storing encryption keys in plaintext      By disabling encryption altogether        What is the typical deployment model for encryption as a service?          Single-tenant only      Multi-tenant or single-tenant      Multi-tenant only      On-premises only        How does encryption as a service contribute to data privacy?          By encrypting data at rest and in transit      By allowing unrestricted access to encrypted data      By storing encryption keys in plaintext      By disabling encryption for certain sensitive data types        What challenges might organizations face when implementing encryption as a service?          Increased complexity in data management      Integration with existing systems and applications      Decreased data security      Reduced performance due to encryption overhead      "
  },
  
  {
    "title": "Azure | Azure CLI Introduction and Usage",
    "url": "/posts/azure-cli/",
    "categories": "Azure, CLI",
    "tags": "microsoft, azure, cli",
    "date": "2024-03-09 00:00:00 +0100",
    





    
    "snippet": "Azure CLI (Command-Line Interface) is a powerful tool for managing Azure resources from the command line. It provides a set of commands for interacting with Azure services and resources, allowing u...",
    "content": "Azure CLI (Command-Line Interface) is a powerful tool for managing Azure resources from the command line. It provides a set of commands for interacting with Azure services and resources, allowing users to automate tasks, deploy resources, and manage infrastructure efficiently. In this guide, we’ll explore the basics of Azure CLI and how to use it with detailed examples.InstallationBefore using Azure CLI, you need to install it on your system. Azure CLI is available for various operating systems, including Windows, macOS, and Linux. You can follow the instructions provided in the official documentation for installation.AuthenticationAfter installing Azure CLI, you need to authenticate with your Azure account to access Azure resources. You can log in using the az login command, which will open a browser window for authentication.az loginOnce authenticated, you can start using Azure CLI to manage your resources.Most Useful Azure CLI Commands with Detailed ExamplesHere are 30 of the most useful Azure CLI commands along with detailed examples:1. az login  Description: Authenticates to Azure and allows you to access Azure resources.  Example:    az login      2. az account list  Description: Lists all Azure subscriptions associated with the logged-in account.  Example:    az account list --output table      3. az account set –subscription &lt;subscription_id&gt;  Description: Sets the active subscription for the current session.  Example:    az account set --subscription &lt;subscription_id&gt;      4. az group create –name &lt;resource_group_name&gt; –location &lt;location&gt;  Description: Creates a new resource group.  Example:    az group create --name MyResourceGroup --location eastus      5. az group list  Description: Lists all resource groups in the current subscription.  Example:    az group list --output table      6. az group delete –name &lt;resource_group_name&gt; –yes –no-wait  Description: Deletes a resource group and all resources within it.  Example:    az group delete --name MyResourceGroup --yes --no-wait      7. az vm create –resource-group &lt;resource_group_name&gt; –name &lt;vm_name&gt; –image &lt;image&gt; –admin-username &lt;username&gt; –generate-ssh-keys  Description: Creates a virtual machine.  Example:    az vm create --resource-group MyResourceGroup --name MyVM --image UbuntuLTS --admin-username azureuser --generate-ssh-keys      8. az vm list –resource-group &lt;resource_group_name&gt;  Description: Lists all virtual machines in a resource group.  Example:    az vm list --resource-group MyResourceGroup --output table      9. az vm start –resource-group &lt;resource_group_name&gt; –name &lt;vm_name&gt;  Description: Starts a virtual machine.  Example:    az vm start --resource-group MyResourceGroup --name MyVM      10. az vm stop –resource-group &lt;resource_group_name&gt; –name &lt;vm_name&gt;- Description: Stops a virtual machine.- Example:  ```bash  az vm stop --resource-group MyResourceGroup --name MyVM  ```11. az storage account create –name &lt;storage_account_name&gt; –resource-group &lt;resource_group_name&gt; –location &lt;location&gt; –sku &lt;sku&gt;- Description: Creates a storage account.- Example:  ```bash  az storage account create --name MyStorageAccount --resource-group MyResourceGroup --location eastus --sku Standard_LRS  ```12. az storage account list- Description: Lists all storage accounts in the current subscription.- Example:  ```bash  az storage account list --output table  ```13. az storage account show-connection-string –name &lt;storage_account_name&gt; –resource-group &lt;resource_group_name&gt;- Description: Shows the connection string for a storage account.- Example:  ```bash  az storage account show-connection-string --name MyStorageAccount --resource-group MyResourceGroup  ```14. az storage blob upload –container-name &lt;container_name&gt; –file &lt;local_file_path&gt; –name &lt;blob_name&gt; –account-name &lt;storage_account_name&gt;- Description: Uploads a file to a storage blob.- Example:  ```bash  az storage blob upload --container-name MyContainer --file myfile.txt --name myblob.txt --account-name MyStorageAccount  ```15. az storage blob list –container-name &lt;container_name&gt; –account-name &lt;storage_account_name&gt;- Description: Lists all blobs in a storage container.- Example:  ```bash  az storage blob list --container-name MyContainer --account-name MyStorageAccount --output table  ```16. az network vnet create –name &lt;vnet_name&gt; –resource-group &lt;resource_group_name&gt; –subnet-name &lt;subnet_name&gt; –address-prefix &lt;address_prefix&gt;- Description: Creates a virtual network.- Example:  ```bash  az network vnet create --name MyVnet --resource-group MyResourceGroup --subnet-name MySubnet --address-prefix 10.0.0.0/16  ```17. az network vnet list –resource-group &lt;resource_group_name&gt;- Description: Lists all virtual networks in a resource group.- Example:  ```bash  az network vnet list --resource-group MyResourceGroup --output table  ```18. az network public-ip create –name &lt;public_ip_name&gt; –resource-group &lt;resource_group_name&gt; –allocation-method &lt;allocation_method&gt;- Description: Creates a public IP address.- Example:  ```bash  az network public-ip create --name MyPublicIP --resource-group MyResourceGroup --allocation-method Static  ```19. az network public-ip list –resource-group &lt;resource_group_name&gt;- Description: Lists all public IP addresses in a resource group.- Example:  ```bash  az network public-ip list --resource-group MyResourceGroup --output table  ```20. az network nsg create –name &lt;nsg_name&gt; –resource-group &lt;resource_group_name&gt;- Description: Creates a network security group.- Example:  ```bash  az network nsg create --name MyNSG --resource-group MyResourceGroup  ```21. az network nsg list –resource-group &lt;resource_group_name&gt;- Description: Lists all network security groups in a resource group.- Example:  ```bash  az network nsg list --resource-group MyResourceGroup --output table  ```22. az network nic create –name &lt;nic_name&gt; –resource-group &lt;resource_group_name&gt; –subnet &lt;subnet_name&gt; –vnet-name &lt;vnet_name&gt; –public-ip-address &lt;public_ip_name&gt;- Description: Creates a network interface.- Example:  ```bash  az network nic create --name MyNIC --resource-group MyResourceGroup --subnet MySubnet --vnet-name MyVnet --public-ip-address MyPublicIP  ```23. az network nic list –resource-group &lt;resource_group_name&gt;- Description: Lists all network interfaces in a resource group.- Example:  ```bash  az network nic list --resource-group MyResourceGroup --output table  ```24. az webapp create –name &lt;app_name&gt; –resource-group &lt;resource_group_name&gt; –plan &lt;app_service_plan_name&gt; –runtime &lt;runtime&gt;- Description: Creates a web app.- Example:  ```bash  az webapp create --name MyWebApp --resource-group MyResourceGroup --plan My  ```What Next?Azure CLI provides a convenient way to manage Azure resources from the command line, enabling automation and efficient resource management. In this guide, we covered the basics of Azure CLI installation, authentication, and usage with detailed examples. Explore more commands and options in the official documentation."
  },
  
  {
    "title": "C# | Types of Classes",
    "url": "/posts/Types-of-Classes-in-C/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development",
    "date": "2024-03-05 00:00:00 +0100",
    





    
    "snippet": "In C#, classes are the building blocks of object-oriented programming. They encapsulate data for the object and define methods to manipulate that data. There are various types of classes in C#, eac...",
    "content": "In C#, classes are the building blocks of object-oriented programming. They encapsulate data for the object and define methods to manipulate that data. There are various types of classes in C#, each serving different purposes. Let’s explore them with detailed examples.1. Regular ClassesRegular classes are the most common type of classes in C#. They are used to define objects and their behavior.Example:public class Person{    public string Name { get; set; }    public int Age { get; set; }    public void PrintDetails()    {        Console.WriteLine($\"Name: {Name}, Age: {Age}\");    }}In this example, we define a Person class with properties Name and Age, and a method PrintDetails() to print the person’s details.2. Abstract ClassesAbstract classes are used as base classes and cannot be instantiated directly. They can contain abstract methods that must be implemented by derived classes.Example:public abstract class Shape{    public abstract double Area();}public class Rectangle : Shape{    public double Length { get; set; }    public double Width { get; set; }    public override double Area()    {        return Length * Width;    }}In this example, Shape is an abstract class with an abstract method Area(). Rectangle is a concrete class that derives from Shape and implements the Area() method.3. Static ClassesStatic classes cannot be instantiated and can only contain static members. They are often used to group related utility methods together.Example:public static class MathUtils{    public static int Add(int a, int b)    {        return a + b;    }    public static int Multiply(int a, int b)    {        return a * b;    }}In this example, MathUtils is a static class containing static methods for addition and multiplication.4. Sealed ClassesSealed classes cannot be inherited. They are often used to prevent further derivation or to increase security.Example:public sealed class FinalClass{    public void Method()    {        Console.WriteLine(\"Method in FinalClass\");    }}In this example, FinalClass is a sealed class that cannot be inherited by other classes.5. Partial ClassesPartial classes allow a class’s members to be defined in multiple files. They are often used in large projects to organize code.Example:File 1 (Person.cs):public partial class Person{    public string Name { get; set; }}File 2 (PersonAdditional.cs):public partial class Person{    public int Age { get; set; }}In this example, Person is defined across two files, each containing a partial definition of the class.What Next?Understanding the different types of classes in C# is crucial for building robust and maintainable applications. Whether it’s regular classes for defining objects, abstract classes for defining base behavior, static classes for utility methods, sealed classes for security, or partial classes for organizing code, each type has its own purpose and use cases. Choose the appropriate type based on your requirements and design principles."
  },
  
  {
    "title": "C# | Implementing Event Driven Microservices Architecture",
    "url": "/posts/Implementing-Event-Driven-Microservices-Architecture-in-.NET/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development",
    "date": "2024-03-02 00:00:00 +0100",
    





    
    "snippet": "In this guide, we’ll explore how to implement an event-driven microservices architecture using .NET technologies, specifically focusing on C# examples. Event-driven architecture (EDA) offers numero...",
    "content": "In this guide, we’ll explore how to implement an event-driven microservices architecture using .NET technologies, specifically focusing on C# examples. Event-driven architecture (EDA) offers numerous benefits such as scalability, loose coupling, and resilience, making it a popular choice for building modern distributed systems. We’ll cover the key concepts and demonstrate how to design and implement event-driven microservices in .NET.Introduction to Event-Driven MicroservicesEvent-driven microservices architecture is a distributed approach where services communicate through events asynchronously. This means services produce and consume events without direct coupling, allowing for better scalability, flexibility, and fault isolation. Key components of event-driven architecture include:  Events: Immutable messages representing a change or an occurrence in the system.  Event producers: Services that generate and publish events.  Event consumers: Services that subscribe to events and react accordingly.  Message brokers: Middleware responsible for routing and delivering events between producers and consumers.Setting Up the Development EnvironmentBefore diving into coding, ensure you have the following prerequisites installed:  .NET SDK for your platform  IDE such as Visual Studio or Visual Studio Code  Messaging middleware (e.g., RabbitMQ, Kafka)Defining Events and MessagesEvents in an event-driven architecture represent meaningful occurrences within the system. Define clear event schemas using a language-neutral format such as JSON or Protobuf. For example:{  \"eventId\": \"guid\",  \"eventType\": \"OrderPlaced\",  \"timestamp\": \"2024-02-18T12:00:00Z\",  \"payload\": {    \"orderId\": \"12345\",    \"customerId\": \"67890\",    \"totalAmount\": 100.00  }}Ensure that events are immutable once published to maintain consistency and reliability.Implementing Microservices with C#When implementing microservices in .NET, follow these best practices:  Use lightweight frameworks such as ASP.NET Core for building microservices.  Design services around business domains to achieve proper encapsulation and separation of concerns.  Implement each microservice as an independent deployable unit, focusing on a single responsibility.  Utilize C# features like async/await for asynchronous communication and handling.// Example of a simple order servicepublic class OrderService{    private readonly IMessageBus _messageBus;    public OrderService(IMessageBus messageBus)    {        _messageBus = messageBus;    }    public async Task PlaceOrderAsync(Order order)    {        // Process order logic...        // Publish order placed event        var orderPlacedEvent = new OrderPlacedEvent(order.Id, order.CustomerId, order.TotalAmount);        await _messageBus.PublishAsync(\"OrderPlaced\", orderPlacedEvent);    }}Using Messaging MiddlewareChoose a messaging middleware that suits your requirements. Popular options include RabbitMQ, Kafka, and Azure Service Bus. Configure the middleware to ensure reliable event delivery and fault tolerance.// Example of RabbitMQ setup in .NETservices.AddMassTransit(x =&gt;{    x.AddBus(provider =&gt; Bus.Factory.CreateUsingRabbitMq(cfg =&gt;    {        cfg.Host(new Uri(\"rabbitmq://localhost/\"), h =&gt;        {            h.Username(\"guest\");            h.Password(\"guest\");        });    }));});Handling Event Consumption and ProcessingConsuming events involves subscribing to event topics and executing appropriate business logic. Use message handler patterns to process events efficiently.// Example of event consumer in .NETpublic class OrderPlacedConsumer : IConsumer&lt;OrderPlacedEvent&gt;{    public async Task Consume(ConsumeContext&lt;OrderPlacedEvent&gt; context)    {        var orderPlacedEvent = context.Message;        // Process order placed event...    }}Testing and DeploymentTest each microservice in isolation using unit tests, integration tests, and contract tests. Deploy microservices independently using containerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).What Next?Event-driven microservices architecture offers a scalable and flexible approach to building distributed systems. By leveraging .NET and C#, you can design robust microservices that communicate asynchronously through events, enabling better decoupling and resilience. With the right tools and practices, you can develop and deploy event-driven microservices efficiently in .NET ecosystem."
  },
  
  {
    "title": "Azure | Infrastructure as Code with ARM Template and Terraform",
    "url": "/posts/Iac/",
    "categories": "Azure, Infrastructure as Code",
    "tags": "microsoft, csharp, azure, iac",
    "date": "2024-02-29 00:00:00 +0100",
    





    
    "snippet": "In modern cloud computing environments, Infrastructure as Code (IaC) has become a cornerstone for managing and provisioning resources efficiently. Two popular tools for implementing IaC are Azure R...",
    "content": "In modern cloud computing environments, Infrastructure as Code (IaC) has become a cornerstone for managing and provisioning resources efficiently. Two popular tools for implementing IaC are Azure Resource Manager (ARM) Templates and Terraform. Below, we’ll explore both with detailed examples.Azure Resource Manager (ARM) TemplateARM Templates are JSON files used to define the infrastructure and configuration of Azure resources. They offer a declarative way to provision resources consistently.Example ARM TemplateBelow is a simple ARM template that deploys an Azure Storage Account:{  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",  \"contentVersion\": \"1.0.0.0\",  \"resources\": [    {      \"type\": \"Microsoft.Storage/storageAccounts\",      \"apiVersion\": \"2019-06-01\",      \"name\": \"mystorageaccount\",      \"location\": \"eastus\",      \"sku\": {        \"name\": \"Standard_LRS\"      },      \"kind\": \"StorageV2\",      \"properties\": {}    }  ]}In this template:  type: Specifies the Azure resource type (Microsoft.Storage/storageAccounts).  apiVersion: Specifies the API version to use for deployment.  name: Specifies the name of the storage account.  location: Specifies the Azure region for deployment.  sku: Specifies the pricing tier and replication type.  kind: Specifies the kind of storage account (StorageV2 in this case).  properties: Specifies additional properties (empty in this example).TerraformTerraform is an open-source infrastructure as code software tool created by HashiCorp. It allows users to define and provision data center infrastructure using a high-level configuration language known as HashiCorp Configuration Language (HCL), or optionally JSON.Example Terraform ConfigurationBelow is a simple Terraform configuration that deploys an Azure Resource Group:provider \"azurerm\" {  features {}}resource \"azurerm_resource_group\" \"example\" {  name     = \"example-resources\"  location = \"East US\"}In this configuration:  provider \"azurerm\": Specifies the Azure provider for Terraform.  azurerm_resource_group: Specifies the resource type (azurerm_resource_group) and a logical name (example).  name: Specifies the name of the resource group.  location: Specifies the Azure region for deployment.What Next?Both ARM Templates and Terraform offer powerful ways to manage infrastructure as code in Azure environments. While ARM Templates are native to Azure and use JSON, Terraform is a multi-cloud tool that supports various providers and uses HCL or JSON for configuration. Choose the tool that best fits your needs and preferences for managing Azure resources efficiently."
  },
  
  {
    "title": "C# | Difference Between Array and ArrayList",
    "url": "/posts/Difference-Between-Array-and-ArrayList-in-C/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development",
    "date": "2024-02-27 00:00:00 +0100",
    





    
    "snippet": "In C#, both arrays and ArrayLists are used to store collections of elements, but they have different characteristics and behaviors. Let’s explore the differences between them with detailed examples...",
    "content": "In C#, both arrays and ArrayLists are used to store collections of elements, but they have different characteristics and behaviors. Let’s explore the differences between them with detailed examples.ArraysArrays are fixed-size data structures that store elements of the same type in contiguous memory locations. Once an array is created, its size cannot be changed.Example of Array in C#// Declaring an array of integers with a fixed size of 5int[] numbers = new int[5];// Initializing array elementsnumbers[0] = 10;numbers[1] = 20;numbers[2] = 30;numbers[3] = 40;numbers[4] = 50;// Accessing array elementsConsole.WriteLine(numbers[2]); // Output: 30Characteristics of Arrays  Fixed size: The size of an array is determined at compile time and cannot be changed at runtime.  Type-safe: Arrays can only store elements of the same type.  Better performance: Arrays offer better performance in terms of accessing elements because they store elements in contiguous memory locations.ArrayListsArrayLists are dynamic collections that can grow or shrink in size dynamically. They can store elements of different types and automatically resize themselves as needed.Example of ArrayList in C#using System;using System.Collections;class Program{    static void Main()    {        // Creating an ArrayList        ArrayList list = new ArrayList();        // Adding elements to the ArrayList        list.Add(10);        list.Add(\"Hello\");        list.Add(3.14);        // Accessing elements in the ArrayList        Console.WriteLine(list[1]); // Output: Hello        // Iterating through the ArrayList        foreach (var item in list)        {            Console.WriteLine(item);        }    }}Characteristics of ArrayLists  Dynamic size: ArrayLists can grow or shrink in size dynamically as elements are added or removed.  Heterogeneous elements: ArrayLists can store elements of different types.  Slower performance: ArrayLists may have slower performance compared to arrays because they involve additional overhead for resizing and boxing/unboxing operations.What Next?Both arrays and ArrayLists have their own advantages and use cases. Arrays are suitable for situations where the size of the collection is known and fixed, and when performance is critical. ArrayLists, on the other hand, are more flexible and convenient for scenarios where the size of the collection may change dynamically or when storing elements of different types. Choose the appropriate data structure based on the specific requirements of your application."
  },
  
  {
    "title": "C# | Unit Tests with xUnit and Complex Inline Data Object",
    "url": "/posts/xUnit-and-Complex-Inline-Data-Object-Theory-Methods/",
    "categories": "C#, TDD",
    "tags": "microsoft, csharp, c#, tdd, unittest",
    "date": "2024-02-24 00:00:00 +0100",
    





    
    "snippet": "In this markdown file, we will explore how to create unit tests using xUnit, a popular unit testing framework for .NET, with complex inline data object theory methods examples.Introduction to xUnit...",
    "content": "In this markdown file, we will explore how to create unit tests using xUnit, a popular unit testing framework for .NET, with complex inline data object theory methods examples.Introduction to xUnitxUnit is an open-source unit testing tool for the .NET Framework. It is highly extensible and provides various features to simplify the process of writing and executing unit tests.Setting up xUnitBefore writing unit tests, make sure you have installed the xUnit framework in your project. You can do this using NuGet Package Manager or .NET CLI.dotnet add package xunitdotnet add package xunit.runner.visualstudioWriting Unit Tests with Complex Inline Data ObjectExample ScenarioLet’s assume we have a class Calculator with a method Add, which takes two integers as input and returns their sum. We want to test this method with different sets of input data, including complex inline data objects.using Xunit;public class Calculator{    public int Add(int a, int b)    {        return a + b;    }}Writing Unit Testsusing Xunit;public class CalculatorTests{    private readonly Calculator _calculator;    public CalculatorTests()    {        _calculator = new Calculator();    }    [Theory]    [InlineData(2, 3, 5)] // Simple inline data    [InlineData(-2, -3, -5)] // Simple inline data with negative numbers    [InlineData(0, 0, 0)] // Simple inline data with zeros    [InlineData(1000, 500, 1500)] // Simple inline data with large numbers    [InlineData(1, -1, 0)] // Simple inline data with mixed positive and negative numbers    [InlineData(2147483647, 1, -2147483648)] // Simple inline data with maximum integer value    [InlineData(-2147483648, -1, 2147483647)] // Simple inline data with minimum integer value    [InlineData(int.MaxValue, 0, int.MaxValue)] // Simple inline data with maximum integer value and zero    [InlineData(int.MinValue, 0, int.MinValue)] // Simple inline data with minimum integer value and zero    [InlineData(2147483647, -2147483648, -1)] // Simple inline data with maximum and minimum integer values    [InlineData(int.MaxValue, int.MaxValue, -2)] // Simple inline data with two maximum integer values    [InlineData(int.MinValue, int.MinValue, 0)] // Simple inline data with two minimum integer values    [InlineData(0, 2147483647, 2147483647)] // Simple inline data with zero and maximum integer value    [InlineData(0, int.MinValue, int.MinValue)] // Simple inline data with zero and minimum integer value    [InlineData(int.MinValue, int.MaxValue, -1)] // Simple inline data with maximum and minimum integer values    public void Add_WithInlineData_ReturnsExpectedResult(int a, int b, int expected)    {        // Arrange        // Act        int result = _calculator.Add(a, b);        // Assert        Assert.Equal(expected, result);    }    [Theory]    [ClassData(typeof(ComplexInlineDataGenerator))]    public void Add_WithComplexInlineData_ReturnsExpectedResult(int a, int b, int expected)    {        // Arrange        // Act        int result = _calculator.Add(a, b);        // Assert        Assert.Equal(expected, result);    }}public class ComplexInlineDataGenerator : IEnumerable&lt;object[]&gt;{    private readonly List&lt;object[]&gt; _data = new List&lt;object[]&gt;    {        new object[] { 2, 3, 5 },        new object[] { -2, -3, -5 },        new object[] { 0, 0, 0 },        new object[] { 1000, 500, 1500 },        new object[] { 1, -1, 0 },        new object[] { 2147483647, 1, -2147483648 },        new object[] { -2147483648, -1, 2147483647 },        new object[] { int.MaxValue, 0, int.MaxValue },        new object[] { int.MinValue, 0, int.MinValue },        new object[] { 2147483647, -2147483648, -1 },        new object[] { int.MaxValue, int.MaxValue, -2 },        new object[] { int.MinValue, int.MinValue, 0 },        new object[] { 0, 2147483647, 2147483647 },        new object[] { 0, int.MinValue, int.MinValue },        new object[] { int.MinValue, int.MaxValue, -1 }    };    public IEnumerator&lt;object[]&gt; GetEnumerator() =&gt; _data.GetEnumerator();    IEnumerator IEnumerable.GetEnumerator() =&gt; GetEnumerator();}In this example:  We have a CalculatorTests class containing tests for the Add method of the Calculator class.  We use [Theory] attribute to denote parameterized tests.  We use [InlineData] attribute to provide inline data for simple test cases.  We define a custom data generator class ComplexInlineDataGenerator implementing IEnumerable&lt;object[]&gt; to generate complex inline data.  The ComplexInlineDataGenerator class provides a collection of test cases with complex inline data.  Each test case consists of three integers: a, b, and the expected result expected.  We use Assert.Equal to verify that the actual result matches the expected result.What Next?By using xUnit with complex inline data object theory methods examples, we can effectively test methods with various input scenarios, ensuring the correctness and robustness of our code."
  },
  
  {
    "title": "C# | Using NSubstitute NuGet Package for C# Unit Tests",
    "url": "/posts/Using-NSubstitute-NuGet-Package-for-C-Unit-Tests/",
    "categories": "C#, TDD",
    "tags": "microsoft, csharp, c#, tdd, unittest",
    "date": "2024-02-21 00:00:00 +0100",
    





    
    "snippet": "NSubstitute is a powerful mocking library for C# unit tests, allowing developers to easily create substitute objects (mocks) for dependencies. These substitutes can be configured to behave in speci...",
    "content": "NSubstitute is a powerful mocking library for C# unit tests, allowing developers to easily create substitute objects (mocks) for dependencies. These substitutes can be configured to behave in specific ways, making them ideal for isolating the unit under test and verifying its interactions with dependencies. In this guide, we’ll explore how to use NSubstitute in your C# unit tests with detailed examples.InstallationBefore you can start using NSubstitute, you need to install the NSubstitute NuGet package in your C# project. You can do this via the NuGet Package Manager Console or through the Visual Studio UI:Install-Package NSubstituteAlternatively, you can search for “NSubstitute” in the NuGet Package Manager UI and install it from there.Basic UsageLet’s start with a basic example of using NSubstitute to mock an interface. Suppose we have the following interface representing a data repository:public interface IDataRepository{    string GetData();}Now, let’s create a unit test for a class (DataProcessor) that depends on this interface. We’ll use NSubstitute to mock the IDataRepository interface:using NSubstitute;using Xunit;public class DataProcessorTests{    [Fact]    public void ProcessData_WhenDataIsAvailable_ReturnsProcessedData()    {        // Arrange        var mockRepository = Substitute.For&lt;IDataRepository&gt;();        mockRepository.GetData().Returns(\"Mocked Data\");        var dataProcessor = new DataProcessor(mockRepository);        // Act        var result = dataProcessor.ProcessData();        // Assert        Assert.Equal(\"Processed Mocked Data\", result);    }}In this example:  We create a substitute for the IDataRepository interface using Substitute.For&lt;IDataRepository&gt;().  We configure the substitute’s behavior using the Returns method to return “Mocked Data” when the GetData method is called.  We then create an instance of DataProcessor, passing the mock repository as a dependency.  Finally, we invoke the method under test (ProcessData) and verify its behavior.Advanced UsageNSubstitute provides various advanced features, such as argument matching, configuring return values based on arguments, and verifying method calls. Let’s look at some examples:Argument MatchingYou can use argument matching to specify different behaviors based on method arguments. For example:mockRepository.GetData(Arg.Any&lt;int&gt;()).Returns(\"Mocked Data\");This configuration will return “Mocked Data” regardless of the integer argument passed to GetData.Verifying Method CallsYou can verify that specific methods were called on the substitute and optionally specify the number of times they were called. For example:mockRepository.Received().GetData();mockRepository.Received(3).GetData(); // Verifies that GetData was called exactly 3 times.Configuring CallbacksYou can configure substitutes to perform custom actions when methods are called. For example:mockRepository.GetData().Returns(x =&gt; \"Mocked Data\");This configuration uses a lambda expression to return “Mocked Data” based on the arguments passed to GetData.What Next?NSubstitute is a powerful mocking library that simplifies unit testing in C#. By allowing developers to create substitutes for dependencies with specific behaviors, NSubstitute enables more effective isolation of units under test and verification of their interactions. With its intuitive syntax and advanced features, NSubstitute is a valuable tool for writing robust and maintainable unit tests in C#.For more information on NSubstitute, refer to the official documentation."
  },
  
  {
    "title": "C# | Clean Architecture",
    "url": "/posts/Clean-Architecture-with-.NET/",
    "categories": "C#, Architecture",
    "tags": "microsoft, csharp, c#, architecture",
    "date": "2024-02-16 00:00:00 +0100",
    





    
    "snippet": "Clean Architecture is a software design philosophy that aims to create systems that are independent of frameworks and libraries. It focuses on separating concerns and creating a clear separation of...",
    "content": "Clean Architecture is a software design philosophy that aims to create systems that are independent of frameworks and libraries. It focuses on separating concerns and creating a clear separation of responsibilities, making the codebase easier to understand, maintain, and test.Principles of Clean ArchitectureClean Architecture is based on several key principles:      Independence of Frameworks: The core business logic should not depend on the details of any specific framework or technology.        Testability: The architecture should facilitate easy testing of the application’s business rules without requiring external dependencies.        Independence of UI: The user interface should be decoupled from the core business logic. This allows for easier changes to the UI without affecting the underlying system.        Independence of Database: The database implementation details should not leak into the core business logic. This allows for easier changes to the database technology if needed.        Separation of Concerns: The architecture should clearly separate different concerns, such as business logic, presentation, and data access.  Components of Clean ArchitectureClean Architecture typically consists of the following layers:      Entities: These represent the business objects or entities in the application. They encapsulate the core business logic and are independent of any external concerns.        Use Cases: Also known as Interactors or Application Services, use cases contain the application-specific business rules and orchestrate the flow of data between entities and external systems.        Interfaces (Adapters): Interfaces define contracts between the application core and external systems, such as databases, user interfaces, or third-party services. Adapters implement these interfaces to interact with external systems.        Frameworks and Drivers: This layer consists of external frameworks, libraries, and tools, such as web frameworks, UI libraries, and database drivers. These are the outermost layers and are responsible for interacting with the outside world.  Example in C#Let’s illustrate the Clean Architecture principles with a simple C# example:// Entitypublic class Product{    public int Id { get; set; }    public string Name { get; set; }    public decimal Price { get; set; }}// Use Casepublic class ProductService{    public IEnumerable&lt;Product&gt; GetProducts()    {        // Logic to retrieve products from a data store        return new List&lt;Product&gt;();    }    public void SaveProduct(Product product)    {        // Logic to save product to a data store    }}// Interface (Repository)public interface IProductRepository{    IEnumerable&lt;Product&gt; GetProducts();    void SaveProduct(Product product);}// Adapter (Repository Implementation)public class ProductRepository : IProductRepository{    public IEnumerable&lt;Product&gt; GetProducts()    {        // Implementation to retrieve products from a database        return new List&lt;Product&gt;();    }    public void SaveProduct(Product product)    {        // Implementation to save product to a database    }}In this example, Product represents the entity, ProductService acts as the use case, IProductRepository defines the interface, and ProductRepository is the adapter implementing the repository interface.What Next?Clean Architecture with .NET promotes maintainable and testable software by emphasizing separation of concerns and independence of external dependencies. By adhering to its principles, developers can create robust and flexible applications that are easier to maintain and evolve over time."
  },
  
  {
    "title": "C# | Understanding the Observer Pattern",
    "url": "/posts/Understanding-the-Observer-Pattern-in-C/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development",
    "date": "2024-02-14 00:00:00 +0100",
    





    
    "snippet": "The Observer Pattern is a behavioral design pattern where an object, known as the subject, maintains a list of its dependents, called observers, and notifies them of any state changes, usually by c...",
    "content": "The Observer Pattern is a behavioral design pattern where an object, known as the subject, maintains a list of its dependents, called observers, and notifies them of any state changes, usually by calling one of their methods. This pattern promotes loose coupling between objects, as observers are only aware of the subject and not each other. In C#, this pattern is commonly used in event-driven programming.ImplementationLet’s understand the Observer Pattern through a detailed example in C#.Subject InterfaceFirst, we define an interface for the subject. This interface will contain methods for registering, unregistering, and notifying observers.public interface ISubject{    void RegisterObserver(IObserver observer);    void UnregisterObserver(IObserver observer);    void NotifyObservers();}Observer InterfaceNext, we define an interface for the observer. This interface will contain a method that the subject will call when it needs to notify observers.public interface IObserver{    void Update();}Concrete SubjectNow, let’s implement a concrete subject class that implements the ISubject interface.public class ConcreteSubject : ISubject{    private List&lt;IObserver&gt; observers = new List&lt;IObserver&gt;();    public void RegisterObserver(IObserver observer)    {        observers.Add(observer);    }    public void UnregisterObserver(IObserver observer)    {        observers.Remove(observer);    }    public void NotifyObservers()    {        foreach (var observer in observers)        {            observer.Update();        }    }}Concrete ObserverNext, let’s implement a concrete observer class that implements the IObserver interface.public class ConcreteObserver : IObserver{    public void Update()    {        Console.WriteLine(\"Observer notified of state change.\");    }}Example UsageNow, let’s see how we can use these classes together.class Program{    static void Main(string[] args)    {        ConcreteSubject subject = new ConcreteSubject();        ConcreteObserver observer1 = new ConcreteObserver();        ConcreteObserver observer2 = new ConcreteObserver();        subject.RegisterObserver(observer1);        subject.RegisterObserver(observer2);        subject.NotifyObservers();        subject.UnregisterObserver(observer1);        subject.NotifyObservers();    }}In this example, ConcreteSubject is the subject, and ConcreteObserver is the observer. When NotifyObservers() is called, both observers are notified of the state change. After unregistering one observer, only the remaining observer is notified.What Next?The Observer Pattern is a powerful way to implement communication between objects in C#. It promotes loose coupling and can be particularly useful in event-driven architectures. By understanding and implementing this pattern, you can write more maintainable and scalable code."
  },
  
  {
    "title": "ASP.NET | SOLID Principles, and Clean Architecture",
    "url": "/posts/ASP.NET-Core-SOLID-Principles-and-Clean-Architecture-in-.NET/",
    "categories": "ASP.NET, Best Practices",
    "tags": "microsoft, csharp, asp.net, bestpractices",
    "date": "2024-02-11 00:00:00 +0100",
    





    
    "snippet": "IntroductionIn modern software development, adhering to principles like SOLID and following Clean Architecture patterns is crucial for building scalable, maintainable, and testable applications. Th...",
    "content": "IntroductionIn modern software development, adhering to principles like SOLID and following Clean Architecture patterns is crucial for building scalable, maintainable, and testable applications. This markdown file aims to provide an overview of these concepts within the context of ASP.NET Core using C# examples.SOLID PrinciplesSOLID is an acronym representing five key principles of object-oriented programming and design. They are:      Single Responsibility Principle (SRP): A class should have only one reason to change.     public class UserManager {     public void AddUser(User user)     {         // Add user to database     }     public void DeleteUser(User user)     {         // Delete user from database     } }            Open/Closed Principle (OCP): Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification.     public abstract class Shape {     public abstract double Area(); } public class Rectangle : Shape {     public double Width { get; set; }     public double Height { get; set; }     public override double Area()     {         return Width * Height;     } } public class Circle : Shape {     public double Radius { get; set; }     public override double Area()     {         return Math.PI * Radius * Radius;     } }            Liskov Substitution Principle (LSP): Objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program.     public class Rectangle {     public virtual int Height { get; set; }     public virtual int Width { get; set; }     public int Area()     {         return Height * Width;     } } public class Square : Rectangle {     private int _side;     public override int Height     {         get =&gt; _side;         set =&gt; _side = value;     }     public override int Width     {         get =&gt; _side;         set =&gt; _side = value;     } }            Interface Segregation Principle (ISP): A client should not be forced to implement an interface that it doesn’t use.     public interface IShape {     double Area(); } public interface IResizable {     void Resize(double factor); } public class Square : IShape, IResizable {     private double _side;     public double Area()     {         return _side * _side;     }     public void Resize(double factor)     {         _side *= factor;     } }            Dependency Inversion Principle (DIP): High-level modules should not depend on low-level modules. Both should depend on abstractions, and abstractions should not depend on details.     public interface ILogger {     void Log(string message); } public class FileLogger : ILogger {     public void Log(string message)     {         // Log message to file     } } public class DatabaseLogger : ILogger {     public void Log(string message)     {         // Log message to database     } }      Clean Architecture in .NETClean Architecture emphasizes separation of concerns and maintaining a clear distinction between business logic, application layers, and external dependencies.Layers of Clean Architecture  Entities: Contain business objects or business data structures.  Use Cases (Interactors or Application Services): Contain application-specific business rules.  Interfaces (Adapters): Act as bridges between the use cases and external systems.  Frameworks and Drivers: Includes frameworks such as databases, web frameworks, etc.Example StructureMyApp.Core├── Entities│   ├── User.cs│   └── ...├── UseCases│   ├── AddUserUseCase.cs│   ├── DeleteUserUseCase.cs│   └── ...└── Interfaces    ├── IUserRepository.cs    └── ...MyApp.Infrastructure├── Repositories│   ├── UserRepository.cs│   └── ...└── ExternalServices    ├── EmailService.cs    └── ...MyApp.Presentation├── Controllers│   ├── UserController.cs│   └── ...└── Views    ├── Index.cshtml    └── ...Example Code// Core layernamespace MyApp.Core.Entities{    public class User    {        public int Id { get; set; }        public string Name { get; set; }        // other properties    }}namespace MyApp.Core.UseCases{    public class AddUserUseCase    {        private readonly IUserRepository _userRepository;        public AddUserUseCase(IUserRepository userRepository)        {            _userRepository = userRepository;        }        public void Execute(User user)        {            // Validation, business logic, etc.            _userRepository.Add(user);        }    }}// Infrastructure layernamespace MyApp.Infrastructure.Repositories{    public class UserRepository : IUserRepository    {        public void Add(User user)        {            // Add user to the database        }        // other repository methods    }}// Presentation layernamespace MyApp.Presentation.Controllers{    public class UserController : Controller    {        private readonly AddUserUseCase _addUserUseCase;        public UserController(AddUserUseCase addUserUseCase)        {            _addUserUseCase = addUserUseCase;        }        public IActionResult AddUser(User user)        {            _addUserUseCase.Execute(user);            return RedirectToAction(\"Index\");        }    }}What Next?By incorporating SOLID principles and Clean Architecture patterns into ASP.NET Core applications, developers can achieve code that is more modular, maintainable, and adaptable to changes in requirements or technologies."
  },
  
  {
    "title": "C# | Avoid Multiple Nested If-Else Statements Using Guard Clause",
    "url": "/posts/Avoid-Multiple-Nested-If-Else-Statements-Using-Guard-Clause-in-C/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development, bestpractices",
    "date": "2024-02-07 00:00:00 +0100",
    





    
    "snippet": "In C#, multiple nested if-else statements can often lead to code that is hard to read, understand, and maintain. Guard Clauses offer a technique to refactor such code and make it more readable and ...",
    "content": "In C#, multiple nested if-else statements can often lead to code that is hard to read, understand, and maintain. Guard Clauses offer a technique to refactor such code and make it more readable and maintainable. In this guide, we’ll explore how to avoid multiple nested if-else statements using Guard Clauses in C# with detailed examples.What are Guard Clauses?Guard Clauses, also known as Early Returns, are conditional statements placed at the beginning of a method to handle exceptional cases or conditions. Instead of nesting multiple if-else blocks, Guard Clauses provide a clear exit strategy if certain conditions are met, reducing the complexity of the code.Example without Guard ClausesConsider the following C# method that calculates the total price of a product with different discounts based on the customer type:public decimal CalculateTotalPrice(Product product, CustomerType customerType){    decimal totalPrice = 0;    if (product != null)    {        decimal basePrice = product.BasePrice;        if (customerType == CustomerType.Regular)        {            totalPrice = basePrice * 0.95m; // 5% discount for regular customers        }        else if (customerType == CustomerType.Premium)        {            totalPrice = basePrice * 0.90m; // 10% discount for premium customers        }        else if (customerType == CustomerType.VIP)        {            totalPrice = basePrice * 0.85m; // 15% discount for VIP customers        }    }    return totalPrice;}This code contains multiple nested if-else statements, which can make it difficult to understand and maintain as more conditions are added.Example with Guard ClausesNow, let’s refactor the above method using Guard Clauses:public decimal CalculateTotalPrice(Product product, CustomerType customerType){    if (product == null)    {        return 0; // No product, return 0    }    decimal basePrice = product.BasePrice;    if (customerType == CustomerType.Regular)    {        return basePrice * 0.95m; // 5% discount for regular customers    }    if (customerType == CustomerType.Premium)    {        return basePrice * 0.90m; // 10% discount for premium customers    }    if (customerType == CustomerType.VIP)    {        return basePrice * 0.85m; // 15% discount for VIP customers    }    return basePrice; // Default price if no specific discount applies}In this refactored version, each special condition is checked at the beginning of the method, and if it’s met, the method returns immediately. This approach reduces nesting and makes the code more readable and maintainable.Benefits of Using Guard Clauses  Readability: Guard Clauses make the code easier to understand by removing unnecessary nesting.  Maintainability: It’s easier to maintain and extend the code as new conditions can be added without adding more nesting.  Debugging: Guard Clauses provide clear exit points, making it easier to debug and trace the flow of execution.What Next?Guard Clauses offer a cleaner and more maintainable way to handle special conditions in C# methods, especially when dealing with multiple nested if-else statements. By using Guard Clauses, you can improve the readability and maintainability of your code, making it easier to understand and debug."
  },
  
  {
    "title": "C# | Alias any type",
    "url": "/posts/Alias-any-type-in-C/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, development",
    "date": "2024-02-03 00:00:00 +0100",
    





    
    "snippet": "In C#, you can create aliases for types using the using directive. This allows you to create shorter, more descriptive names for types, making your code more readable. This feature is particularly ...",
    "content": "In C#, you can create aliases for types using the using directive. This allows you to create shorter, more descriptive names for types, making your code more readable. This feature is particularly useful when dealing with long or complex type names, or when you want to clarify the purpose of a type within a specific context.Syntaxusing AliasName = OriginalTypeName;Example UsageBasic Aliasusing Dict = System.Collections.Generic.Dictionary&lt;string, int&gt;;class Program{    static void Main(string[] args)    {        Dict myDictionary = new Dict();        myDictionary.Add(\"One\", 1);        myDictionary.Add(\"Two\", 2);        Console.WriteLine($\"Value for 'One': {myDictionary[\"One\"]}\");        Console.WriteLine($\"Value for 'Two': {myDictionary[\"Two\"]}\");    }}In this example, Dict is an alias for System.Collections.Generic.Dictionary&lt;string, int&gt;. This makes the code cleaner and more readable.Alias with Namespacenamespace MyNamespace{    using Dict = System.Collections.Generic.Dictionary&lt;string, int&gt;;    class Program    {        static void Main(string[] args)        {            Dict myDictionary = new Dict();            myDictionary.Add(\"One\", 1);            myDictionary.Add(\"Two\", 2);            Console.WriteLine($\"Value for 'One': {myDictionary[\"One\"]}\");            Console.WriteLine($\"Value for 'Two': {myDictionary[\"Two\"]}\");        }    }}Here, the alias Dict is used within the MyNamespace namespace.Alias for Nested Typenamespace MyNamespace{    using Inner = Outer.InnerClass;    class Outer    {        public class InnerClass        {            public void Display()            {                Console.WriteLine(\"InnerClass Display method\");            }        }    }    class Program    {        static void Main(string[] args)        {            Inner innerObj = new Inner();            innerObj.Display();        }    }}In this example, Inner is an alias for the nested class Outer.InnerClass.Benefits of Using Alias  Readability: Alias names can make code more readable, especially when dealing with long or nested type names.  Reduced Typing: Shorter aliases reduce the amount of typing required, improving developer productivity.  Clarity: Aliases can provide clarity about the purpose of a type within a specific context.What Next?Using aliases in C# can enhance the readability and maintainability of your code by providing shorter, more descriptive names for types. It’s a powerful feature that can improve code quality and developer productivity. However, it’s essential to use aliases judiciously and ensure they enhance readability without sacrificing clarity."
  },
  
  {
    "title": "Azure | Interview Questions and Answers",
    "url": "/posts/Azure-Interview-Questions-and-Answers/",
    "categories": "Azure, Interview",
    "tags": "microsoft, azure, questions, answers",
    "date": "2024-01-29 00:00:00 +0100",
    





    
    "snippet": "1. What is Microsoft Azure?Microsoft Azure is a cloud computing platform provided by Microsoft that offers a wide range of services, including computing power, storage, networking, databases, analy...",
    "content": "1. What is Microsoft Azure?Microsoft Azure is a cloud computing platform provided by Microsoft that offers a wide range of services, including computing power, storage, networking, databases, analytics, and more, enabling organizations to build, deploy, and manage applications and services in the cloud.2. What is the Azure Resource Manager (ARM)?Azure Resource Manager is the deployment and management service for Azure. It provides a way to deploy and manage resources in a consistent and repeatable manner, using JSON templates to define the infrastructure and configurations.3. What is Azure Virtual Machine (VM)?Azure Virtual Machine is a scalable computing resource in the cloud that allows users to run Windows or Linux-based applications. VMs can be customized with various configurations, and users only pay for the compute capacity they consume.4. Explain Azure Blob Storage.Azure Blob Storage is a scalable object storage solution for the cloud, designed to store and manage massive amounts of unstructured data. It is commonly used for storing and retrieving binary data, such as images, videos, and documents.5. What is Azure Active Directory (AAD)?Azure Active Directory is Microsoft’s cloud-based identity and access management service. It helps organizations manage user identities and secure access to applications, resources, and data.6. What is Azure Functions?Azure Functions is a serverless computing service that allows developers to run event-triggered code without having to explicitly provision or manage infrastructure. It supports multiple programming languages and can be used for various scenarios, such as data processing and automation.7. How does Azure Load Balancer work?Azure Load Balancer distributes incoming network traffic across multiple servers to ensure no single server becomes overwhelmed. It improves the availability and reliability of applications by distributing traffic evenly.8. Explain Azure DevOps.Azure DevOps is a set of development tools and services that facilitate the entire development lifecycle, including planning, coding, building, testing, and deployment. It includes services such as Azure Boards, Azure Repos, Azure Pipelines, and Azure Test Plans.9. What is Azure SQL Database?Azure SQL Database is a fully managed relational database service in the cloud. It allows users to build, scale, and extend applications with high availability and security features, without the need to manage the underlying infrastructure.10. What is Azure Kubernetes Service (AKS)?Azure Kubernetes Service is a managed container orchestration service that simplifies the deployment, management, and scaling of containerized applications using Kubernetes.11. What is Azure Virtual Network?Azure Virtual Network enables the creation of private, isolated networks in the Azure cloud. It allows users to securely connect Azure resources and extend on-premises networks to the cloud.12. How does Azure Backup work?Azure Backup is a cloud-based backup service that enables the backup and recovery of data and applications from the Microsoft Azure cloud. It supports various workloads, including virtual machines, databases, and files.13. What is Azure Logic Apps?Azure Logic Apps is a cloud service that allows users to automate workflows and integrate different services and systems. It provides a visual designer for building workflows and supports a wide range of connectors.14. Explain Azure Policy.Azure Policy is a service that allows organizations to define, enforce, and audit policies for resources in Azure. It helps maintain compliance with organizational standards and regulations.15. What is Azure Key Vault?Azure Key Vault is a cloud service that securely stores and manages sensitive information, such as secrets, encryption keys, and certificates. It helps protect and control access to critical data used by cloud applications and services.16. How does Azure Active Directory B2B work?Azure Active Directory B2B (Business to Business) allows organizations to securely share their applications and services with guest users from other organizations, while maintaining control over access and permissions.17. What is Azure Monitor?Azure Monitor is a comprehensive solution for collecting, analyzing, and acting on telemetry data from Azure resources. It provides insights into the performance and health of applications and infrastructure.18. Explain Azure ExpressRoute.Azure ExpressRoute is a dedicated network connection that provides private and reliable connectivity between on-premises data centers and Azure. It offers higher reliability and lower latency than internet-based connections.19. What are Azure Cognitive Services?Azure Cognitive Services are a set of AI services and APIs that enable developers to add vision, speech, language, and search capabilities to their applications without requiring expertise in machine learning.20. What is Azure AD Application Proxy?Azure AD Application Proxy enables organizations to securely publish on-premises applications for remote access. It allows users to access applications from outside the corporate network without the need for a VPN.21. How does Azure Traffic Manager work?Azure Traffic Manager is a DNS-based traffic load balancer that distributes user traffic across global data centers to ensure high availability and responsiveness of applications.22. What is Azure Blueprints?Azure Blueprints is a service that helps organizations define and enforce standards for Azure environments. It allows the creation of reusable templates for deploying and governing Azure resources.23. What is Azure DevTest Labs?Azure DevTest Labs provides a self-service sandbox environment for quickly creating, deploying, and managing test and development environments. It helps save costs and ensures efficient use of resources.24. Explain Azure Managed Disks.Azure Managed Disks simplify the management of storage associated with Azure Virtual Machines. They offer scalable and highly available block storage without the need to manage individual storage accounts.25. What is Azure CDN?Azure Content Delivery Network (CDN) is a distributed network of servers that deliver web content, including images, videos, and scripts, to users based on their geographic location. It improves the performance and reduces latency.26. What is Azure Policy Initiative?Azure Policy Initiative allows organizations to create sets of policies to enforce requirements and controls over resource deployments. It helps ensure compliance with organizational standards and regulatory requirements.27. How does Azure Active Directory Domain Services (AAD DS) work?Azure Active Directory Domain Services provides managed domain services such as domain join, group policy, and LDAP in the Azure cloud. It enables organizations to lift and shift applications to Azure without the need for domain controllers.28. What is Azure Data Factory?Azure Data Factory is a cloud-based data integration service that allows users to create, schedule, and manage data pipelines for moving and transforming data from various sources to different destinations.29. Explain Azure Event Hubs.Azure Event Hubs is a scalable and real-time event processing service that can ingest and process millions of events per second. It is commonly used for building big data and IoT solutions.30. What is Azure Information Protection?Azure Information Protection is a cloud-based solution that helps organizations classify, label, and protect sensitive data based on policies. It provides persistent protection, even when the data is shared outside the organization.What Next?These are just a few examples of Azure interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "Azure DevOps | Interview Questions and Answers",
    "url": "/posts/Azure-DevOps-Interview-Questions-and-Answers/",
    "categories": "Azure DevOps, Interview",
    "tags": "microsoft, azuredevops, azure, questions, answers",
    "date": "2024-01-23 00:00:00 +0100",
    





    
    "snippet": "IntroductionAzure DevOps is a comprehensive set of development tools and services provided by Microsoft. It covers a wide range of activities from source control to build and release automation. Th...",
    "content": "IntroductionAzure DevOps is a comprehensive set of development tools and services provided by Microsoft. It covers a wide range of activities from source control to build and release automation. This document compiles common interview questions related to Azure DevOps along with their answers.Basic QuestionsQ1: What is Azure DevOps?Azure DevOps is a set of development tools and services designed to help teams plan, develop, test, and deliver software. It includes services for version control, build automation, release management, and more.Q2: Name some key components of Azure DevOps.  Azure Repos  Azure Pipelines  Azure Boards  Azure Test Plans  Azure ArtifactsSource ControlQ3: What is Azure Repos?Azure Repos is a version control system that allows teams to manage and track changes to their code. It supports both Git and Team Foundation Version Control (TFVC).Q4: How do you create a branch in Azure Repos?To create a branch in Azure Repos, you can use the following command in Git:git checkout -b branch_nameQ5: Explain the difference between Git and TFVC.Git is a distributed version control system, while TFVC is a centralized version control system. Git allows for more flexibility in branching and merging, making it more suitable for distributed development.Build and ReleaseQ6: What is a Build Pipeline in Azure DevOps?A Build Pipeline in Azure DevOps is a set of instructions that defines how source code is compiled, tested, and packaged into artifacts.Q7: How do you trigger a build in Azure Pipelines?Builds in Azure Pipelines can be triggered manually, on a schedule, or automatically on code commits using triggers defined in the pipeline configuration.PipelinesQ8: What is an Azure Pipeline?Azure Pipelines is a cloud service that enables continuous integration (CI) and continuous delivery (CD) to test, build, and deploy applications.Q9: Explain the difference between Multi-Stage and Multi-Job Pipelines.In Multi-Stage Pipelines, each stage represents a phase in the release pipeline. In Multi-Job Pipelines, multiple jobs run in parallel within a stage, providing more control over dependencies and parallelism.Azure DevOps ServicesQ10: How do you integrate Azure Boards with Azure Repos?Azure Boards can be integrated with Azure Repos by linking work items to branches and pull requests. This allows for better traceability between code changes and work items.Q11: What is Azure Test Plans?Azure Test Plans is a tool in Azure DevOps for managing test cases and executing tests. It helps in tracking test results and ensuring the quality of the application.What Next?These are just a few examples of Azure Devops interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "C# | Interview Questions and Answers",
    "url": "/posts/C-Interview-Questions-and-Answers/",
    "categories": "C#, Interview",
    "tags": "microsoft, csharp, c#, questions, answers",
    "date": "2024-01-17 00:00:00 +0100",
    





    
    "snippet": "1. What is C#?C# (pronounced C-sharp) is a modern, object-oriented programming language developed by Microsoft as part of the .NET platform.2. Differentiate between String and StringBuilder in C#. ...",
    "content": "1. What is C#?C# (pronounced C-sharp) is a modern, object-oriented programming language developed by Microsoft as part of the .NET platform.2. Differentiate between String and StringBuilder in C#.  String is immutable, meaning its value cannot be changed after it’s created. StringBuilder is mutable and allows for efficient manipulation of strings.3. Explain the difference between const and readonly in C#.  const is compile-time constant, and its value must be assigned at the time of declaration. readonly allows the value to be assigned either at the time of declaration or within a constructor.4. What is the purpose of the using statement in C#?  The using statement is used to include a namespace in the program. It also helps in resource management by automatically disposing of objects when they are no longer needed.5. Describe the concept of boxing and unboxing in C#.  Boxing is the process of converting a value type to a reference type, and unboxing is the reverse process. It involves converting a reference type back to a value type.6. What is the significance of the var keyword in C#?  The var keyword is used for implicitly declaring variables, letting the compiler infer the type based on the assigned value.7. Explain the purpose of the async and await keywords.  async is used to declare an asynchronous method, and await is used to asynchronously wait for a task to complete, allowing other tasks to run in the meantime.8. Differentiate between == and Equals method in C#.  == is used for comparing the values of two variables, while the Equals method is used for comparing the contents of objects.9. What is a delegate in C#?  A delegate is a type that represents references to methods with a specific signature. It is similar to a function pointer in C or C++.10. Explain the use of LINQ in C#.  LINQ (Language Integrated Query) is a set of features that extends C# by the addition of query expressions, allowing the manipulation of data from different sources.11. How does garbage collection work in C#?  Garbage collection is an automatic memory management process in C# that identifies and collects objects that are no longer needed, freeing up memory.12. Describe the Singleton design pattern.  The Singleton pattern ensures that a class has only one instance and provides a global point of access to that instance. It involves a private constructor and a static method to retrieve the instance.13. What is the difference between throw and throw ex in C#?  throw is used to rethrow the current exception, preserving the original stack trace, while throw ex discards the original stack trace and starts a new one.14. How does the try, catch, and finally blocks work in C#?  try is used to enclose the code that might throw an exception. catch is used to handle the exception, and finally is used to specify a block of code that will be executed, regardless of whether an exception is thrown or not.15. What is the purpose of the sealed keyword in C#?  The sealed keyword is used to prevent a class from being inherited. It ensures that the class cannot be a base class for other classes.16. Explain the concept of an interface in C#.  An interface is a contract that defines a set of methods without providing the implementation. Classes that implement an interface must provide concrete implementations for all its methods.17. How does method overloading work in C#?  Method overloading allows a class to have multiple methods with the same name but different parameters, enabling the flexibility of using different argument lists.18. Describe the use of the out keyword in C#.  The out keyword is used to pass a parameter by reference, allowing the called method to modify the value of the parameter.19. What is a property in C#?  A property is a member that provides a flexible mechanism to read, write, or compute the value of a private field.20. How does the IEnumerable interface work in C#?  IEnumerable is an interface that provides an enumerator, allowing a collection to be iterated using a foreach loop.21. Explain the concept of indexers in C#.  Indexers allow objects to be indexed like arrays, providing a convenient way to access elements within a class.22. What is the purpose of the using directive in C#?  The using directive is used to include namespaces in a program, allowing the use of types defined in those namespaces without fully qualifying the type names.23. Describe the concept of events in C#.  Events are a mechanism for communication between objects. They allow a class to notify other classes or objects when an action or state change occurs.24. How does the lock statement work in C#?  The lock statement is used to synchronize access to a shared resource by acquiring a mutual exclusion lock.25. What is the role of the params keyword in C#?  The params keyword allows a method to accept a variable number of parameters, making it more flexible.26. Explain the concept of nullable types in C#.  Nullable types allow variables to be assigned a value or null. They are useful when dealing with databases or other situations where values may be missing.27. What is the purpose of the ref keyword in C#?  The ref keyword is used to pass a parameter by reference, allowing the called method to modify the value of the parameter.28. Differentiate between interface and abstract class in C#.  An interface cannot contain any implementation, while an abstract class can have both abstract and concrete methods. A class can implement multiple interfaces but can inherit from only one abstract class.29. How does the yield keyword work in C#?  The yield keyword is used in iterator methods to simplify the implementation of custom iterators.30. Explain the concept of boxing and unboxing in C#.  Boxing is the process of converting a value type to a reference type, and unboxing is the reverse process. It involves converting a reference type back to a value type.31. What are the different access modifiers in C#?  The main access modifiers in C# are public, private, protected, internal, protected internal, and private protected.32. Describe the use of the base keyword in C#.  The base keyword is used to access members of the base class from within a derived class.33. What is the purpose of the this keyword in C#?  The this keyword is used to refer to thecurrent instance of the class. It is often used to differentiate between instance variables and parameters with the same name.34. Explain the concept of polymorphism in C#.  Polymorphism allows objects of different types to be treated as objects of a common base type. It includes method overloading and method overriding.35. How does the String.Split method work in C#?  The String.Split method is used to split a string into an array of substrings based on a specified delimiter.36. What is the purpose of the Dispose method in C#?  The Dispose method is used to release unmanaged resources held by an object. It is often implemented by classes that use resources such as files or network connections.37. Describe the concept of generics in C#.  Generics allow the creation of classes, interfaces, and methods with placeholder types, providing type safety and code reusability.38. How does the Nullable&lt;T&gt; struct work in C#?  The Nullable&lt;T&gt; struct allows value types to have a null value, providing a way to represent the absence of a value.39. What is the purpose of the volatile keyword in C#?  The volatile keyword is used to indicate that a field might be modified by multiple threads, preventing compiler optimizations that could cause unexpected behavior in a multi-threaded environment.40. How does the as keyword work in C#?  The as keyword is used for safe type casting. It attempts to cast an object to a specified type and returns null if the cast fails instead of throwing an exception.41. Explain the role of the static keyword in C#.  The static keyword is used to declare a member that belongs to the type itself rather than to a specific instance of the type.42. What is the difference between a value type and a reference type in C#?  Value types store the actual data, while reference types store a reference to the memory location where the data is stored.43. How does the using statement work in C# for implementing IDisposable?  The using statement ensures that the Dispose method of an object that implements IDisposable is called, providing a convenient way to manage resources.44. Describe the concept of inversion of control (IoC) in C#.  Inversion of Control is a design principle where the control of the flow of a program is inverted, typically achieved through dependency injection.45. Explain the concept of extension methods in C#.  Extension methods allow adding new methods to existing types without modifying them, enhancing the functionality of types from external libraries.46. How does the try pattern work in C# 7.0 and later?  The try pattern in C# 7.0 and later allows for pattern matching within the catch block, enabling more expressive and concise exception handling.47. What is the purpose of the nameof operator in C#?  The nameof operator is used to obtain the simple (unqualified) string name of a variable, type, or member.48. How does the Expression class work in C#?  The Expression class in C# allows representing code as data, enabling operations like compiling and manipulating expressions at runtime.49. Explain the concept of the async/await pattern in C#.  The async/await pattern is used for asynchronous programming in C#, making it easier to write asynchronous code without using callbacks or blocking threads.50. Describe the role of the Thread class in C#.  The Thread class in C# is used to create and control threads, providing methods for starting, pausing, and terminating threads.What Next?These are just a few examples of C# interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your C# interviews!"
  },
  
  {
    "title": "C# | Dapper Using Stored Procedures",
    "url": "/posts/Dapper-Using-Stored-Procedures/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, dapper",
    "date": "2024-01-08 00:00:00 +0100",
    





    
    "snippet": "IntroductionDapper is a simple, lightweight Object-Relational Mapping (ORM) library for .NET. It is designed to provide high performance and reduce the overhead typically associated with traditiona...",
    "content": "IntroductionDapper is a simple, lightweight Object-Relational Mapping (ORM) library for .NET. It is designed to provide high performance and reduce the overhead typically associated with traditional ORMs. One of the powerful features of Dapper is its support for executing stored procedures. In this guide, we will explore how to use stored procedures in C# with Dapper.PrerequisitesBefore getting started, make sure you have the following installed:  Dapper NuGet package  SQL Server or another database with a stored procedure to work withExample: Basic Setupusing System;using System.Data;using System.Data.SqlClient;using Dapper;class Program{    static void Main()    {        // Connection string for your database        string connectionString = \"YourConnectionStringHere\";        using (IDbConnection dbConnection = new SqlConnection(connectionString))        {            // Example of calling a stored procedure with Dapper            var result = dbConnection.Query&lt;int&gt;(\"YourStoredProcedureName\", commandType: CommandType.StoredProcedure);                        // Process the result as needed            foreach (var value in result)            {                Console.WriteLine(value);            }        }    }}In this example, replace YourConnectionStringHere with your actual database connection string and YourStoredProcedureName with the name of your stored procedure.Example: Stored Procedure with Parametersusing System;using System.Data;using System.Data.SqlClient;using Dapper;class Program{    static void Main()    {        string connectionString = \"YourConnectionStringHere\";        using (IDbConnection dbConnection = new SqlConnection(connectionString))        {            // Parameters for the stored procedure            var parameters = new { Param1 = \"Value1\", Param2 = 42 };            // Example of calling a stored procedure with parameters using Dapper            var result = dbConnection.Query&lt;int&gt;(\"YourStoredProcedureName\", parameters, commandType: CommandType.StoredProcedure);                        foreach (var value in result)            {                Console.WriteLine(value);            }        }    }}In this example, define the parameters for your stored procedure and replace Value1 and 42 with the actual values.What Next?Dapper makes working with stored procedures in C# straightforward. It provides a clean and efficient way to interact with databases using a minimal amount of code. Experiment with the provided examples and adapt them to your specific use case to leverage the power of Dapper in your C# projects."
  },
  
  {
    "title": "C# | Dapper using Generic Repository",
    "url": "/posts/Dapper-using-Generic-Repository/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, dapper",
    "date": "2024-01-05 00:00:00 +0100",
    





    
    "snippet": "IntroductionDapper is a simple, lightweight, and high-performance Object-Relational Mapping (ORM) library for .NET. It is widely used for database access in C# applications due to its speed and sim...",
    "content": "IntroductionDapper is a simple, lightweight, and high-performance Object-Relational Mapping (ORM) library for .NET. It is widely used for database access in C# applications due to its speed and simplicity. In this guide, we will explore how to use Dapper in combination with a Generic Repository pattern to streamline database interactions.Setting Up DapperBefore using Dapper, make sure to install the Dapper NuGet package in your C# project. You can do this using the following command in the Package Manager Console:Install-Package DapperGeneric RepositoryA Generic Repository is a design pattern that provides a generic interface to interact with various types of entities in a consistent way. It allows you to perform common database operations (CRUD) without writing repetitive code for each entity.Let’s create a simple Generic Repository interface:public interface IRepository&lt;T&gt;{    IEnumerable&lt;T&gt; GetAll();    T GetById(int id);    void Insert(T entity);    void Update(T entity);    void Delete(int id);}Implementing the Generic Repository with DapperNow, let’s implement the Generic Repository using Dapper for a hypothetical Product entity.using Dapper;using System.Collections.Generic;using System.Data;using System.Data.SqlClient;using System.Linq;public class DapperRepository&lt;T&gt; : IRepository&lt;T&gt;{    private readonly IDbConnection _dbConnection;    public DapperRepository(string connectionString)    {        _dbConnection = new SqlConnection(connectionString);    }    public IEnumerable&lt;T&gt; GetAll()    {        return _dbConnection.Query&lt;T&gt;(\"SELECT * FROM \" + typeof(T).Name);    }    public T GetById(int id)    {        return _dbConnection.QueryFirstOrDefault&lt;T&gt;(\"SELECT * FROM \" + typeof(T).Name + \" WHERE Id = @Id\", new { Id = id });    }    public void Insert(T entity)    {        _dbConnection.Execute($\"INSERT INTO {typeof(T).Name} VALUES (@Property1, @Property2, ...)\", entity);    }    public void Update(T entity)    {        _dbConnection.Execute($\"UPDATE {typeof(T).Name} SET Property1 = @Property1, Property2 = @Property2, ... WHERE Id = @Id\", entity);    }    public void Delete(int id)    {        _dbConnection.Execute($\"DELETE FROM {typeof(T).Name} WHERE Id = @Id\", new { Id = id });    }}Example UsageNow, let’s see how to use the DapperRepository with a Product entity:public class Product{    public int Id { get; set; }    public string Name { get; set; }    public decimal Price { get; set; }}class Program{    static void Main()    {        var connectionString = \"your_database_connection_string\";        var productRepository = new DapperRepository&lt;Product&gt;(connectionString);        // Retrieve all products        var allProducts = productRepository.GetAll();        // Retrieve a product by Id        var productId = 1;        var product = productRepository.GetById(productId);        // Insert a new product        var newProduct = new Product { Name = \"New Product\", Price = 19.99 };        productRepository.Insert(newProduct);        // Update an existing product        product.Name = \"Updated Product\";        productRepository.Update(product);        // Delete a product        productRepository.Delete(productId);    }}This example demonstrates how to use Dapper with a Generic Repository for a simple Product entity. Adjust the code according to your specific entity and database schema.Make sure to replace \"your_database_connection_string\" with the actual connection string for your database."
  },
  
  {
    "title": "SCRUM | Interview Questions and Answers",
    "url": "/posts/Scrum-Interview-Questions-and-Answers/",
    "categories": "Best Practices, Project Management",
    "tags": "scrum, project, management, agile, questions, answers",
    "date": "2024-01-02 00:00:00 +0100",
    





    
    "snippet": "1. What is Scrum?Answer: Scrum is an Agile framework that provides a structured yet flexible way for teams to collaborate on complex projects. It emphasizes iterative development, continuous feedba...",
    "content": "1. What is Scrum?Answer: Scrum is an Agile framework that provides a structured yet flexible way for teams to collaborate on complex projects. It emphasizes iterative development, continuous feedback, and the delivery of a potentially shippable product at the end of each iteration, known as a Sprint.2. What are the key roles in Scrum?Answer: The key roles in Scrum are the Product Owner, Scrum Master, and Development Team. The Product Owner is responsible for defining and prioritizing the product backlog, the Scrum Master facilitates the Scrum process, and the Development Team is responsible for delivering the product increment.3. What is a Sprint in Scrum?Answer: A Sprint is a time-boxed iteration in Scrum, typically lasting 2 to 4 weeks, during which a potentially shippable product increment is created. Sprints help teams focus on delivering a specific set of features or user stories.4. Explain the concept of a Product Backlog.Answer: The Product Backlog is a prioritized list of all features, enhancements, and fixes that need to be addressed in the product. It is maintained by the Product Owner and evolves over time based on feedback and changing priorities.5. What is the purpose of the Daily Scrum?Answer: The Daily Scrum is a short, daily meeting where the Development Team discusses progress, challenges, and plans for the day. It ensures everyone is on the same page and promotes collaboration and problem-solving.6. How does Scrum handle changes in requirements?Answer: Scrum handles changes through the Product Backlog. The Product Owner can reprioritize items based on changing requirements, and new features or adjustments can be added during Sprint Planning or at any time.7. What is the role of the Scrum Master?Answer: The Scrum Master serves as a servant-leader, supporting the Scrum Team and ensuring that Scrum practices are followed. They facilitate meetings, remove impediments, and help the team continuously improve.8. What is a Sprint Review?Answer: A Sprint Review is a meeting at the end of each Sprint where the Development Team demonstrates the completed work to stakeholders. It provides an opportunity for feedback and ensures alignment with the product vision.9. How does Scrum address team collaboration?Answer: Scrum promotes collaboration through regular ceremonies like Sprint Planning, Daily Scrum, and Sprint Review. The emphasis on self-organizing teams fosters open communication, transparency, and collective ownership.10. Explain the concept of Velocity in Scrum.Answer: Velocity is a metric used in Scrum to measure the amount of work a team can complete in a Sprint. It helps with future Sprint planning by providing a reference for the team’s capacity.11. What is a Burndown Chart?Answer: A Burndown Chart visually represents the amount of work remaining in the Sprint. It helps the team track progress and identify any deviations from the planned trajectory.12. How does Scrum handle risks?Answer: Scrum addresses risks through transparency and adaptability. Regular inspections during Sprint Reviews and Retrospectives allow teams to identify and mitigate risks early in the development process.13. What is the Definition of Done?Answer: The Definition of Done is a set of criteria that must be met for a product increment to be considered complete. It ensures that the team delivers a high-quality, potentially shippable product at the end of each Sprint.14. Explain the term “Empirical Process Control” in Scrum.Answer: Empirical Process Control is a fundamental principle in Scrum. It involves making decisions based on observation, experimentation, and regular feedback. The three pillars supporting empirical process control are transparency, inspection, and adaptation.15. How does Scrum support continuous improvement?Answer: Scrum supports continuous improvement through the Sprint Retrospective. The team reflects on the previous Sprint, identifies areas for improvement, and implements changes in the upcoming Sprints.16. What is a User Story in Scrum?Answer: A User Story is a simple, concise description of a feature or functionality from an end user’s perspective. It typically follows the format “As a [user], I want [feature] so that [benefit].”17. What is the role of the Product Owner in Sprint Planning?Answer: The Product Owner is responsible for presenting the prioritized Product Backlog items during Sprint Planning and answering any questions the Development Team may have. They help the team understand the overall goals and vision.18. How does Scrum handle dependencies between teams?Answer: Scrum of Scrums is a technique used to address dependencies between multiple Scrum Teams. Representatives from each team meet regularly to discuss progress, challenges, and coordinate efforts.19. What is the purpose of the Sprint Retrospective?Answer: The Sprint Retrospective is a meeting held at the end of each Sprint where the team reflects on the Sprint process, identifies what went well and what could be improved, and plans actions for continuous improvement.20. How does Scrum support product quality?Answer: Scrum supports product quality through the Definition of Done and continuous inspection. Each increment must meet the defined quality criteria, and regular inspections during Sprints ensure that any quality issues are addressed promptly.21. Can Scrum be used for non-software projects?Answer: Yes, Scrum can be adapted for various types of projects beyond software development, such as marketing, research, and hardware development. The key is to embrace Scrum’s principles and tailor its practices to the specific context.22. What is the role of the Development Team in Sprint Planning?Answer: The Development Team in Sprint Planning collaborates with the Product Owner to understand and estimate the effort required for each Product Backlog item. They commit to delivering a specific set of items during the Sprint.23. Explain the concept of “Shippable Increment” in Scrum.Answer: A Shippable Increment is the product increment created by the Development Team during a Sprint. It is a potentially releasable version of the product that meets the Definition of Done and can be deployed to production if desired.24. How does Scrum handle conflicting priorities?Answer: The Product Owner is responsible for resolving conflicting priorities in the Product Backlog. They must prioritize items based on business value and collaborate with stakeholders to make informed decisions.25. What is the role of the Scrum Master during the Sprint?Answer: The Scrum Master supports the team during the Sprint by facilitating Scrum events, removing impediments, and fostering a positive and collaborative team environment. They ensure that Scrum processes are followed.26. How does Scrum address scope changes during a Sprint?Answer: Scope changes are generally avoided during a Sprint to maintain focus and stability. If absolutely necessary, the Product Owner and the team may negotiate and, if agreed upon, adjust the Sprint Backlog within the framework of the current Sprint goals.27. What is the purpose of the Sprint Goal?Answer: The Sprint Goal is a short statement that provides a clear objective for the Development Team during the Sprint. It helps guide the team’s work and decision-making to ensure a cohesive focus on delivering value.28. How does Scrum handle unplanned work or issues during a Sprint?Answer: Unplanned work or issues are addressed through collaboration within the team. The Development Team, along with the Product Owner and Scrum Master, may discuss and reprioritize the Sprint Backlog to accommodate the unexpected work.29. What is the difference between a Sprint Review and a Sprint Retrospective?Answer: The Sprint Review focuses on demonstrating the completed work to stakeholders and gathering feedback, while the Sprint Retrospective is a reflection on the Sprint process itself, aiming to identify areas for improvement and planning actions for the next Sprint.30. How does Scrum promote transparency?Answer: Scrum promotes transparency through open communication, visibility of work, and the use of artifacts like the Product Backlog, Sprint Backlog, and Burndown Charts. Regular ceremonies and collaboration ensure that information is accessible to all team members and stakeholders.What Next?These are just a few examples of SCRUM interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "React | Interview Questions and Answers",
    "url": "/posts/React-Interview-Questions-and-Answers/",
    "categories": "React, Interview",
    "tags": "frontend, react, questions, answers",
    "date": "2023-12-26 00:00:00 +0100",
    





    
    "snippet": "1. What is React?React is a JavaScript library for building user interfaces. It allows developers to create reusable UI components and manage the state of an application efficiently.2. Explain JSX ...",
    "content": "1. What is React?React is a JavaScript library for building user interfaces. It allows developers to create reusable UI components and manage the state of an application efficiently.2. Explain JSX in React.JSX (JavaScript XML) is a syntax extension for JavaScript used in React. It allows developers to write HTML elements and components in a syntax similar to XML or HTML. JSX gets compiled into JavaScript code that React can understand.Example:const element = &lt;h1&gt;Hello, React!&lt;/h1&gt;;3. What is the significance of virtual DOM in React?The virtual DOM (Document Object Model) is a lightweight copy of the actual DOM. React uses the virtual DOM to improve performance by minimizing the number of direct manipulations to the actual DOM. It calculates the difference between the virtual DOM and the real DOM, and only updates the parts that have changed, reducing the overall computational cost.4. Describe the component lifecycle in React.React components go through various lifecycle phases, including mounting, updating, and unmounting. The main lifecycle methods are:  componentDidMount: Invoked after a component is rendered for the first time.  componentDidUpdate: Called after a component is updated (re-rendered) due to changes in props or state.  componentWillUnmount: Invoked before a component is unmounted and destroyed.5. What is state in React?State is a JavaScript object that represents the current condition of a component. It influences the rendering of the component, and when the state changes, React re-renders the component. State should be treated as immutable to ensure predictable behavior.6. Explain the difference between props and state.      Props (Properties): Passed from parent components to child components. Props are immutable, and the child components cannot modify them.        State: Represents the internal state of a component. It is mutable and can be changed using setState().  7. What is the purpose of the setState method in React?The setState method is used to update the state of a React component. When setState is called, React re-renders the component, and any child components affected by the state change.Example:this.setState({ count: this.state.count + 1 });8. How does React Router work?React Router is a library for handling navigation in a React application. It uses a component-based approach, where different components are rendered based on the current URL. The &lt;BrowserRouter&gt; or &lt;HashRouter&gt; components are typically used to wrap the application and provide navigation functionality.9. What is a Higher-Order Component (HOC) in React?A Higher-Order Component is a function that takes a component and returns a new component with additional props, state, or behavior. HOCs are used for code reuse, logic abstraction, and adding features to components.Example:const withLogger = (WrappedComponent) =&gt; {  return class WithLogger extends React.Component {    // ... additional logic    render() {      return &lt;WrappedComponent {...this.props} /&gt;;    }  };};10. How does React handle forms?In React, forms are controlled components. The form elements, such as &lt;input&gt;, &lt;textarea&gt;, and &lt;select&gt;, maintain their state in the component’s state. The state is updated through the onChange event, and the component is re-rendered accordingly.Example:class MyForm extends React.Component {  constructor(props) {    super(props);    this.state = { inputValue: '' };  }  handleChange = (event) =&gt; {    this.setState({ inputValue: event.target.value });  }  render() {    return (      &lt;form&gt;        &lt;input type=\"text\" value={this.state.inputValue} onChange={this.handleChange} /&gt;      &lt;/form&gt;    );  }}What Next?These are just a few examples of React interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your React interviews!"
  },
  
  {
    "title": "Docker | Interview Questions and Answers",
    "url": "/posts/Docker-Interview-Questions-and-Answers/",
    "categories": "Docker, Interview",
    "tags": "docker, questions, answers",
    "date": "2023-12-23 00:00:00 +0100",
    





    
    "snippet": "1. What is Docker?Docker is a platform that enables developers to automate the deployment of applications inside lightweight, portable containers. Containers package an application and its dependen...",
    "content": "1. What is Docker?Docker is a platform that enables developers to automate the deployment of applications inside lightweight, portable containers. Containers package an application and its dependencies, ensuring consistency across different environments.2. How does Docker differ from virtualization?Docker uses containerization, which shares the host OS kernel and isolates applications at the user level. Virtualization, on the other hand, emulates an entire OS and runs multiple OS instances on a hypervisor.3. What is a Docker image?A Docker image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools.4. How do you create a Docker container?You create a Docker container by defining a Dockerfile, which includes instructions for building an image. Afterward, you use the docker build command to build the image and docker run to create and run the container.5. Explain the concept of a Dockerfile.A Dockerfile is a text document containing instructions for building a Docker image. It includes commands to copy files, set environment variables, and execute commands during the image-building process.6. What is the purpose of Docker Compose?Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to describe the services, networks, and volumes in a YAML file and then use docker-compose to start and manage the application.7. How do you link containers in Docker?Docker Compose facilitates linking containers by defining them within the same docker-compose.yml file. Containers can then communicate using the service names defined in the file.8. Explain the difference between an image and a container.An image is a static, immutable file that includes application code and its dependencies. A container is a running instance of an image, with its own isolated filesystem, network, and process space.9. What is Docker Swarm?Docker Swarm is a native clustering and orchestration solution for Docker. It allows you to create and manage a swarm of Docker nodes, turning them into a single, virtual Docker host.10. How do you persist data in Docker containers?Data persistence in Docker is achieved using volumes. Volumes are external storage entities mounted into containers, ensuring that data is retained even if the container is stopped or removed.11. Explain the role of Docker Hub.Docker Hub is a cloud-based registry service that enables users to share Docker images publicly or privately. It serves as a central repository for Docker images and facilitates collaboration among developers.12. How can you monitor Docker containers?Docker provides various tools for container monitoring, such as docker stats for basic container statistics, and third-party solutions like Prometheus or Grafana for more advanced monitoring and visualization.13. What is the purpose of the docker-compose up command?The docker-compose up command is used to start all the services defined in a docker-compose.yml file. It creates and runs the containers based on the specified configuration.14. How do you scale services in Docker Swarm?Scaling services in Docker Swarm is accomplished using the docker service scale command. It allows you to adjust the number of replicas for a service, distributing them across the swarm nodes.15. Explain Docker networking modes.Docker supports different networking modes, such as bridge, host, and overlay. Bridge mode creates an internal network for container communication, host mode uses the host’s network stack, and overlay enables communication across multiple Docker hosts.16. What is the purpose of the Dockerfile FROM instruction?The FROM instruction in a Dockerfile specifies the base image for the subsequent instructions. It sets the foundation for building the application image by pulling an existing image from a registry.17. How do you remove all Docker containers?You can remove all Docker containers using the command docker rm $(docker ps -aq). This command lists all container IDs and removes them.18. Explain the concept of Docker volumes.Docker volumes are a way to persistently store data outside of containers. They provide a mechanism for sharing data between containers and ensuring data durability across container restarts.19. What is the difference between a Docker image and a Docker container?A Docker image is a snapshot of a filesystem with application code and dependencies, while a Docker container is a running instance of that image. Containers are isolated and encapsulate the runtime environment.20. How can you pass environment variables to a Docker container?You can pass environment variables to a Docker container using the -e option with the docker run command. For example, docker run -e KEY=value my_container.21. What is the purpose of the docker inspect command?The docker inspect command provides detailed information about Docker objects like containers, images, volumes, and networks. It’s useful for troubleshooting and gathering information about Docker resources.22. How does Docker ensure isolation between containers?Docker uses containerization technology, which leverages namespaces and cgroups in the Linux kernel to provide process and filesystem isolation. This ensures that each container operates independently.23. Explain the concept of Docker registries.Docker registries are repositories for storing and distributing Docker images. Docker Hub is a public registry, and organizations often use private registries to manage and control access to their images.24. What is the significance of the ENTRYPOINT instruction in a Dockerfile?The ENTRYPOINT instruction in a Dockerfile specifies the command that will be executed when the container starts. It sets the primary executable for the container and cannot be overridden during docker run.25. How can you limit the resources a Docker container can use?Docker allows you to limit container resources using the --cpus and --memory options with the docker run command. These options restrict CPU and memory usage, respectively.26. Explain the difference between Docker volumes and bind mounts.Docker volumes are managed by Docker and offer data persistence, while bind mounts are linked to a host directory and allow sharing files between the host and the container.27. What is the purpose of the docker-compose down command?The docker-compose down command stops and removes containers, networks, and volumes defined in the docker-compose.yml file. It is the opposite of docker-compose up.28. How do you update a Docker service in Docker Swarm?You can update a Docker service in Docker Swarm using the docker service update command. It allows you to modify service parameters, such as the number of replicas or the image version.29. What is the significance of the HEALTHCHECK instruction in a Dockerfile?The HEALTHCHECK instruction defines a command to check the container’s health. It is used to determine if the container is running correctly and can influence decisions made by orchestration tools like Docker Swarm or Kubernetes.30. How can you secure Docker containers?To secure Docker containers, follow best practices such as using minimal base images, regularly updating images and dependencies, implementing least privilege principles, and monitoring container activity. Additionally, use Docker’s security featuresWhat Next?These are just a few examples of Docker interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "Kubernetes | Interview Questions and Answers",
    "url": "/posts/Kubernetes-Interview-Questions-and-Answers/",
    "categories": "Kubernetes, Interview",
    "tags": "microsoft, kubernetes, k8s, questions, answers",
    "date": "2023-12-19 00:00:00 +0100",
    





    
    "snippet": "1. What is Kubernetes?Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.2. Explain the key component...",
    "content": "1. What is Kubernetes?Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.2. Explain the key components of Kubernetes architecture.Kubernetes architecture consists of the following key components:  Master Node: Manages the control plane.  Node (Minion): Runs applications and services.  etcd: Distributed key-value store for cluster data.  Kubelet: Agent running on nodes.  Kube Proxy: Maintains network rules.3. What is a Pod in Kubernetes?A Pod is the smallest deployable unit in Kubernetes, representing a single instance of a running process. It can contain one or more containers that share the same network namespace.4. How does a ReplicaSet differ from a Deployment?A ReplicaSet ensures a specified number of replicas of a Pod are running, while a Deployment provides declarative updates to applications, managing ReplicaSets and allowing for rollbacks.5. Explain the purpose of a Kubernetes Service.A Service is an abstraction that defines a logical set of Pods and a policy to access them. It provides a stable endpoint for accessing the deployed application.6. What is the role of a ConfigMap in Kubernetes?ConfigMaps are used to decouple configuration artifacts from container images, enabling the modification of configuration without changing the application code.7. Describe the difference between a StatefulSet and a Deployment.A StatefulSet is used for stateful applications that require stable network identifiers and persistent storage. Deployments are more suitable for stateless applications.8. What is a Namespace in Kubernetes?A Namespace provides a way to divide cluster resources between multiple users or projects, preventing naming conflicts and providing a scope for resource quotas.9. Explain the concept of Ingress in Kubernetes.Ingress is an API object that manages external access to services within a cluster, providing HTTP and HTTPS routing rules.10. How does Horizontal Pod Autoscaling work?Horizontal Pod Autoscaling adjusts the number of replica Pods in a deployment or replica set based on observed CPU utilization or custom metrics.11. What is the role of kube-proxy?Kube-proxy is responsible for maintaining network rules on nodes, enabling communication across Pods and external traffic.12. How can you update a running Kubernetes application?You can update a running application in Kubernetes by modifying the container image in the Pod specification, then applying the changes using kubectl apply or by updating the Deployment.13. Explain Rolling Deployment in Kubernetes.Rolling Deployment is a strategy where a new version of an application is gradually deployed across Pods, minimizing downtime by replacing old Pods with new ones in a controlled manner.14. What is a Persistent Volume (PV) in Kubernetes?A Persistent Volume is a cluster-wide resource that represents a piece of networked storage, allowing data to persist beyond the lifespan of an individual Pod.15. How does a Helm chart simplify Kubernetes deployments?Helm is a package manager for Kubernetes that simplifies the deployment and management of applications by packaging them into charts, allowing for versioning and easy sharing.16. Explain the role of Taints and Tolerations in Kubernetes.Taints are applied to nodes, and tolerations are applied to Pods, allowing or preventing Pods from running on specific nodes based on matching taints and tolerations.17. What is the purpose of liveness and readiness probes in Kubernetes?Liveness probes determine if a container is running, and readiness probes determine if a container should receive traffic. They are used to enhance the reliability and availability of applications.18. How can you expose a Pod to the external world in Kubernetes?You can expose a Pod externally using a Service of type LoadBalancer or NodePort, depending on your infrastructure and requirements.19. Explain the concept of Helm Releases.Helm Releases represent a deployed instance of a Helm chart, allowing for easy management, versioning, and rollback of application releases.20. What is the purpose of a Job in Kubernetes?A Job in Kubernetes is used to run short-lived and parallel tasks to completion, ensuring that a specified number of Pods successfully terminate.21. How does Secrets work in Kubernetes?Secrets are used to store sensitive information, such as passwords or API keys, in a more secure manner, and they can be mounted as volumes or exposed as environment variables in Pods.22. What is the difference between a DaemonSet and a Deployment?A DaemonSet ensures that a copy of a Pod runs on all or some nodes, while a Deployment manages the deployment and scaling of Pods, typically for stateless applications.23. Explain the concept of Helm Values.Helm Values are configuration parameters that can be customized when deploying Helm charts, allowing for flexibility and abstraction of configuration details.24. How does Pod-to-Pod communication work in Kubernetes?Pods within the same Kubernetes cluster can communicate with each other directly using their IP addresses, and services provide a stable endpoint for communication between Pods.25. What is the purpose of Resource Quotas in Kubernetes?Resource Quotas limit the amount of CPU, memory, and other resources that can be consumed by objects within a namespace, preventing resource exhaustion.26. How can you scale a Kubernetes cluster?You can scale a Kubernetes cluster by adding or removing nodes. Tools like autoscaling groups in cloud providers can automatically adjust the number of nodes based on resource utilization.27. Explain the concept of Network Policies in Kubernetes.Network Policies allow you to control the communication between Pods, defining rules for ingress and egress traffic based on labels, ports, and IP ranges.28. What is Helm Chart Dependency?Helm Chart Dependency allows you to specify dependencies on other charts, simplifying the management of complex applications with multiple components.29. How does Kubernetes handle storage orchestration?Kubernetes uses Persistent Volumes and Persistent Volume Claims to abstract and manage storage resources, allowing Pods to request and use storage dynamically.30. What is Kubelet and its role in a Kubernetes cluster?Kubelet is an agent that runs on each node and ensures that containers are running in a Pod. It communicates with the API server and manages containers’ lifecycle.What Next?These are just a few examples of Kubernetes interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "C# | Understanding Generics",
    "url": "/posts/Understanding-Generics/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#",
    "date": "2023-12-15 00:00:00 +0100",
    





    
    "snippet": "Generics in C# provide a way to write flexible and reusable code by allowing the creation of classes, structures, interfaces, and methods with placeholders for data types. This enables the writing ...",
    "content": "Generics in C# provide a way to write flexible and reusable code by allowing the creation of classes, structures, interfaces, and methods with placeholders for data types. This enables the writing of code that works with any data type without sacrificing type safety.Generic ClassesA generic class is a template that can work with any data type. Here’s a simple example:public class Box&lt;T&gt;{    private T value;    public void Set(T newValue)    {        value = newValue;    }    public T Get()    {        return value;    }}// UsageBox&lt;int&gt; intBox = new Box&lt;int&gt;();intBox.Set(42);int intValue = intBox.Get();Box&lt;string&gt; stringBox = new Box&lt;string&gt;();stringBox.Set(\"Hello, Generics!\");string stringValue = stringBox.Get();In this example, the Box class is generic with the type parameter T. This allows creating instances of Box with different data types.Generic MethodsGeneric methods allow specifying the data type at the method level:public class Utility{    public static void Swap&lt;T&gt;(ref T a, ref T b)    {        T temp = a;        a = b;        b = temp;    }}// Usageint num1 = 5, num2 = 10;Utility.Swap(ref num1, ref num2);string str1 = \"Hello\", str2 = \"World\";Utility.Swap(ref str1, ref str2);In this example, the Swap method can be used to swap the values of variables of any data type.ConstraintsGenerics support constraints to restrict the types that can be used. For instance, you might want to ensure that a generic type implements a particular interface:public interface IShape{    double Area();}public class ShapeContainer&lt;T&gt; where T : IShape{    private T shape;    public ShapeContainer(T shape)    {        this.shape = shape;    }    public double GetArea()    {        return shape.Area();    }}// Usagepublic class Circle : IShape{    public double Radius { get; set; }    public double Area()    {        return Math.PI * Radius * Radius;    }}ShapeContainer&lt;Circle&gt; circleContainer = new ShapeContainer&lt;Circle&gt;(new Circle { Radius = 5.0 });double circleArea = circleContainer.GetArea();Here, the ShapeContainer class only accepts types that implement the IShape interface.What Next?Generics in C# are a powerful tool for creating flexible and reusable code, promoting type safety, and enhancing code maintainability."
  },
  
  {
    "title": "C# | Virtual Methods",
    "url": "/posts/Virtual-Methods/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#",
    "date": "2023-12-14 00:00:00 +0100",
    





    
    "snippet": "In C#, a virtual method is a method that can be overridden in derived classes. This allows for polymorphism, where a base class reference can be used to invoke methods on a derived class object.Syn...",
    "content": "In C#, a virtual method is a method that can be overridden in derived classes. This allows for polymorphism, where a base class reference can be used to invoke methods on a derived class object.Syntax:public class BaseClass{    public virtual void MyVirtualMethod()    {        // Base class implementation    }}public class DerivedClass : BaseClass{    public override void MyVirtualMethod()    {        // Derived class implementation    }}Example:Consider the following example:using System;public class Animal{    public virtual void MakeSound()    {        Console.WriteLine(\"Animal makes a generic sound\");    }}public class Dog : Animal{    public override void MakeSound()    {        Console.WriteLine(\"Dog barks\");    }}public class Cat : Animal{    public override void MakeSound()    {        Console.WriteLine(\"Cat meows\");    }}class Program{    static void Main()    {        Animal myDog = new Dog();        Animal myCat = new Cat();        myDog.MakeSound();  // Output: Dog barks        myCat.MakeSound();  // Output: Cat meows    }}In this example, the Animal class has a virtual method MakeSound(). The Dog and Cat classes override this method with their own implementations. When instances of Dog and Cat are assigned to Animal references, the overridden methods are called based on the actual object type, demonstrating polymorphism.What Next?Virtual methods provide a way to implement and leverage the concept of dynamic method dispatch in object-oriented programming."
  },
  
  {
    "title": "SQL | Interview Questions and Answers",
    "url": "/posts/SQL-Interview-Questions-and-Answers/",
    "categories": "SQL, Interview",
    "tags": "microsoft, sql, database, questions, answers",
    "date": "2023-12-10 00:00:00 +0100",
    





    
    "snippet": "1. What is SQL?SQL (Structured Query Language) is a programming language designed for managing and manipulating relational databases. It is used for tasks such as querying data, updating data, and ...",
    "content": "1. What is SQL?SQL (Structured Query Language) is a programming language designed for managing and manipulating relational databases. It is used for tasks such as querying data, updating data, and defining the structure of a database.2. What is the difference between SQL and NoSQL databases?SQL databases are relational databases that use structured query language for defining and manipulating the data. NoSQL databases, on the other hand, are non-relational databases that can handle unstructured or semi-structured data and do not require a fixed schema.3. What is a primary key?A primary key is a unique identifier for a record in a database table. It ensures that each record in a table can be uniquely identified.4. Explain the difference between INNER JOIN and LEFT JOIN.  INNER JOIN returns only the matching rows from both tables.  LEFT JOIN returns all rows from the left table and the matching rows from the right table. If there is no match, NULL values are returned for columns from the right table.5. What is normalization?Normalization is the process of organizing data in a database to eliminate redundancy and dependency. It involves dividing large tables into smaller tables and defining relationships between them.6. Write a SQL query to retrieve all employees from the “employees” table.SELECT * FROM employees;7. How do you add a new column to an existing table in SQL?ALTER TABLE table_nameADD COLUMN new_column_name data_type;8. Explain the difference between UNION and UNION ALL.  UNION combines the result sets of two or more SELECT statements and removes duplicate rows.  UNION ALL also combines result sets but includes all rows, including duplicates.9. What is a foreign key?A foreign key is a column or a set of columns in a table that refers to the primary key of another table. It establishes a link between the two tables.10. Write a SQL query to calculate the average salary from the “salaries” table.SELECT AVG(salary) FROM salaries;11. How do you delete a record from a table in SQL?DELETE FROM table_nameWHERE condition;12. Explain the ACID properties in the context of database transactions.ACID stands for Atomicity, Consistency, Isolation, and Durability. These properties ensure that database transactions are processed reliably.  Atomicity: Ensures that a transaction is treated as a single, indivisible unit.  Consistency: Ensures that a transaction brings the database from one valid state to another.  Isolation: Ensures that the concurrent execution of transactions does not result in data inconsistencies.  Durability: Ensures that once a transaction is committed, its changes are permanent.13. What is the purpose of the GROUP BY clause?The GROUP BY clause is used to group rows that have the same values in specified columns into summary rows. It is often used with aggregate functions like COUNT, SUM, AVG, etc.14. Write a SQL query to find the second-highest salary from the “employees” table.SELECT MAX(salary) FROM employeesWHERE salary &lt; (SELECT MAX(salary) FROM employees);15. How do you create an index in SQL?CREATE INDEX index_nameON table_name (column1, column2, ...);16. Explain the difference between a clustered index and a non-clustered index.  A clustered index determines the physical order of data in a table. There can be only one clustered index per table.  A non-clustered index does not affect the physical order of the table. Tables can have multiple non-clustered indexes.17. What is a stored procedure?A stored procedure is a precompiled collection of one or more SQL statements that can be executed as a single unit. It is stored in the database and can be called from applications or other stored procedures.18. Write a SQL query to find the total number of rows in a table.SELECT COUNT(*) FROM table_name;19. Explain the difference between a view and a table.  A table is a physical storage structure that holds data.  A view is a virtual table based on the result of a SELECT query. It does not store data itself but provides a way to represent the result of a query.20. How do you update data in a table using SQL?UPDATE table_nameSET column1 = value1, column2 = value2, ...WHERE condition;21. What is the purpose of the HAVING clause?The HAVING clause is used in conjunction with the GROUP BY clause to filter the results of a grouped query based on a specified condition.22. Write a SQL query to find the top N records from a table.SELECT * FROM table_nameLIMIT N;23. How do you perform a self-join in SQL?A self-join is a regular join, but the table is joined with itself. It is typically used when a table has a foreign key that references its own primary key.SELECT t1.column1, t2.column2FROM table_name t1JOIN table_name t2 ON t1.common_column = t2.common_column;24. Explain the purpose of the CASE statement in SQL.The CASE statement is used to perform conditional logic within a SQL query. It allows you to perform different actions based on different conditions.SELECT   column1,  CASE     WHEN condition1 THEN result1    WHEN condition2 THEN result2    ELSE result3  END AS new_columnFROM table_name;25. What is the difference between CHAR and VARCHAR data types?  CHAR is a fixed-length character data type.  VARCHAR is a variable-length character data type.26. Write a SQL query to find the third-highest salary from the “employees” table.SELECT MAX(salary) FROM employeesWHERE salary &lt; (SELECT MAX(salary) FROM employees WHERE salary &lt; (SELECT MAX(salary) FROM employees));27. What is a trigger in SQL?A trigger is a set of instructions that are automatically executed (or “triggered”) in response to specific events on a particular table or view. These events can include INSERT, UPDATE, DELETE operations.28. Explain the difference between a database and a schema.  A database is a container for tables and related objects.  A schema is a collection of database objects (tables, views, procedures) that are logically grouped together.29. How do you find duplicate rows in a table?SELECT column1, column2, COUNT(*)FROM table_nameGROUP BY column1, column2HAVING COUNT(*) &gt; 1;30. What is a subquery?A subquery is a query nested inside another query. It can be used to retrieve data that will be used in the main query as a condition to further restrict the data to be retrieved.SELECT column1FROM table_nameWHERE column2 IN (SELECT column2 FROM another_table WHERE condition);This concludes the list of SQL interview questions and answers. Feel free to use these questions as a study guide or reference for your SQL interviews.What Next?These are just a few examples of SQL interview questions. Depending on the level of expertise required, interview questions may vary from basic to advanced topics. Good luck with your interviews!"
  },
  
  {
    "title": "C# | Command Design Pattern",
    "url": "/posts/Command-Design-Pattern/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#",
    "date": "2023-12-06 00:00:00 +0100",
    





    
    "snippet": "The Command Pattern is a behavioral design pattern that turns a request into a standalone object. This object contains all the information about the request, allowing for parameterization of client...",
    "content": "The Command Pattern is a behavioral design pattern that turns a request into a standalone object. This object contains all the information about the request, allowing for parameterization of clients with different requests, queuing of requests, and logging of the operations. It also supports undoable operations.Key Components:  Command:          Defines an interface for executing a particular operation.      Typically includes an Execute method.        ConcreteCommand:          Implements the Command interface.      Holds the information about the operation to be executed.      Invokes the operation through a receiver.        Invoker:          Asks the command to execute the request.      Does not know how the request is handled.        Receiver:          Knows how to perform the operation.      Can be any object that can perform the requested action.        Client:          Creates a Command object and associates it with a Receiver.      Passes the command to the invoker.      Example in C#:Let’s consider a simple example of a remote control that can turn on and off devices:1. Command Interface:public interface ICommand{    void Execute();}2. Concrete Commands:public class LightOnCommand : ICommand{    private readonly Light _light;    public LightOnCommand(Light light)    {        _light = light;    }    public void Execute()    {        _light.TurnOn();    }}public class LightOffCommand : ICommand{    private readonly Light _light;    public LightOffCommand(Light light)    {        _light = light;    }    public void Execute()    {        _light.TurnOff();    }}3. Receiver:public class Light{    public void TurnOn()    {        Console.WriteLine(\"Light is ON\");    }    public void TurnOff()    {        Console.WriteLine(\"Light is OFF\");    }}4. Invoker:public class RemoteControl{    private ICommand _command;    public void SetCommand(ICommand command)    {        _command = command;    }    public void PressButton()    {        _command.Execute();    }}5. Client Code:class Program{    static void Main()    {        Light livingRoomLight = new Light();        ICommand livingRoomLightOn = new LightOnCommand(livingRoomLight);        ICommand livingRoomLightOff = new LightOffCommand(livingRoomLight);        RemoteControl remote = new RemoteControl();        remote.SetCommand(livingRoomLightOn);        remote.PressButton(); // Light is ON        remote.SetCommand(livingRoomLightOff);        remote.PressButton(); // Light is OFF    }}In this example, the RemoteControl acts as the invoker, and the commands (LightOnCommand and LightOffCommand) encapsulate the operations. The Light class is the receiver that knows how to perform the requested actions.What Next?The Command Pattern provides flexibility and decouples the sender and receiver of a request, making it easier to extend and maintain the code."
  },
  
  {
    "title": "C# | Use of the short Keyword",
    "url": "/posts/Use-of-the-short-Keyword/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#",
    "date": "2023-11-23 00:00:00 +0100",
    





    
    "snippet": "In C#, short is a keyword used to declare a 16-bit signed integer data type. It is a primitive data type that can store whole numbers in the range of -32,768 to 32,767.Syntaxshort variableName;Exam...",
    "content": "In C#, short is a keyword used to declare a 16-bit signed integer data type. It is a primitive data type that can store whole numbers in the range of -32,768 to 32,767.Syntaxshort variableName;Exampleusing System;class ShortExample{    static void Main()    {        // Declare a short variable        short myShort = 3000;        Console.WriteLine(\"Value of myShort: \" + myShort);        // Perform arithmetic operations        short result = (short)(myShort + 2000);        Console.WriteLine(\"Result after addition: \" + result);        // Overflow example        short maxShort = short.MaxValue;        Console.WriteLine(\"Max value of short: \" + maxShort);        // Overflow will occur        short overflowedResult = (short)(maxShort + 1);        Console.WriteLine(\"Overflowed result: \" + overflowedResult);    }}In the example above:  We declare a short variable named myShort and initialize it with the value 3000.  Perform addition on myShort and display the result.  Illustrate the concept of overflow by attempting to add 1 to the maximum value of short, resulting in an overflow.It’s important to note that when performing arithmetic operations that may lead to overflow or underflow, explicit casting is required to avoid compilation errors.Use Cases  When memory optimization is crucial, and the range of values to be stored is within the limits of a 16-bit signed integer.  Situations where the storage of larger integer values is not required, saving memory compared to int or long.What Next?In summary, the short keyword in C# is useful for scenarios where memory efficiency is a priority, and the range of values falls within the limits of a 16-bit signed integer."
  },
  
  {
    "title": "C# | Best Practices for Pagination using EF Core 8",
    "url": "/posts/Best-Practices-for-Pagination-using-EF-Core-8/",
    "categories": "C#, Entity Framework",
    "tags": "microsoft, csharp, c#, entityframework",
    "date": "2023-11-18 00:00:00 +0100",
    





    
    "snippet": "Pagination is a crucial aspect of application development, especially when dealing with large datasets. Entity Framework (EF) Core 8 in C# provides powerful features for implementing efficient pagi...",
    "content": "Pagination is a crucial aspect of application development, especially when dealing with large datasets. Entity Framework (EF) Core 8 in C# provides powerful features for implementing efficient pagination. In this guide, we’ll explore best practices for implementing pagination using EF Core 8, along with examples.1. Use Skip and Take for Simple PaginationEF Core provides the Skip and Take methods, which are essential for implementing pagination efficiently. Skip allows you to skip a specified number of rows, and Take limits the number of rows returned.var pageNumber = 1;var pageSize = 10;var result = dbContext.YourEntity    .OrderBy(e =&gt; e.SortingProperty)    .Skip((pageNumber - 1) * pageSize)    .Take(pageSize)    .ToList();In this example, pageNumber and pageSize determine the current page and the number of items per page, respectively.2. Use AsNoTracking for Read-Only OperationsFor read-only operations like fetching data for display purposes, consider using AsNoTracking to improve performance by avoiding the overhead of tracking changes.var result = dbContext.YourEntity    .AsNoTracking()    .OrderBy(e =&gt; e.SortingProperty)    .Skip((pageNumber - 1) * pageSize)    .Take(pageSize)    .ToList();This is particularly useful when you don’t intend to update or save changes to the entities retrieved.3. Leverage Indexed Columns for SortingEnsure that the columns used for sorting are indexed. Indexed columns significantly improve the performance of sorting operations.// Ensure SortingProperty is indexedmodelBuilder.Entity&lt;YourEntity&gt;()    .HasIndex(e =&gt; e.SortingProperty);Efficiently indexed columns will accelerate sorting and enhance overall pagination performance.4. Use Count for Total Record CountTo determine the total number of records without fetching all data, use Count before applying pagination. This avoids loading unnecessary data.var totalRecords = dbContext.YourEntity.Count();var result = dbContext.YourEntity    .OrderBy(e =&gt; e.SortingProperty)    .Skip((pageNumber - 1) * pageSize)    .Take(pageSize)    .ToList();5. Handle Concurrent Modifications with Take and SkipBe cautious when using Skip and Take for pagination in scenarios where data can be concurrently modified. In such cases, consider using alternative methods like keyset pagination for better consistency.What Next?Implementing pagination efficiently is crucial for enhancing the performance of applications dealing with large datasets. By following these best practices, you can ensure that your pagination logic is optimized and scalable when using EF Core 8 in C#."
  },
  
  {
    "title": "C# | var vs Explicit Type Declarations",
    "url": "/posts/var-vs-Explicit-Type-Declarations/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#",
    "date": "2023-11-12 00:00:00 +0100",
    





    
    "snippet": "In C#, developers have the option to use var for implicit type inference or explicitly declare the data type of a variable. Both approaches have their advantages and use cases. Let’s explore when t...",
    "content": "In C#, developers have the option to use var for implicit type inference or explicitly declare the data type of a variable. Both approaches have their advantages and use cases. Let’s explore when to use var and when to use explicit type declarations.var - Implicit Type InferenceThe var keyword was introduced in C# 3.0 and allows the compiler to infer the type of a variable based on the assigned value. It enhances code readability and can reduce redundancy. However, it’s essential to use var judiciously to maintain code clarity.Example:var name = \"John Doe\";var age = 25;var isStudent = true;// Compiler infers types: string, int, boolIn the above example, the types of name, age, and isStudent are inferred by the compiler based on the assigned values.Explicit Type DeclarationsExplicitly declaring the data type of a variable can be beneficial in certain scenarios, providing clarity to readers and preventing unintended type changes. It also helps when the initializer doesn’t make the type obvious.Example:string productName = \"Widget\";int quantity = 100;bool isAvailable = true;// Explicitly declaring types for clarityHere, the explicit type declarations make it clear that productName is a string, quantity is an integer, and isAvailable is a boolean.Guidelines for Choosing Between var and Explicit Types      Readability: Use var when the variable’s type is obvious from the assigned value, enhancing code readability.        Explicitness: Use explicit type declarations when clarity is crucial or when the initializer doesn’t clearly indicate the type.        Consistency: Maintain consistency within the codebase. Choose one approach and stick to it for a consistent coding style.        Complex Types: For complex types or when working with anonymous types, explicit type declarations are often necessary.  What Next?The decision to use var or explicit type declarations depends on the specific context and readability goals. Striking a balance between concise code and clarity ensures maintainable and understandable C# code."
  },
  
  {
    "title": "C# | Balancing Cross-Cutting Concerns in Clean Architecture",
    "url": "/posts/Balancing-Cross-Cutting-Concerns-in-Clean-Architecture/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, architecture",
    "date": "2023-11-09 00:00:00 +0100",
    





    
    "snippet": "IntroductionClean Architecture is an architectural pattern that promotes separation of concerns and maintainability in software development. However, managing cross-cutting concerns can be challeng...",
    "content": "IntroductionClean Architecture is an architectural pattern that promotes separation of concerns and maintainability in software development. However, managing cross-cutting concerns can be challenging in any architecture. This markdown file explores strategies for balancing cross-cutting concerns in Clean Architecture, with examples in C#.1. Understanding Cross-Cutting ConcernsCross-cutting concerns are aspects of a system that affect multiple modules or layers. Examples include logging, authentication, and error handling. In Clean Architecture, these concerns must be managed without compromising the integrity of the core business logic.2. Strategies for Balancing Cross-Cutting Concerns2.1 Dependency InjectionDependency injection is a key technique in Clean Architecture for managing cross-cutting concerns. By injecting dependencies, such as logging or authentication services, into the appropriate layers, you can achieve separation of concerns.Example in C#:public class SomeService{    private readonly ILogger _logger;    public SomeService(ILogger logger)    {        _logger = logger;    }    public void PerformAction()    {        _logger.Log(\"Performing action\");        // Business logic    }}2.2 Aspect-Oriented Programming (AOP)AOP allows you to modularize cross-cutting concerns, making it easier to maintain and manage them separately from the core business logic.Example in C#:[Log]public class SomeService{    public void PerformAction()    {        // Business logic    }}[AttributeUsage(AttributeTargets.Method)]public class LogAttribute : Attribute{    public void OnEntry()    {        // Logging logic    }}2.3 Middleware in Web ApplicationsFor web applications, middleware can be used to handle cross-cutting concerns in a modular and reusable way.Example in C# (ASP.NET Core):public class LoggingMiddleware{    private readonly RequestDelegate _next;    public LoggingMiddleware(RequestDelegate next)    {        _next = next;    }    public async Task Invoke(HttpContext context)    {        // Logging logic        await _next(context);    }}What Next?Balancing cross-cutting concerns in Clean Architecture is crucial for maintaining a modular and maintainable codebase. By using techniques like dependency injection, AOP, and middleware, you can achieve separation of concerns without sacrificing the integrity of your core business logic. Experiment with these strategies and choose the one that best fits the requirements of your project."
  },
  
  {
    "title": "C# | Building a Command-Line (CLI) App using System.CommandLine Library",
    "url": "/posts/Building-a-Command-Line-App-using-System.CommandLine-Library/",
    "categories": "C#, Development",
    "tags": "microsoft, csharp, c#, cli",
    "date": "2023-11-06 00:00:00 +0100",
    





    
    "snippet": "IntroductionIn this guide, we will explore how to build a Command-Line Interface (CLI) application using the System.CommandLine library in C# and .NET. System.CommandLine simplifies the process of ...",
    "content": "IntroductionIn this guide, we will explore how to build a Command-Line Interface (CLI) application using the System.CommandLine library in C# and .NET. System.CommandLine simplifies the process of creating robust and feature-rich command-line interfaces for your applications.PrerequisitesBefore getting started, make sure you have the following installed:  .NET SDK (version 5.0 or later)Step 1: Create a new Console Applicationdotnet new console -n MyCommandLineAppcd MyCommandLineAppStep 2: Add System.CommandLine NuGet Packagedotnet add package System.CommandLine --version 2.0.0-beta1.21308.1Step 3: Define Command-Line OptionsIn your Program.cs, define the command-line options using System.CommandLine:using System.CommandLine;using System.CommandLine.Invocation;class Program{    static int Main(string[] args)    {        var rootCommand = new RootCommand        {            new Option&lt;int&gt;(\"--number\", \"An integer option\"),            new Option&lt;bool&gt;(\"--flag\", \"A boolean option\"),            new Argument&lt;string&gt;(\"input\", \"A required input argument\")        };        rootCommand.Handler = CommandHandler.Create&lt;int, bool, string&gt;((number, flag, input) =&gt;        {            // Your application logic goes here            Console.WriteLine($\"Number: {number}\");            Console.WriteLine($\"Flag: {flag}\");            Console.WriteLine($\"Input: {input}\");        });        return rootCommand.Invoke(args);    }}Step 4: Run the CLI Appdotnet run -- --number 42 --flag true \"Hello, CLI!\"Replace the values with your own and see the output.Step 5: Customize Help TextAdd descriptions to your options and arguments for better help text:var rootCommand = new RootCommand{    new Option&lt;int&gt;(\"--number\", \"An integer option\"),    new Option&lt;bool&gt;(\"--flag\", \"A boolean option\"),    new Argument&lt;string&gt;(\"input\", \"A required input argument\")};rootCommand.Description = \"A simple CLI app\";rootCommand.Handler = CommandHandler.Create&lt;int, bool, string&gt;((number, flag, input) =&gt;{    Console.WriteLine($\"Number: {number}\");    Console.WriteLine($\"Flag: {flag}\");    Console.WriteLine($\"Input: {input}\");});What Next?You have successfully created a basic Command-Line Interface (CLI) application using the System.CommandLine library in C# and .NET. Customize and extend the application based on your specific requirements.For more information, refer to the official documentation: System.CommandLine GitHub"
  },
  
  {
    "title": "Azure | Azure API Management",
    "url": "/posts/Azure-API-Management/",
    "categories": "Azure, API Management",
    "tags": "microsoft, csharp, c#, Azure, api",
    "date": "2023-11-04 00:00:00 +0100",
    





    
    "snippet": "Azure API ManagementAzure API Management is a comprehensive solution for publishing, managing, securing, and analyzing APIs. It allows organizations to expose their services as APIs to internal and...",
    "content": "Azure API ManagementAzure API Management is a comprehensive solution for publishing, managing, securing, and analyzing APIs. It allows organizations to expose their services as APIs to internal and external developers, making it easier to consume and manage APIs efficiently. In this guide, we will explore the key concepts of Azure API Management along with examples.Key Concepts1. APIs :  APIs represent the services that you want to expose, whether they are web services, functions, or other endpoints.2. API Products :  API Products group APIs and define their usage policies. For example, you can create a “Basic” product with limited access and a “Premium” product with more features.3. Operations :  Operations define how to interact with an API, including HTTP methods (GET, POST, etc.) and request/response transformations.4. Policies :  Policies are rules applied to APIs or operations. They can include rate limiting, authentication, caching, and more.5. Developer Portal :  Azure API Management provides a developer portal where developers can discover and test your APIs.Getting Started      Create an Azure API Management Service: In the Azure Portal, create an API Management service.        Import or Create APIs: You can import APIs from existing services or create a new API within Azure API Management.        Configure API Products: Define API products with usage policies, like rate limits or subscription requirements.        Secure Your APIs: Implement authentication and authorization policies to secure your APIs.  Creating an APILet’s create a simple API for weather information.      Create an API: In Azure API Management, add a new API and define its backend URL, such as https://weatherapi.hbolajraf.com.        Define Operations: Add operations like GET /weather/{city} to retrieve weather information for a specific city.        Test in the Developer Portal: Use the developer portal to test the API operations.  Securing an APITo secure your API, you can require an API key or OAuth token.      Authentication: Configure policies to require API keys for access. You can generate API keys for developers in your developer portal.        OAuth 2.0: If you want to use OAuth 2.0, configure OAuth settings and allow developers to obtain tokens for your API.  Analyzing API UsageAzure API Management provides analytics and monitoring capabilities.      Analytics: Monitor API usage, error rates, and response times through Azure Monitor.        Reports: Access detailed reports on API traffic and usage patterns.        Alerts: Set up alerts to be notified when unusual activity occurs.  Use case exampleIn this example, we will demonstrate how to consume an API hosted in Azure API Management from a C# application.Prerequisites  Azure API Management instance set up with an API.  Visual Studio or any C# development environment.  Azure SDK for .NET installed.Step 1: Obtain API Management Subscription KeyBefore you can call an API in Azure API Management, you need to obtain a subscription key. Here’s how:      In the Azure Portal, navigate to your API Management instance.        Under the “APIs” section, select the API you want to access.        In the API’s settings, go to the “Security” tab.        Generate a new subscription key or use an existing one.  Step 2: Create a C# Console ApplicationCreate a new C# console application in your development environment.Step 3: Install Required PackagesIn your C# project, install the Microsoft.Azure.Management.ApiManagement and Microsoft.Azure.Management.ApiManagement.Fluent NuGet packages. These packages allow you to interact with Azure API Management.Install-Package Microsoft.Azure.Management.ApiManagementInstall-Package Microsoft.Azure.Management.ApiManagement.FluentStep 4: Write C# Code to Consume the APIHere is a sample C# code that demonstrates how to consume the weather API from Azure API Management using the subscription key obtained in Step 1:using Microsoft.Azure.Management.ApiManagement;using Microsoft.Azure.Management.ApiManagement.Models;using Microsoft.Azure.Management.Fluent;using Microsoft.Azure.Management.ResourceManager.Fluent;using Microsoft.Azure.Services.AppAuthentication;using Microsoft.Rest.Azure.Authentication;using System;class Program{    static async System.Threading.Tasks.Task Main(string[] args)    {        string clientId = \"your-client-id\";        string clientSecret = \"your-client-secret\";        string tenantId = \"your-tenant-id\";        string subscriptionId = \"your-subscription-id\";        string resourceGroupName = \"your-resource-group\";        string serviceName = \"your-api-management-service\";        string apiKey = \"your-subscription-key\";                var serviceClientCredentials = ApplicationTokenProvider.LoginSilentAsync(            tenantId, clientId, clientSecret).Result;        var azure = Azure.Authenticate(serviceClientCredentials).WithSubscription(subscriptionId);        var apiManagementClient = new ApiManagementClient(serviceClientCredentials)        {            SubscriptionId = subscriptionId        };        var apiManagement = await azure.ApiManagementServices.GetByResourceGroupAsync(resourceGroupName, serviceName);        var baseUri = $\"https://weatherapi.hbolajraf.com\";        var operationResponse = await apiManagementClient.HttpOperations.ListByServiceAsync(resourceGroupName, serviceName);        if (operationResponse != null)        {            var operation = operationResponse.First();            var request = new HttpRequestMessage            {                Method = HttpMethod.Get,                RequestUri = new Uri(new Uri(baseUri), operation.UrlTemplate),            };            request.Headers.Add(\"Ocp-Apim-Subscription-Key\", apiKey);            using (var response = await httpClient.SendAsync(request))            {                var content = await response.Content.ReadAsStringAsync();                Console.WriteLine($\"Response: {content}\");            }        }    }}Replace the placeholders ( your-client-id, your-client-secret, your-tenant-id, your-subscription-id, etc.) with your actual Azure and API Management service details.Step 5: Run the C# ApplicationBuild and run your C# application. It will call the API hosted in Azure API Management using the subscription key and display the response.What Next?  Azure API Management simplifies the process of publishing, managing, and securing APIs. It offers a developer-friendly experience with the developer portal and extensive analytics to help you make data-driven decisions about your APIs.The Use Case example demonstrates how to consume an API in Azure API Management using a C# application. You can adapt this code to fit your specific use case and integrate it into your applications as needed."
  },
  
  {
    "title": "Docker | Tips and Tricks",
    "url": "/posts/Docker-Tips-and-Tricks/",
    "categories": "Docker, Tips And Tricks",
    "tags": "microsoft, docker, tips&tricks",
    "date": "2023-10-26 00:00:00 +0200",
    





    
    "snippet": "Docker is a powerful tool for containerization, allowing you to package and run applications with their dependencies in isolated containers. Here are some tips and tricks for using Docker effective...",
    "content": "Docker is a powerful tool for containerization, allowing you to package and run applications with their dependencies in isolated containers. Here are some tips and tricks for using Docker effectively.Installation and Setup1. Install Docker  To get started, install Docker by following the official installation instructions for your operating system on the Docker website.2. Use Docker Compose  Docker Compose is a tool for defining and running multi-container Docker applications. It simplifies the process of managing complex applications with multiple containers.Basic Docker Commands3. Pull an Image  Use docker pull to download Docker images from a registry. For example, docker pull ubuntu will pull the Ubuntu image.4. List Images  To list all downloaded images, use docker images or docker image ls.5. Run a Container  Start a new container with docker run. For example, docker run -it ubuntu bash runs an interactive Ubuntu container.6. Attach to a Running Container  To attach to a running container, use docker exec -it &lt;container_name&gt; bash.7. Stop and Remove Containers  Use docker stop &lt;container_id&gt; to stop a running container. To remove a stopped container, use docker rm &lt;container_id&gt;.8. View Container Logs  View container logs with docker logs &lt;container_id&gt;.9. Naming Containers  When running containers, provide a --name flag to give them human-readable names.Advanced Docker Commands10. Build a Docker Image  Create a Docker image from a Dockerfile using docker build. For example, docker build -t my-image:1.0 . builds an image from the current directory.11. Docker Registry Login  Log in to a Docker registry using docker login. This is necessary for pushing images to a private registry.12. Push Images to a Registry  Push your Docker images to a registry with docker push. For example, docker push my-image:1.0 pushes an image to the registry.13. Docker Network  Create custom Docker networks to connect containers. Use docker network create to create a network and --network to specify it when running containers.14. Volume Mounting  Share data between your host and container by using volume mounts with the -v or --volume flag. For example, docker run -v /host/path:/container/path.15. Docker Compose for Multi-Container Apps  Use a docker-compose.yml file to define and run multi-container applications. Run them with docker-compose up.Docker Security16. Limit Container Capabilities  Reduce a container’s capabilities by using the --cap-drop and --cap-add flags in the docker run command.17. Scan Images for Vulnerabilities  Utilize tools like Clair or Trivy to scan your Docker images for known vulnerabilities before deploying them.18. Regularly Update Images  Keep your base images up to date, as they may contain security patches. Use the latest base images from the official repositories.Docker Cleanup19. Remove Dangling Images  Remove unused images with docker image prune.20. Clean Up Containers  Remove all stopped containers with docker container prune.What Next?  Remember to consult the Docker documentation and community resources for additional tips and best practices when working with Docker containers."
  },
  
  {
    "title": "GitHub | Using Copilot with Visual Studio Code",
    "url": "/posts/GitHub-Using-Copilot-with-Visual-Studio-Code/",
    "categories": "GitHub, Copilot",
    "tags": "microsoft, github, copilot, zipkin",
    "date": "2023-10-15 00:00:00 +0200",
    





    
    "snippet": "Using GitHub Copilot with Visual Studio CodeGitHub Copilot is an AI-powered pair programmer developed by GitHub and OpenAI. It helps developers write code faster by providing suggestions and auto-c...",
    "content": "Using GitHub Copilot with Visual Studio CodeGitHub Copilot is an AI-powered pair programmer developed by GitHub and OpenAI. It helps developers write code faster by providing suggestions and auto-completions. In this guide, we’ll explore how to set up GitHub Copilot with Visual Studio Code and provide some examples of how to use it effectively.PrerequisitesBefore you start, ensure you have the following:  Visual Studio Code installed.  A GitHub Copilot subscription or access to the GitHub Copilot technical preview.Installing GitHub Copilot  Open Visual Studio Code.  Go to the Extensions view by clicking on the square icon on the sidebar or using Ctrl+Shift+X.  Search for “GitHub Copilot” and click “Install” next to the GitHub Copilot extension.  Follow the on-screen instructions to set up GitHub Copilot.Using GitHub CopilotGitHub Copilot works alongside you as you write code, providing code completions and suggestions. Here are some key features:Code CompletionsAs you type, GitHub Copilot suggests code completions based on what you’re writing. You can accept suggestions by pressing Tab or Enter.Code SuggestionsGitHub Copilot offers helpful code suggestions when you’re stuck or need guidance. You can use the /// trigger to see suggestions for comments.ExamplesExample 1: Basic Code CompletionsLet’s say you’re writing a Python function to calculate the square of a number. With GitHub Copilot installed, as you start typing, it provides code completions.# Start typing a functiondef calculate_square(number):GitHub Copilot will suggest completing the function like this:# Copilot suggestiondef calculate_square(number):    \"\"\"Calculate the square of a number.\"\"\"    return number ** 2Simply accept the suggestion by pressing Tab, and you have a complete function to calculate the square of a number.Example 2: Code Suggestions for CommentsGitHub Copilot is great at helping with code comments. If you’re documenting a function, use the /// trigger to get suggestions.def divide_numbers(a, b):    ///GitHub Copilot will provide a comment suggestion like this:def divide_numbers(a, b):    \"\"\"Divide two numbers and return the result.\"\"\"What Next?By accepting the suggestion, you can quickly add descriptive comments to your code.GitHub Copilot is a powerful tool that can save you time and help you write high-quality code more efficiently. Experiment with it and let it assist you in various programming languages and scenarios."
  },
  
  {
    "title": "Azure | Build .Net IoT App using C#",
    "url": "/posts/Azure-Build-.Net-IoT-App-using-C/",
    "categories": "Azure, IoT",
    "tags": "microsoft, csharp, c#, azure, iot",
    "date": "2023-10-04 00:00:00 +0200",
    





    
    "snippet": "Build .Net IoT App using C#Azure IoT provides a robust platform for building Internet of Things (IoT) solutions. Here are some C# examples to help you get started with Azure IoT services.Azure IoT ...",
    "content": "Build .Net IoT App using C#Azure IoT provides a robust platform for building Internet of Things (IoT) solutions. Here are some C# examples to help you get started with Azure IoT services.Azure IoT HubAzure IoT Hub is a fully managed service that enables reliable and secure communication between IoT devices and the cloud. Here’s how to interact with it using C#:Send Telemetry Datausing Microsoft.Azure.Devices.Client;using System.Text;using System.Threading.Tasks;string deviceConnectionString = \"Your Device Connection String\";using var deviceClient = DeviceClient.CreateFromConnectionString(deviceConnectionString, TransportType.Mqtt);var telemetryData = new{    temperature = 25.5,    humidity = 60};var messageString = JsonConvert.SerializeObject(telemetryData);var message = new Message(Encoding.UTF8.GetBytes(messageString));await deviceClient.SendEventAsync(message);Receive Cloud-to-Device Messagesusing Microsoft.Azure.Devices.Client;using System.Text;using System.Threading.Tasks;string deviceConnectionString = \"Your Device Connection String\";using var deviceClient = DeviceClient.CreateFromConnectionString(deviceConnectionString, TransportType.Mqtt);Message receivedMessage = await deviceClient.ReceiveAsync();if (receivedMessage != null){    var messageData = Encoding.ASCII.GetString(receivedMessage.GetBytes());    Console.WriteLine($\"Received message: {messageData}\");    await deviceClient.CompleteAsync(receivedMessage);}Azure IoT Device Provisioning ServiceAzure IoT Device Provisioning Service (DPS) simplifies the initial setup of IoT devices. Here’s how to use it with C#:Register a Device with DPSusing Microsoft.Azure.Devices.Provisioning.Service;using Microsoft.Azure.Devices.Shared;using System.Threading.Tasks;string idScope = \"Your DPS ID Scope\";string registrationId = \"Your Device Registration ID\";string primaryKey = \"Your Device Primary Key\";var provisioningServiceClient = ProvisioningServiceClient.CreateFromConnectionString($\"HostName={idScope};SharedAccessKeyName=provisioningserviceowner;SharedAccessKey={primaryKey}\");var individualEnrollment = new IndividualEnrollment(registrationId){    Attestation = new TpmAttestation(),    ProvisioningStatus = ProvisioningStatus.Enabled,    DeviceID = \"Your Device ID\"};await provisioningServiceClient.CreateOrUpdateIndividualEnrollmentAsync(individualEnrollment);Azure IoT EdgeAzure IoT Edge extends IoT Hub to edge devices. You can run code and manage devices on the edge. Here’s how to get started with C#:Create an IoT Edge Moduleusing Microsoft.Azure.Devices;using Microsoft.Azure.Devices.Edge.Agent.Core;using Microsoft.Azure.Devices.Edge.ModuleUtil;string connectionString = \"Your IoT Hub Connection String\";string deviceId = \"Your Device ID\";string moduleId = \"Your Module ID\";var edgeAgentModule = new DockerModule(    \"mcr.microsoft.com/azureiotedge-agent:1.0\",    Constants.EdgeAgentModuleName,    Constants.EdgeRuntimeContainerName,    new DockerConfig(\"linux/amd64\", new DockerLoggingConfig(), new DockerRestartPolicy(0, \"never\")), null);var deploymentConfig = new DeploymentConfig(\"1.0\");await ModuleUtil.DeployModuleAsync(connectionString, deviceId, moduleId, edgeAgentModule, deploymentConfig);What Next?These examples cover some of the essential tasks you can perform using C# with Azure IoT services. For more advanced scenarios and detailed documentation, refer to the Azure IoT documentation.Make sure to replace placeholders like “Your Device Connection String” or “Your DPS ID Scope” with your actual Azure IoT service information."
  },
  
  {
    "title": "Git | Tips and Tricks",
    "url": "/posts/Git-Tips-and-Tricks/",
    "categories": "GitHub, Git",
    "tags": "git, github, sourcecontrol",
    "date": "2023-10-01 00:00:00 +0200",
    





    
    "snippet": "Git Tips and TricksGit is a powerful version control system that can make your development workflow more efficient. Here are some tips and tricks to help you get the most out of Git.Configure GitBe...",
    "content": "Git Tips and TricksGit is a powerful version control system that can make your development workflow more efficient. Here are some tips and tricks to help you get the most out of Git.Configure GitBefore you start using Git, it’s a good idea to configure it with your name and email address. This information will be associated with your commits.git config --global user.name \"hbolajraf\"git config --global user.email \"hassan.bolajraf@gmail.com\"You can also set other configurations, such as your preferred text editor and default branch.Basic Commands1. Initialize a Repository: To start a new Git repository, use git init in your project directory.2. Clone a Repository: To clone a repository from a remote URL, use git clone &lt;URL&gt;.3. Commit Changes: After making changes, use git commit -m \"Your commit message\" to save them.4. Check the Status: Use git status to see the status of your working directory.BranchingBranches are essential for managing different lines of development.1. Create a Branch: Use git branch &lt;branch_name&gt; to create a new branch.2. Switch Branches: To switch to a different branch, use git checkout &lt;branch_name&gt;.3. Merge Branches: Merge changes from one branch into another with git merge &lt;branch_name&gt;.4. Delete Branch: Use git branch -d &lt;branch_name&gt; to delete a branch.StashingStashing is useful when you need to save your changes temporarily.1. Stash Changes: Use git stash to save your changes.2. Apply Stash: To reapply your changes, use git stash apply.3. List Stashes: See a list of stashes with git stash list.Interactive RebaseInteractive rebase allows you to modify commit history.1. Rebase Interactive: Use git rebase -i HEAD~n to interactively rebase the last n commits.2. Edit Commits: Change “pick” to “edit” to modify a commit.3. Amend Commits: Use git commit --amend to edit the current commit.Git AliasesGit aliases let you create shortcuts for Git commands.1. Create an Alias: Add an alias to your global Git configuration.git config --global alias.co checkout2. Usage: Now, you can use git co as a shorthand for git checkout.Git HooksGit hooks are scripts that run automatically on certain Git events.1. Pre-Commit Hook: Create a .git/hooks/pre-commit script to run actions before a commit.2. Post-Receive Hook: In a server’s Git repository, create a hooks/post-receive script to perform actions after receiving a push.Ignoring FilesYou can specify files or patterns to ignore using a .gitignore file.1. Create .gitignore: Create a file named .gitignore and list the files, directories, or patterns you want to ignore.2. Example .gitignore:# Ignore build artifactsbin/obj/# Ignore log files*.log# Ignore a specific directorydocs/What Next?These tips and tricks will help you become more proficient with Git, making your version control tasks more efficient and your development process smoother."
  },
  
  {
    "title": "C# | Configure QoS within API Gateway using ocelot and Polly",
    "url": "/posts/Configure-QoS-within-API-Gateway-using-ocelot-and-Polly/",
    "categories": "C#, Web API",
    "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, polly, webapi, c#, gateway, qos",
    "date": "2023-09-27 00:00:00 +0200",
    





    
    "snippet": "IntroductionAPI Gateway is an entry point for backend application. It maintains routing, authentication, logging, service discovery etc. Ocelot is used to design and develop API gateway for .net ba...",
    "content": "IntroductionAPI Gateway is an entry point for backend application. It maintains routing, authentication, logging, service discovery etc. Ocelot is used to design and develop API gateway for .net based application. QoS is generally configured in API gateway which provides different priorities for different applications, users or traffic. In this article, we will configure and discuss Quality of Services (QoS) using ocelot and Polly on ASP.NET Core web API project.What is Quality of Service (QoS)?QoS provides different priorities to different applications, users or data flow. We have already mentioned, Ocelot is used to design API Gateway and Ocelot uses Polly to achieve QoS.The QoSOptions node contains three important properties.      ExceptionsAllowedBeforeBreakingThis value must greater than 0. It means that the circuit breaker will break after a certain number of exceptions occur. For example:        DurationOfBreakThis value specifies how long the circuit breaker will stay open after it is tripped. The unit of this value is milliseconds. For example: 5000 means 5 seconds        TimeoutValueThis value specifies that a request will automatically be timed out if it takes more than this value. The unit of this value is milliseconds as well. For example: 3000 means 3 seconds.  Tools and Technology used  Visual Studio 2022  .NET 6.0  In Memory Database  Entity Framework  ASP.NET Core Web API  C#  Ocelot and  MMLib.SwaggerForOcelotImplementationStep 1: Create solution and projects.  Create a solution name QoS.  Add 2 new web api projects, name – Catalog.API, BFF.WebHere, BFF.Web project will act as API Gateway.Step 2: Install nuget packages.  Install following nuget packages in Catalog.API    PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemory    PM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServer    PM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools    PM&gt; Install-Package Microsoft.VisualStudio.Web.CodeGeneration.Design      Install following nuget packages in BFF.Web    PM&gt; Install-Package MMLib.SwaggerForOcelot    PM&gt; Install-Package Ocelot    PM&gt; Install-Package Ocelot.Provider.PollyStep 3: Organize Catalog.API  Create CatalogItem model in Model folderCatalogItem.cs    using System.ComponentModel.DataAnnotations;    using System.ComponentModel.DataAnnotations.Schema;        namespace Catalog.API.Model    {        public class CatalogItem        {            [Key]            [DatabaseGenerated(DatabaseGeneratedOption.Identity)]            public int Id { get; set; }            public string Name { get; set; }                public string Description { get; set; }                public decimal Price { get; set; }                public int AvailableStock { get; set; }                public int RestockThreshold { get; set; }        }    }  Create DbContext class as CatalogContext in Db folderCatalogContext.cs    using Catalog.API.Model;    using Microsoft.EntityFrameworkCore;        namespace Catalog.API.Db    {        public class CatalogContext : DbContext        {            public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options)            {                }                protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)            {                base.OnConfiguring(optionsBuilder);            }                public DbSet&lt;CatalogItem&gt; CatalogItems { get; set; }        }    }  Create SeedDataProvider class in Db folderSeedDataProvider.cs    using Catalog.API.Model;        namespace Catalog.API.Db    {        public class SeedDataProvider        {            public static void Initialize(CatalogContext catalogContext)            {                if(!catalogContext.CatalogItems.Any())                {                    var catalogs = new List&lt;CatalogItem&gt;                    {                        new CatalogItem                        {                            Name = \"T-Shirt\",                            Description = \"Cats Eye T-Shirt\",                            Price = 1000,                            AvailableStock = 100,                            RestockThreshold = 10                        },                            new CatalogItem                        {                            Name = \"Samsung Mobile\",                            Description = \"Samsung A30\",                            Price = 30000,                            AvailableStock = 100,                            RestockThreshold = 5                        },                            new CatalogItem                        {                            Name = \"Meril Beauty Soap\",                            Description = \"Beauty Soap\",                            Price = 40,                            AvailableStock = 500,                            RestockThreshold = 20                        }                    };                        catalogContext.CatalogItems.AddRange(catalogs);                    catalogContext.SaveChanges();                }            }        }    }  Modify Program class as follows.Program.cs    using Catalog.API.Db;    using Microsoft.EntityFrameworkCore;        var builder = WebApplication.CreateBuilder(args);        // Add services to the container.        builder.Services.AddControllers();        builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));        // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();    builder.Services.AddSwaggerGen();        var app = builder.Build();        // For Seed data generation    using(var scope = app.Services.CreateScope())    {        var services = scope.ServiceProvider;        var db = services.GetRequiredService&lt;CatalogContext&gt;();        SeedDataProvider.Initialize(db);    }        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();    }        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();Here, the following line is used to configure in memory database    builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));The following code snippet is used to initialize seed data    using(var scope = app.Services.CreateScope())    {        var services = scope.ServiceProvider;        var db = services.GetRequiredService&lt;CatalogContext&gt;();        SeedDataProvider.Initialize(db);    }  Add CatalogItemsController class in Controllers folder as follows.CatalogItemsController.cs    using Catalog.API.Db;    using Catalog.API.Model;    using Microsoft.AspNetCore.Mvc;    using Microsoft.EntityFrameworkCore;        namespace Catalog.API.Controllers    {        [Route(\"api/[controller]\")]        [ApiController]        public class CatalogItemsController : ControllerBase        {            private readonly CatalogContext _context;            private static int _count = 0;                public CatalogItemsController(CatalogContext context)            {                _context = context;            }                // GET: api/CatalogItems            [HttpGet]            public async Task&lt;ActionResult&lt;IEnumerable&lt;CatalogItem&gt;&gt;&gt; GetCatalogItems()            {                _count++;                if(_count &lt;= 3)                {                    // Sleep for 4 seconds                    Thread.Sleep(4000);                }                    if (_context.CatalogItems == null)                {                    return NotFound();                }                return await _context.CatalogItems.ToListAsync();            }                // GET: api/CatalogItems/5            [HttpGet(\"{id}\")]            public async Task&lt;ActionResult&lt;CatalogItem&gt;&gt; GetCatalogItem(int id)            {                if (_context.CatalogItems == null)                {                    return NotFound();                }                var catalogItem = await _context.CatalogItems.FindAsync(id);                    if (catalogItem == null)                {                    return NotFound();                }                    return catalogItem;            }                // PUT: api/CatalogItems/5            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPut(\"{id}\")]            public async Task&lt;IActionResult&gt; PutCatalogItem(int id, CatalogItem catalogItem)            {                if (id != catalogItem.Id)                {                    return BadRequest();                }                    _context.Entry(catalogItem).State = EntityState.Modified;                    try                {                    await _context.SaveChangesAsync();                }                catch (DbUpdateConcurrencyException)                {                    if (!CatalogItemExists(id))                    {                        return NotFound();                    }                    else                    {                        throw;                    }                }                    return NoContent();            }                // POST: api/CatalogItems            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPost]            public async Task&lt;ActionResult&lt;CatalogItem&gt;&gt; PostCatalogItem(CatalogItem catalogItem)            {                if (_context.CatalogItems == null)                {                    return Problem(\"Entity set 'CatalogContext.CatalogItems'  is null.\");                }                _context.CatalogItems.Add(catalogItem);                await _context.SaveChangesAsync();                    return CreatedAtAction(\"GetCatalogItem\", new { id = catalogItem.Id }, catalogItem);            }                // DELETE: api/CatalogItems/5            [HttpDelete(\"{id}\")]            public async Task&lt;IActionResult&gt; DeleteCatalogItem(int id)            {                if (_context.CatalogItems == null)                {                    return NotFound();                }                var catalogItem = await _context.CatalogItems.FindAsync(id);                if (catalogItem == null)                {                    return NotFound();                }                    _context.CatalogItems.Remove(catalogItem);                await _context.SaveChangesAsync();                    return NoContent();            }                private bool CatalogItemExists(int id)            {                return (_context.CatalogItems?.Any(e =&gt; e.Id == id)).GetValueOrDefault();            }        }    }Step 4: Organize BFF.WebIn this stage, we are going to configure a gateway to communicate with other services using ocelot.      Create a folder name - Routes.dev in root directory and add the following files. ocelot.catalog-api.json, ocelot.global.json, ocelot.SwaggerEndPoints.json in Routes.dev folder.        Now modify the json files as follows.  ocelot.catalog-api.json    {      \"Routes\": [        {          \"DownstreamPathTemplate\": \"/{everything}\",          \"DownstreamScheme\": \"https\",          \"SwaggerKey\": \"catalog-api\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": \"7282\"            }          ],          \"UpstreamPathTemplate\": \"/catalog/{everything}\",          \"UpstreamHttpMethod\": [            \"GET\",            \"POST\",            \"PUT\",            \"DELETE\"          ],              \"QoSOptions\": {            \"ExceptionsAllowedBeforeBreaking\": 2,            \"DurationOfBreak\": 5000,            \"TimeoutValue\": 3000          }            }      ]    }QoSOptions section of the above file basically configure QoS for Catalog service. The above configuration means that if the server does not response for 3 minutes, it will throw timeout exception. If the server throws two exceptions, it will not be accessible for five minutes.ocelot.global.json    {      \"GlobalConfiguration\": {        \"BaseUrl\": \"https://localhost:7205\"        //\"ServiceDiscoveryProvider\": {            //  \"Host\": \"localhost\",        //  \"Port\": 7205        //}      }    }ocelot.SwaggerEndPoints.json    {      \"SwaggerEndPoints\": [        {          \"Key\": \"bffweb\",          \"TransformByOcelotConfig\": false,          \"Config\": [            {              \"Name\": \"BFF.Web\",              \"Version\": \"1.0\",              \"Url\": \"https://localhost:7205/swagger/v1/swagger.json\"            }          ]        },            {          \"Key\": \"catalog-api\",          \"TransformByOcelotConfig\": true,          \"Config\": [            {              \"Name\": \"Catalog.API\",              \"Version\": \"1.0\",              \"Url\": \"https://localhost:7205/catalog/swagger/v1/swagger.json\"            }          ]        }      ]    }Note: I have added the following code block in CatalogItemController to produce timeout manually.            [HttpGet]            public async Task&lt;ActionResult&lt;IEnumerable&lt;CatalogItem&gt;&gt;&gt; GetCatalogItems()            {                _count++;                if(_count &lt;= 3)                {                    // Sleep for 4 seconds                    Thread.Sleep(4000);                }                    if (_context.CatalogItems == null)                {                    return NotFound();                }                return await _context.CatalogItems.ToListAsync();            }Step 5: Run and test the applicationNow run both web projects. In the BFF.Web, select Catalog.API-1.0 from swagger definition (“Select a definition on the top right corner”) and execute the api (CatalogItems) as follows.When we visit the first time (or quickly second time), it tells us that circuit is breaking for 5000 ms. Look at the console of BFF.Web.Then, the second time (quickly) it tells us that the circuit is open, and we cannot visit the service for 3 seconds as follows.After 3 seconds, the service is accessible. If you execute now, you will see the output like below.Source code"
  },
  
  {
    "title": "C# | Search AD entry by ObjectSid using Novell Directory Ldap Nuget package",
    "url": "/posts/Search-AD-entry-by-ObjectSid-using-Novell-Directory-Ldap-Nuget-package/",
    "categories": "C#, Active Directory",
    "tags": "ldap, microsoft, c#, dotnet, activedirectory",
    "date": "2023-09-25 00:00:00 +0200",
    





    
    "snippet": "SID stands for security identifier, a unique string that Windows Server automatically assigns to each computer, user and group in order to mark and clearly distinguish them.Windows SID Format :SIDs...",
    "content": "SID stands for security identifier, a unique string that Windows Server automatically assigns to each computer, user and group in order to mark and clearly distinguish them.Windows SID Format :SIDs always follow the same structure, with values separated by dashes:Novell Directory LdapNovell Directory Ldap nuget package allows you to connect to the active directory in order to perform CRUD actions on all AD objects, among other Users Groups and computers. it support both platforms windows and linux, so even if you are under kubernetes  clusters on docker containers you will be able to manage AD objects.C# code practical caseThis exemple is based on a project that was created based on .Net 6 under Visual studio 2022 and with a docker support to manage Ad Objects within a linux docker container.The nuget package version that was used are the following :&lt;PackageReference Include=\"Novell.Directory.Ldap.NETStandard\" Version=\"3.6.0\" /&gt;When we will process a search within the AD for a dedicated entry we will use a string ObjectSid instead of a bytes value. To do that we will create a helper method that will return the string value based on the input ObjectSid bytes value as bellow :1. Add Helper method to convert ObjectSID Byte value in string format public static string ConvertSidByteValueToStringValue(Byte[] sidBytes)        {            short sSubAuthorityCount = 0;            StringBuilder strSid = new StringBuilder();            strSid.Append(\"S-\");            try            {                // Add SID revision.                strSid.Append(sidBytes[0].ToString());                sSubAuthorityCount = Convert.ToInt16(sidBytes[1]);                // Next six bytes are SID authority value.                if (sidBytes[2] != 0 || sidBytes[3] != 0)                {                    string strAuth = String.Format(\"0x{0:2x}{1:2x}{2:2x}{3:2x}{4:2x}{5:2x}\",                                   (Int16)sidBytes[2],                                   (Int16)sidBytes[3],                                   (Int16)sidBytes[4],                                   (Int16)sidBytes[5],                                   (Int16)sidBytes[6],                                   (Int16)sidBytes[7]);                    strSid.Append(\"-\");                    strSid.Append(strAuth);                }                else                {                    Int64 iVal = sidBytes[7] +                         (sidBytes[6] &lt;&lt; 8) +                         (sidBytes[5] &lt;&lt; 16) +                         (sidBytes[4] &lt;&lt; 24);                    strSid.Append(\"-\");                    strSid.Append(iVal.ToString());                }                // Get sub authority count...                int idxAuth = 0;                for (int i = 0; i &lt; sSubAuthorityCount; i++)                {                    idxAuth = 8 + i * 4;                    UInt32 iSubAuth = BitConverter.ToUInt32(sidBytes, idxAuth);                    strSid.Append(\"-\");                    strSid.Append(iSubAuth.ToString());                }            }            catch (Exception ex)            {                Trace.TraceWarning(ex.Message);                throw;            }            return strSid.ToString();        }This method will get ObjectSid information parts from the array of Bytes input value as bellow :byte[0]      - Revision Levelbyte[1]      - count of Sub-Authoritiesbyte[2 - 7]  - 48 bit Authority(big-endian)byte[8 +]    - n Sub-Authorities, 32 bits And after that we return a String ObjectSID on the format bellow :S-{Revision}-{Authority}-{ SubAuthority1}-{ SubAuthority2}...-{ SubAuthorityN}2. Call Active Directory to retrieve User informations based on objectSid string valueAdding Novell directiveusing Novell.Directory.Ldap;.....Adding the search method to get the User entry object/// Exemple of Dn Value  : OU=User,OU=Accounts,DC=USA,DC=NY        /// Exemple of objectSidStringValue Value  : S-1-5-15-420314761-778715008-4547327-1845947        /// Exemple of objectCategory Value  : User or Group or Computer        /// Exemple of OutputProps Values : \"description\",\"lastLogon\",\"email\",\"name\", \"login\"        public List&lt;Attributes&lt;string, string&gt;&gt;? GetLdapEntryByObjectSid(string Dn, string objectSidStringValue, string objectCategory, string[] OutputProps)        {            var ldapConnectionOptions = new LdapConnectionOptions();            ldapConnectionOptions.UseSsl();            var ldapConx = new LdapConnection(ldapConnectionOptions);            List&lt;Attributes&lt;string, string&gt;&gt; listAttributes = new List&lt;Attributes&lt;string, string&gt;&gt;();            string Filter = $\"(&amp;(objectSid={objectSidStringValue})(objectCategory={objectCategory}))\";                        if (!string.IsNullOrEmpty(Filter))            {                var searchQuery = ldapConx.Search(Dn, Novell.Directory.Ldap.LdapConnection.ScopeSub, Filter, OutputProps, false);                if (searchQuery != null)                {                    while (searchQuery.HasMore())                    {                        LdapEntry nextEntry = null;                            nextEntry = searchQuery.Next();                            //SID Case                             if (OutputProps.Contains(\"objectSid\"))                            {                                //Get Sid Property Bytes value to be converted into string clear value                                var objectSid = nextEntry.GetAttributeSet().FirstOrDefault(e =&gt; e.Key.Equals(\"objectSid\"));                                if (objectSid.Value != null)                                {                                    var sidStringFormat = ConvertSidByteValueToStringValue(objectSid.Value.ByteValue);                                    listAttributes.Add(new Attributes&lt;string, string&gt;(objectSid.Key, sidStringFormat));                                }                            }                            listAttributes.AddRange(nextEntry.GetAttributeSet()                                .Where(e =&gt; !e.Key.Equals(\"objectSid\"))                                .Select(e =&gt; new Attributes&lt;string, string&gt;(e.Key, e.Value.StringValue))                                .ToList());                                           }                }            }            return listAttributes;        }The GetLdapEntryByObjectSid method will process a searchQuery within ldap based on the objectSid filter and also the objectCategory in this example User filter. As mentioned the objectSid within Active directory are persisted as a byte so we will call our Helper method to convert it into a string value: var sidStringFormat = ConvertSidByteValueToStringValue(objectSid.Value.ByteValue);The returned list of properties will contains all properties as string readable values, so we can use them also to process a new search based on other property.Links : Novell Directory - GitHub repositoryNovell Directory - Nuget package"
  },
  
  {
    "title": "C# | Configure Swagger on api gateway using ocelot in asp.net core application",
    "url": "/posts/Configure-Swagger-For-Ocelot-Gateway/",
    "categories": "C#, Web API",
    "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, webapi, c#, gateway, swagger",
    "date": "2023-09-24 00:00:00 +0200",
    





    
    "snippet": "IntroductionSwagger configuration on API gateway is not as simple as you are configure normal application. You have to configure it in different way. In this article I will create an API gateway us...",
    "content": "IntroductionSwagger configuration on API gateway is not as simple as you are configure normal application. You have to configure it in different way. In this article I will create an API gateway using ocelot and asp.net core application and show you how to configure swagger on API gateway.Tools and technologies used  Visual Studio 2022  .NET 6.0  In Memory Database  Entity Framework  ASP.NET Core Web API  C#  Ocelot and  MMLib.SwaggerForOcelotImplementationStep 1: Create solution and projects.  Create a solution name APIGateway  Add 4 new web api project, name - Catalog.API, Location.API, Ordering.API and BFF.Web in the solution.Here, BFF.Web project will act as API Gateway.Step 2: Install nuget packages.  Install following nuget package in Catalog.API Project    PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools        Install following nuget package in Ordering.API Project    PM&gt; Install-Package Microsoft.EntityFrameworkCorePM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools        Install following nuget packages in BFF.Web Project    PM&gt; Install-Package OcelotPM&gt; Install-Package Ocelot.Provider.PollyPM&gt; Install-Package Ocelot.Cache.CacheManagerPM&gt; Install-Package MMLib.SwaggerForOcelot      Step 3: Organize Catalog.API Project  Create a Product model class in Catalog.API/Model folderProduct.cs    using System.ComponentModel.DataAnnotations;    using System.ComponentModel.DataAnnotations.Schema;        namespace Catalog.API.Model    {        public class Product        {            [Key]            [DatabaseGenerated(DatabaseGeneratedOption.Identity)]            public int Id { get; set; }            public string Name { get; set; }                public string Description { get; set; }                public decimal Price { get; set; }                public int AvailableStock { get; set; }                public int RestockThreshold { get; set; }        }    }  Create a CatalogContext class in Catalog.API/Db folderCatalogContext.cs    using Catalog.API.Model;    using Microsoft.EntityFrameworkCore;        namespace Catalog.API.Db    {        public class CatalogContext : DbContext        {            public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options)            {                }                protected override void OnModelCreating(ModelBuilder modelBuilder)            {                }                protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)            {                base.OnConfiguring(optionsBuilder);            }                public DbSet&lt;Product&gt; Products { get; set; }        }    }  Modify Program.cs file as follows    using Catalog.API.Db;    using Microsoft.EntityFrameworkCore;        var builder = WebApplication.CreateBuilder(args);        // Add services to the container.        builder.Services.AddControllers();        builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));        // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();    builder.Services.AddSwaggerGen();        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();    }        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();  Create a conroller class name ProductsController in Catalog.API/Controllers folderCatalogController.cs    using System;    using System.Collections.Generic;    using System.Linq;    using System.Threading.Tasks;    using Microsoft.AspNetCore.Http;    using Microsoft.AspNetCore.Mvc;    using Microsoft.EntityFrameworkCore;    using Catalog.API.Db;    using Catalog.API.Model;        namespace Catalog.API.Controllers    {        [Route(\"api/[controller]\")]        [ApiController]        public class ProductsController : ControllerBase        {            private readonly CatalogContext _context;                public ProductsController(CatalogContext context)            {                _context = context;            }                // GET: api/Products            [HttpGet(\"GetAll\")]            public async Task&lt;ActionResult&lt;IEnumerable&lt;Product&gt;&gt;&gt; GetProducts()            {                return await _context.Products.ToListAsync();            }                // GET: api/Products/5            [HttpGet(\"{id}\")]            public async Task&lt;ActionResult&lt;Product&gt;&gt; GetProduct(int id)            {                var product = await _context.Products.FindAsync(id);                    if (product == null)                {                    return NotFound();                }                    return product;            }                // PUT: api/Products/5            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPut(\"Edit/{id}\")]            public async Task&lt;IActionResult&gt; PutProduct(int id, Product product)            {                if (id != product.Id)                {                    return BadRequest();                }                    _context.Entry(product).State = EntityState.Modified;                    try                {                    await _context.SaveChangesAsync();                }                catch (DbUpdateConcurrencyException)                {                    if (!ProductExists(id))                    {                        return NotFound();                    }                    else                    {                        throw;                    }                }                    return NoContent();            }                // POST: api/Products            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPost(\"Add\")]            public async Task&lt;ActionResult&lt;Product&gt;&gt; PostProduct(Product product)            {                _context.Products.Add(product);                await _context.SaveChangesAsync();                    return CreatedAtAction(\"GetProduct\", new { id = product.Id }, product);            }                // DELETE: api/Products/5            [HttpDelete(\"Delete/{id}\")]            public async Task&lt;IActionResult&gt; DeleteProduct(int id)            {                var product = await _context.Products.FindAsync(id);                if (product == null)                {                    return NotFound();                }                    _context.Products.Remove(product);                await _context.SaveChangesAsync();                    return NoContent();            }                private bool ProductExists(int id)            {                return _context.Products.Any(e =&gt; e.Id == id);            }        }    }Step 4: Organize Ordering.API Project  Create a Order model class in Ordering.API/Model folderOrder.cs    namespace Ordering.API.Models    {        public class Order        {            public int Id { get; set; }            public string Address { get; set; }                public DateTime OrderDate { get; set; }                public string Comments { get; set; }        }    }  Create a OrderingContext class in Ordering.API/Db folderOrderingContext.cs    using Microsoft.EntityFrameworkCore;    using Ordering.API.Models;        namespace Ordering.API.Db    {        public class OrderingContext : DbContext        {            public OrderingContext(DbContextOptions&lt;OrderingContext&gt; options) : base(options)            {                }            public DbSet&lt;Ordering.API.Models.Order&gt; Order { get; set; }        }    }  Modify Program.cs file as follows    using Microsoft.EntityFrameworkCore;    using Ordering.API.Db;        var builder = WebApplication.CreateBuilder(args);        // Add services to the container.        builder.Services.AddControllers();        builder.Services.AddDbContext&lt;OrderingContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));        // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();    builder.Services.AddSwaggerGen();        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();    }        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();      Create a conroller class name OrdersController in Ordering.API/Controllers folderOrdersController.cs    using System;    using System.Collections.Generic;    using System.Linq;    using System.Threading.Tasks;    using Microsoft.AspNetCore.Http;    using Microsoft.AspNetCore.Mvc;    using Microsoft.EntityFrameworkCore;    using Ordering.API.Db;    using Ordering.API.Models;        namespace Ordering.API.Controllers    {        [Route(\"api/[controller]\")]        [ApiController]        public class OrdersController : ControllerBase        {            private readonly OrderingContext _context;                public OrdersController(OrderingContext context)            {                _context = context;            }                // GET: api/Orders            [HttpGet(\"GetAll\")]            public async Task&lt;ActionResult&lt;IEnumerable&lt;Order&gt;&gt;&gt; GetOrder()            {                return await _context.Order.ToListAsync();            }                // GET: api/Orders/5            [HttpGet(\"{id}\")]            public async Task&lt;ActionResult&lt;Order&gt;&gt; GetOrder(int id)            {                var order = await _context.Order.FindAsync(id);                    if (order == null)                {                    return NotFound();                }                    return order;            }                // PUT: api/Orders/5            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPut(\"Edit/{id}\")]            public async Task&lt;IActionResult&gt; PutOrder(int id, Order order)            {                if (id != order.Id)                {                    return BadRequest();                }                    _context.Entry(order).State = EntityState.Modified;                    try                {                    await _context.SaveChangesAsync();                }                catch (DbUpdateConcurrencyException)                {                    if (!OrderExists(id))                    {                        return NotFound();                    }                    else                    {                        throw;                    }                }                    return NoContent();            }                // POST: api/Orders            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPost(\"Add\")]            public async Task&lt;ActionResult&lt;Order&gt;&gt; PostOrder(Order order)            {                _context.Order.Add(order);                await _context.SaveChangesAsync();                    return CreatedAtAction(\"GetOrder\", new { id = order.Id }, order);            }                // DELETE: api/Orders/5            [HttpDelete(\"Delete/{id}\")]            public async Task&lt;IActionResult&gt; DeleteOrder(int id)            {                var order = await _context.Order.FindAsync(id);                if (order == null)                {                    return NotFound();                }                    _context.Order.Remove(order);                await _context.SaveChangesAsync();                    return NoContent();            }                private bool OrderExists(int id)            {                return _context.Order.Any(e =&gt; e.Id == id);            }        }    }Step 5: Organize Location.API Project  Create CountriesController in Location.API/Controllers folder    using Microsoft.AspNetCore.Mvc;        namespace Location.API.Controllers    {        [ApiController]        [Route(\"api/[controller]\")]        public class CountriesController : ControllerBase        {          [HttpGet(\"GetAll\")]          public IEnumerable&lt;string&gt; Get()            {                return new string[] {\"America\",\"Bangladesh\", \"Canada\" };            }        }    }Step 6: Organize BFF.Web (API Gateway) Project  Create a folder name Routes and add the following files in that folderocelot.catalog.api.json    {      \"Routes\": [        {          \"DownstreamPathTemplate\": \"/{everything}\",          \"DownstreamScheme\": \"https\",          \"SwaggerKey\": \"catalog\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": \"7282\"            }          ],          \"UpstreamPathTemplate\": \"/catalog/{everything}\",          \"UpstreamHttpMethod\": [            \"GET\",            \"POST\",            \"PUT\",            \"DELETE\"          ]        }      ]    }ocelot.location.api.json    {      \"Routes\": [        {          \"DownstreamPathTemplate\": \"/{everything}\",          \"DownstreamScheme\": \"https\",          \"SwaggerKey\": \"location\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": \"7003\"            }          ],          \"UpstreamPathTemplate\": \"/location/{everything}\",          \"UpstreamHttpMethod\": [            \"GET\",            \"POST\",            \"PUT\",            \"DELETE\"          ]        }      ]    }ocelot.ordering.api.json    {      \"Routes\": [        {          \"DownstreamPathTemplate\": \"/{everything}\",          \"DownstreamScheme\": \"https\",          \"SwaggerKey\": \"ordering\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": \"7126\"            }          ],          \"UpstreamPathTemplate\": \"/ordering/{everything}\",          \"UpstreamHttpMethod\": [            \"GET\",            \"POST\",            \"PUT\",            \"DELETE\"          ]        }      ]    }ocelot.global.json    {      \"GlobalConfiguration\": {        \"BaseUrl\": \"http://localhost:5205\"      }    }ocelot.SwaggerEndPoints.json    {      \"SwaggerEndPoints\": [        {          \"Key\": \"bffweb\",          \"TransformByOcelotConfig\": false,          \"Config\": [            {              \"Name\": \"BFF.Web\",              \"Version\": \"1.0\",              \"Url\": \"http://localhost:5205/swagger/v1/swagger.json\"            }          ]        },        {          \"Key\": \"location\",          \"TransformByOcelotConfig\": true,          \"Config\": [            {              \"Name\": \"Location.API\",              \"Version\": \"1.0\",              \"Url\": \"http://localhost:5205/location/swagger/v1/swagger.json\"            }          ]        },        {          \"Key\": \"catalog\",          \"TransformByOcelotConfig\": true,          \"Config\": [            {              \"Name\": \"Catalog.API\",              \"Version\": \"1.0\",              \"Url\": \"http://localhost:5205/catalog/swagger/v1/swagger.json\"            }          ]        },        {          \"Key\": \"ordering\",          \"TransformByOcelotConfig\": true,          \"Config\": [            {              \"Name\": \"Ordering.API\",              \"Version\": \"1.0\",              \"Url\": \"http://localhost:5205/catalog/swagger/v1/swagger.json\"            }          ]        }      ]    }  Add AlterUpstream Class in Config FolderAlterUpstream.cs    using Newtonsoft.Json;    using Newtonsoft.Json.Linq;        namespace BFF.Web.Config    {        public class AlterUpstream        {            public static string AlterUpstreamSwaggerJson(HttpContext context, string swaggerJson)            {                var swagger = JObject.Parse(swaggerJson);                // ... alter upstream json                return swagger.ToString(Formatting.Indented);            }        }    }  Modify Program.cs file as followsProgram.cs    using BFF.Web.Config;    using MMLib.SwaggerForOcelot.DependencyInjection;    using Ocelot.DependencyInjection;    using Ocelot.Middleware;    using Ocelot.Provider.Polly;        var builder = WebApplication.CreateBuilder(args);        var routes = \"Routes\";        builder.Configuration.AddOcelotWithSwaggerSupport(options =&gt;    {        options.Folder = routes;    });        builder.Services.AddOcelot(builder.Configuration).AddPolly();    builder.Services.AddSwaggerForOcelot(builder.Configuration);        var environment = Environment.GetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\");    builder.Configuration.SetBasePath(Directory.GetCurrentDirectory())        .AddOcelot(routes, builder.Environment)        .AddEnvironmentVariables();            // Add services to the container.        builder.Services.AddControllers();    // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();        // Swagger for ocelot    builder.Services.AddSwaggerGen();        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();    }            app.UseHttpsRedirection();        app.UseAuthorization();        app.UseSwaggerForOcelotUI(options =&gt;    {        options.PathToSwaggerGenerator = \"/swagger/docs\";        options.ReConfigureUpstreamSwaggerJson = AlterUpstream.AlterUpstreamSwaggerJson;        }).UseOcelot().Wait();        app.MapControllers();        app.Run();Step 7: Run and Test application  Now run multiple (all) projects and test application using postman.  Check all end point using api gateway and swagger using the following URLhttps://localhost:7205/swagger/index.html  Select Swagger definition from top right corner of BFFSource code"
  },
  
  {
    "title": "Azure DevOps | Running JMeter Test Collection using JMeter Docker Image",
    "url": "/posts/Azure-DevOps-Running-JMeter-Test-Collection-using-JMeter-Docker-Image/",
    "categories": "Azure DevOps, JMeter",
    "tags": "microsoft, azuredevops, azure, jmeter, docker",
    "date": "2023-09-18 00:00:00 +0200",
    





    
    "snippet": "Running JMeter Test Collection using JMeter Docker ImageJMeter is a popular open-source tool for performance testing and load testing of web applications. Running JMeter tests using Docker is a con...",
    "content": "Running JMeter Test Collection using JMeter Docker ImageJMeter is a popular open-source tool for performance testing and load testing of web applications. Running JMeter tests using Docker is a convenient way to ensure consistency and isolation. In this guide, we’ll show you how to run a JMeter test collection using a JMeter Docker image.PrerequisitesBefore you begin, ensure that you have the following prerequisites:  Docker installed on your machine.Steps      Pull the JMeter Docker Image:    You can pull the official JMeter Docker image from Docker Hub:    docker pull justb4/jmeter:latest        This will download the latest JMeter Docker image to your local machine.        Prepare Your JMeter Test Collection:    Create a directory that contains your JMeter test plan files (.jmx). You can organize your collection of test plans in this directory.        Run JMeter Tests using the Docker Image:    You can run your JMeter test collection by mounting the test plan directory to the Docker container and specifying the JMX file to execute.    Replace YOUR_TEST_DIRECTORY and YOUR_TEST_FILE.jmx with your actual test directory and JMX file.    docker run -it --rm -v /path/to/YOUR_TEST_DIRECTORY:/mnt/jmeter -w /mnt/jmeter justb4/jmeter -n -t /mnt/jmeter/YOUR_TEST_FILE.jmx              -it - Runs the container in interactive mode.      --rm - Removes the container when it stops.      -v /path/to/YOUR_TEST_DIRECTORY:/mnt/jmeter - Mounts your test directory to the container at /mnt/jmeter.      -w /mnt/jmeter - Sets the working directory to /mnt/jmeter within the container.      justb4/jmeter - Specifies the JMeter Docker image.      -n - Runs JMeter in non-GUI mode.      -t /mnt/jmeter/YOUR_TEST_FILE.jmx - Specifies the JMX file to execute.            View Test Results:    After the test is completed, you can view the results in the console output. You can also configure JMeter to save the test results in various formats, such as CSV or XML, by adding appropriate listeners to your JMX file.  What Next?Running JMeter test collections using a Docker image simplifies the setup and execution process, making it easier to perform load testing and performance testing on your web applications. You can easily automate and integrate this process into your CI/CD pipelines for continuous performance monitoring.Remember to replace YOUR_TEST_DIRECTORY and YOUR_TEST_FILE.jmx with your actual test collection directory and JMX file, and customize any additional parameters as needed for your specific testing requirements."
  },
  
  {
    "title": "C# | Building GraphQL APIs in C#",
    "url": "/posts/C-GraphQL-in-.NET-Core-and-C/",
    "categories": "C#, Web API",
    "tags": "microsoft, csharp, c#, webapi, graphql",
    "date": "2023-09-16 00:00:00 +0200",
    





    
    "snippet": "What is GraphQL?GraphQL is a query language for APIs that was developed by Facebook. It allows clients to request only the data they need and nothing more. Unlike traditional REST APIs, where the s...",
    "content": "What is GraphQL?GraphQL is a query language for APIs that was developed by Facebook. It allows clients to request only the data they need and nothing more. Unlike traditional REST APIs, where the server determines the structure and format of the response, GraphQL puts the power in the hands of the client to specify the shape and depth of the data.Key Concepts1. SchemaA GraphQL schema defines the types of data that can be queried and the relationships between them. It serves as a contract between the client and the server.2. TypesTypes represent the structure of the data in GraphQL. There are two main types:      Scalar Types: These are atomic types like Int, Float, String, Boolean, and ID.        Object Types: These are user-defined types that represent complex objects with fields.  3. QueryA GraphQL query is a request for specific data from the server. It resembles the shape of the data it expects to receive. Queries are hierarchical and match the structure of the GraphQL schema.4. MutationWhile queries are used to read data, mutations are used to modify or create data on the server. They are similar to queries but are used for write operations.GraphQL in C# with HotChocolateHotChocolate is a popular GraphQL server implementation for .NET. It allows you to easily integrate GraphQL into your C# projects.InstallationTo use HotChocolate in your C# project, you can install the necessary NuGet packages:dotnet add package HotChocolate.AspNetCoredotnet add package HotChocolate.AspNetCore.InterceptorsUsageHere is a basic example of setting up a GraphQL server using HotChocolate in C#:      Create a new ASP.NET Core Web API project:     dotnet new webapi -n GraphQLExample            Modify Startup.cs to configure GraphQL:     using Microsoft.AspNetCore.Builder; using Microsoft.Extensions.DependencyInjection; using HotChocolate.AspNetCore; public class Startup {     public void ConfigureServices(IServiceCollection services)     {         services.AddGraphQLServer()                 .AddQueryType&lt;Query&gt;();     }     public void Configure(IApplicationBuilder app)     {         app.UseRouting();         app.UseEndpoints(endpoints =&gt;         {             endpoints.MapGraphQL(\"/graphql\");         });     } }            Create a Query.cs file for GraphQL queries:     using HotChocolate; public class Query {     [GraphQLName(\"hello\")]     public string GetHello() =&gt; \"Hello, GraphQL!\"; }      Running the Application      Run your GraphQL API:     cd GraphQLExample dotnet run            Access your GraphQL API:    Open your browser and navigate to http://localhost:5283/graphql. You can now execute GraphQL queries(Check the correct port number within launchSettings.json applicationUrl property).  This simple setup defines a GraphQL server with a single query (hello) that returns a string.What next?GraphQL provides a flexible and efficient way to query and manipulate data. With the HotChocolate library, integrating GraphQL into your C# projects becomes straightforward.For more advanced features and customization options, refer to the HotChocolate Documentation.Source Code"
  },
  
  {
    "title": "C# | Implementing API gateway using ocelot in asp.net core application",
    "url": "/posts/Implementing-Ocelot-Gateway/",
    "categories": "C#, Web API",
    "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, webapi, c#, gateway",
    "date": "2023-09-13 00:00:00 +0200",
    





    
    "snippet": "API Gateway is an API management tools that sits between a client application and backend application. It agregates different services, maintain load balancing and work as reverse proxy. Ocelot is ...",
    "content": "API Gateway is an API management tools that sits between a client application and backend application. It agregates different services, maintain load balancing and work as reverse proxy. Ocelot is an api managment tool which is very powerful and best fit for .net application.Tools and technologies used  Visual Studio 2022  .NET 6.0  In Memory Database  Entity Framework  ASP.NET Core Web API  C#  OcelotImplementationStep 1: Create solution and projects.  Create a solution name APIGateway  Add 4 new web api project, name - Catalog.API, Location.API, Ordering.API and BFF.Web in the solution.Here, BFF.Web project will act as API Gateway.Step 2: Install nuget packages.  Install following nuget package in Catalog.API Project    PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools        Install following nuget package in Ordering.API Project    PM&gt; Install-Package Microsoft.EntityFrameworkCorePM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools        Install following nuget packages in BFF.Web Project    PM&gt; Install-Package OcelotPM&gt; Install-Package Ocelot.Cache.CacheManager      Step 3: Organize Catalog.API Project  Create a Product model class in Catalog.API/Model folderProduct.cs    using System.ComponentModel.DataAnnotations;    using System.ComponentModel.DataAnnotations.Schema;        namespace Catalog.API.Model    {        public class Product        {            [Key]            [DatabaseGenerated(DatabaseGeneratedOption.Identity)]            public int Id { get; set; }            public string Name { get; set; }                public string Description { get; set; }                public decimal Price { get; set; }                public int AvailableStock { get; set; }                public int RestockThreshold { get; set; }        }    }  Create a CatalogContext class in Catalog.API/Db folderCatalogContext.cs    using Catalog.API.Model;    using Microsoft.EntityFrameworkCore;        namespace Catalog.API.Db    {        public class CatalogContext : DbContext        {            public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options)            {                }                protected override void OnModelCreating(ModelBuilder modelBuilder)            {                }                protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)            {                base.OnConfiguring(optionsBuilder);            }                public DbSet&lt;Product&gt; Products { get; set; }        }    }  Modify Program.cs file as follows    using Catalog.API.Db;    using Microsoft.EntityFrameworkCore;        var builder = WebApplication.CreateBuilder(args);        // Add services to the container.        builder.Services.AddControllers();        builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));        // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();    builder.Services.AddSwaggerGen();        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();    }        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();  Create a conroller class name ProductsController in Catalog.API/Controllers folderCatalogContoller.cs    using System;    using System.Collections.Generic;    using System.Linq;    using System.Threading.Tasks;    using Microsoft.AspNetCore.Http;    using Microsoft.AspNetCore.Mvc;    using Microsoft.EntityFrameworkCore;    using Catalog.API.Db;    using Catalog.API.Model;        namespace Catalog.API.Controllers    {        [Route(\"api/[controller]\")]        [ApiController]        public class ProductsController : ControllerBase        {            private readonly CatalogContext _context;                public ProductsController(CatalogContext context)            {                _context = context;            }                // GET: api/Products            [HttpGet(\"GetAll\")]            public async Task&lt;ActionResult&lt;IEnumerable&lt;Product&gt;&gt;&gt; GetProducts()            {                return await _context.Products.ToListAsync();            }                // GET: api/Products/5            [HttpGet(\"{id}\")]            public async Task&lt;ActionResult&lt;Product&gt;&gt; GetProduct(int id)            {                var product = await _context.Products.FindAsync(id);                    if (product == null)                {                    return NotFound();                }                    return product;            }                // PUT: api/Products/5            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPut(\"Edit/{id}\")]            public async Task&lt;IActionResult&gt; PutProduct(int id, Product product)            {                if (id != product.Id)                {                    return BadRequest();                }                    _context.Entry(product).State = EntityState.Modified;                    try                {                    await _context.SaveChangesAsync();                }                catch (DbUpdateConcurrencyException)                {                    if (!ProductExists(id))                    {                        return NotFound();                    }                    else                    {                        throw;                    }                }                    return NoContent();            }                // POST: api/Products            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPost(\"Add\")]            public async Task&lt;ActionResult&lt;Product&gt;&gt; PostProduct(Product product)            {                _context.Products.Add(product);                await _context.SaveChangesAsync();                    return CreatedAtAction(\"GetProduct\", new { id = product.Id }, product);            }                // DELETE: api/Products/5            [HttpDelete(\"Delete/{id}\")]            public async Task&lt;IActionResult&gt; DeleteProduct(int id)            {                var product = await _context.Products.FindAsync(id);                if (product == null)                {                    return NotFound();                }                    _context.Products.Remove(product);                await _context.SaveChangesAsync();                    return NoContent();            }                private bool ProductExists(int id)            {                return _context.Products.Any(e =&gt; e.Id == id);            }        }    }Step 4: Organize Ordering.API Project  Create a Order model class in Ordering.API/Model folderOrder.cs    namespace Ordering.API.Models    {        public class Order        {            public int Id { get; set; }            public string Address { get; set; }                public DateTime OrderDate { get; set; }                public string Comments { get; set; }        }    }  Create a OrderingContext class in Ordering.API/Db folderOrderingContext.cs    using Microsoft.EntityFrameworkCore;    using Ordering.API.Models;        namespace Ordering.API.Db    {        public class OrderingContext : DbContext        {            public OrderingContext(DbContextOptions&lt;OrderingContext&gt; options) : base(options)            {                }            public DbSet&lt;Ordering.API.Models.Order&gt; Order { get; set; }        }    }  Modify Program.cs file as follows    using Microsoft.EntityFrameworkCore;    using Ordering.API.Db;        var builder = WebApplication.CreateBuilder(args);        // Add services to the container.        builder.Services.AddControllers();        builder.Services.AddDbContext&lt;OrderingContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));        // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();    builder.Services.AddSwaggerGen();        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();    }        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();  Create a conroller class name OrdersController in Ordering.API/Controllers folderOrdersController.cs    using System;    using System.Collections.Generic;    using System.Linq;    using System.Threading.Tasks;    using Microsoft.AspNetCore.Http;    using Microsoft.AspNetCore.Mvc;    using Microsoft.EntityFrameworkCore;    using Ordering.API.Db;    using Ordering.API.Models;        namespace Ordering.API.Controllers    {        [Route(\"api/[controller]\")]        [ApiController]        public class OrdersController : ControllerBase        {            private readonly OrderingContext _context;                public OrdersController(OrderingContext context)            {                _context = context;            }                // GET: api/Orders            [HttpGet(\"GetAll\")]            public async Task&lt;ActionResult&lt;IEnumerable&lt;Order&gt;&gt;&gt; GetOrder()            {                return await _context.Order.ToListAsync();            }                // GET: api/Orders/5            [HttpGet(\"{id}\")]            public async Task&lt;ActionResult&lt;Order&gt;&gt; GetOrder(int id)            {                var order = await _context.Order.FindAsync(id);                    if (order == null)                {                    return NotFound();                }                    return order;            }                // PUT: api/Orders/5            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPut(\"Edit/{id}\")]            public async Task&lt;IActionResult&gt; PutOrder(int id, Order order)            {                if (id != order.Id)                {                    return BadRequest();                }                    _context.Entry(order).State = EntityState.Modified;                    try                {                    await _context.SaveChangesAsync();                }                catch (DbUpdateConcurrencyException)                {                    if (!OrderExists(id))                    {                        return NotFound();                    }                    else                    {                        throw;                    }                }                    return NoContent();            }                // POST: api/Orders            // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754            [HttpPost(\"Add\")]            public async Task&lt;ActionResult&lt;Order&gt;&gt; PostOrder(Order order)            {                _context.Order.Add(order);                await _context.SaveChangesAsync();                    return CreatedAtAction(\"GetOrder\", new { id = order.Id }, order);            }                // DELETE: api/Orders/5            [HttpDelete(\"Delete/{id}\")]            public async Task&lt;IActionResult&gt; DeleteOrder(int id)            {                var order = await _context.Order.FindAsync(id);                if (order == null)                {                    return NotFound();                }                    _context.Order.Remove(order);                await _context.SaveChangesAsync();                    return NoContent();            }                private bool OrderExists(int id)            {                return _context.Order.Any(e =&gt; e.Id == id);            }        }    }Step 5: Organize Location.API Project  Create CountriesController in Location.API/Controllers folder    using Microsoft.AspNetCore.Mvc;        namespace Location.API.Controllers    {        [ApiController]        [Route(\"api/[controller]\")]        public class CountriesController : ControllerBase        {          [HttpGet(\"GetAll\")]          public IEnumerable&lt;string&gt; Get()            {                return new string[] {\"Morrocco\",\"Spain\", \"Portugal\" };            }        }    }Step 6: Organize BFF.Web (API Gateway) Project  Add a configuraton file for api gateway. I keep this file name - ocelot.json. Add this file in the root directory.ocelot.json    {      //---Location Service: Start ----------//      \"Routes\": [        {          \"DownstreamPathTemplate\": \"/api/Countries/GetAll\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7003            }          ],              // Configure caching          // the cache will expire after 30 seconds.          \"FileCacheOptions\": {            \"TtlSeconds\": 30,            \"Region\": \"countriescaching\"          },              \"UpstreamPathTemplate\": \"/Countries/GetAll\",          \"UpstreamHttpMethod\": [ \"Get\" ],              // Enable case sensative Routing/URL          \"RouteIsCaseSensitive\": true        },            //---Location Service: End ----------//            // Catalog Services        //------------------//        {          \"DownstreamPathTemplate\": \"/api/Products/GetAll\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7282            }          ],              // Implement rate limiting          // maximum admitted 1 per 5s.          \"RateLimitOptions\": {            \"ClientWhitelist\": [              // This is an array used to specify the clients that should not be affected by the rate-limiting            ],            \"EnableRateLimiting\": true,            \"Period\": \"5s\",            \"PeriodTimespan\": 1,            \"Limit\": 1          },              \"UpstreamPathTemplate\": \"/Products/GetAll\",          \"UpstreamHttpMethod\": [ \"Get\" ]        },        {          \"DownstreamPathTemplate\": \"/api/Products/Add\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7282            }          ],              \"UpstreamPathTemplate\": \"/Products/Add\",          \"UpstreamHttpMethod\": [ \"Post\" ]        },            {          \"DownstreamPathTemplate\": \"/api/Products/{id}\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7282            }          ],              \"UpstreamPathTemplate\": \"/Products/{id}\",          \"UpstreamHttpMethod\": [ \"Get\" ]        },            {          \"DownstreamPathTemplate\": \"/api/Products/Edit/{id}\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7282            }          ],              \"UpstreamPathTemplate\": \"/Products/Edit/{id}\",          \"UpstreamHttpMethod\": [ \"Put\" ]        },            {          \"DownstreamPathTemplate\": \"/api/Products/Delete/{id}\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7282            }          ],              \"UpstreamPathTemplate\": \"/Products/Delete/{id}\",          \"UpstreamHttpMethod\": [ \"Delete\" ]        },            //---Catalog service : End ------------//            //---Ordering Service: Start ----------//        // Catch All Routing        {          \"DownstreamPathTemplate\": \"/{url}\",          \"DownstreamScheme\": \"https\",          \"DownstreamHostAndPorts\": [            {              \"Host\": \"localhost\",              \"Port\": 7126            }          ],              \"UpstreamPathTemplate\": \"/{url}\",          \"UpstreamHttpMethod\": [ \"Get\", \"Post\", \"Put\", \"Delete\" ]        }          ],          //---Ordering Service: End ----------//          //https://localhost:7282/api/Products/GetAll          \"GlobalConfiguration\": {        // enable request correleation id to capture request information        \"RequestIdKey\": \"X-Correlation-Id\",        \"BaseUrl\": \"https://localhost:7205/\"      }    }  Modify Program.cs file as followsProgram.cs    using Ocelot.DependencyInjection;    using Ocelot.Middleware;    using Ocelot.Cache.CacheManager;            var builder = WebApplication.CreateBuilder(args);            var environment = Environment.GetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\");    builder.Configuration.SetBasePath(Directory.GetCurrentDirectory())        .AddJsonFile(\"ocelot.json\", optional: false, reloadOnChange: true)        .AddEnvironmentVariables();            // Add services to the container.        builder.Services.AddControllers();    // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle    builder.Services.AddEndpointsApiExplorer();        // Swagger for ocelot    //builder.Services.AddSwaggerForOcelot(builder.Configuration);    //builder.Services.AddSwaggerForOcelot();    builder.Services.AddSwaggerGen();        //For ocelot    builder.Services.AddOcelot()                // Added for caching        .AddCacheManager(x =&gt; {            x.WithDictionaryHandle();        });        var app = builder.Build();        // Configure the HTTP request pipeline.    if (app.Environment.IsDevelopment())    {        app.UseSwagger();        app.UseSwaggerUI();        //app.UseSwaggerForOcelotUI();    }        app.UseOcelot();        app.UseHttpsRedirection();        app.UseAuthorization();        app.MapControllers();        app.Run();Step 7: Run and Test application  Now run multiple (all) projects and test application using postman.  You have to check application using api gateway.  For Location service, test using https://localhost:7205/Countries/GetAll  For Catalog service, test using https://localhost:7205/Products/{endpoints}  For Ordering service, test using https://localhost:7205/api/Orders/{endpoints}Note:Configure caching in api gateway  Added the following section in ocelot.json file.    \"FileCacheOptions\": {    \"TtlSeconds\": 30,    \"Region\": \"countriescaching\"    }  Added the following code block in the the Program.cs file as follows    //For ocelotbuilder.Services.AddOcelot()            // Added for caching    .AddCacheManager(x =&gt; {        x.WithDictionaryHandle();    });      Enable case sensative URL  Add following blocks in ocelot.json for case sensative URL          // Enable case sensative Routing/URL      \"RouteIsCaseSensitive\": true      Implement rate limiting  Add following blocks in ocelot.json for rate limiting    // Implement rate limiting    // maximum admitted 1 per 5s.    \"RateLimitOptions\": {    \"ClientWhitelist\": [        // This is an array used to specify the clients that should not be affected by the rate-limiting    ],    \"EnableRateLimiting\": true,    \"Period\": \"5s\",    \"PeriodTimespan\": 1,    \"Limit\": 1    }Catch all routing  Add following blocks in ocelot.json for catch all routing    {        \"DownstreamPathTemplate\": \"/{url}\",        \"DownstreamScheme\": \"https\",        \"DownstreamHostAndPorts\": [        {            \"Host\": \"localhost\",            \"Port\": 7126        }        ],            \"UpstreamPathTemplate\": \"/{url}\",        \"UpstreamHttpMethod\": [ \"Get\", \"Post\", \"Put\", \"Delete\" ]    }Source code"
  },
  
  {
    "title": "Certification | AZ-400 Certification Path",
    "url": "/posts/AZ-400-Certification-Path/",
    "categories": "Microsoft, Certification",
    "tags": "certification, microsoft, azuredevops, dotnet",
    "date": "2023-09-10 00:00:00 +0200",
    





    
    "snippet": "Microsoft certification paths changesBoth the MCSD and MCT certifications were discontinued by Microsoft in 2017. They were replaced by newer certification programs that reflect the changing demand...",
    "content": "Microsoft certification paths changesBoth the MCSD and MCT certifications were discontinued by Microsoft in 2017. They were replaced by newer certification programs that reflect the changing demands of the tech industry and the evolving needs of professionals.The MCSD certification was replaced by the Azure Developer Associate certification, which is a newer certification program that is designed for developers who want to demonstrate their expertise in buildingand deploying cloud-based solutions using Microsoft Azure. The MCT certification was replaced by the Microsoft Certified: Learning Consultant certification, which is a program that recognizes individuals who have demonstrated expertise in designing and delivering training programs on Microsoft technologies.Microsoft offers a wide range of certification programs that are designed to validate the skills of professionals in a variety of different roles. Some of the most popular certification programs offered by Microsoft by role include:      Developers: Microsoft offers several certification programs that are designed for developers, including the Azure Developer Associate certification, the Microsoft Certified: Power Platform Developer Associate certification, and noth the Microsoft Certified: Azure Solutions Architect Expert certification and DevOps Engineer Expert.        IT Administrators: Microsoft offers several certification programs that are designed for IT administrators, including the Microsoft Certified: Azure Administrator Associate certification, the Microsoft Certified: Azure Security Engineer Associate certification, and the Microsoft Certified: Windows Server 2016 certification.        Data Professionals: Microsoft offers several certification programs that are designed for data professionals, including the Microsoft Certified: Data Analyst Associate certification, the Microsoft Certified: Data Scientist Associate certification, and the Microsoft Certified: Azure Data Engineer Associate certification.        Business Professionals: Microsoft offers several certification programs that are designed for business professionals, including the Microsoft Certified: Power Platform Fundamentals certification, the Microsoft Certified: Dynamics 365 Fundamentals certification, and the Microsoft Certified: Dynamics 365 Marketing certification.        Educators: Microsoft offers several certification programs that are designed for educators, including the Microsoft Certified: Educator certification, the Microsoft Certified: Trainer certification, and the Microsoft Certified: Learning Consultant certification.  We will focus in this post on Developers role. So, actually for a dotnet developer the best Microsoft certification to achieve is by following the DevOps Engineer Expert Certification path. Currently the different missions of a dotnet developer require some experience on the devops side for the build and release parts and as well as on the Microsoft Azure part too.Describe certificate pathAccording to The 2021 Pearson VUE Value of IT Certification survey 91% of certified IT professional get more professional credibility, 76% are more valuable to their employers, and 28% of the surveyed candidates reported a salary increase as a benefit of getting certified.Azure DevOps Expert Certification is a three-stars certification that falls under the Microsoft expert-level certifications.AZ-400 Exam PrerequisiteA candidate for this exam must have written either or both of the assoiate Microsoft certification in the schema bellow. Note that the Exam AZ-900: Microsoft Azure Fundamentals are optional and it open to you to pass it. From my part i choose to write the Azure Developer Associate because of my experience as a software developer. For Devops engineer it more recommended to pass the Administrator Associate certification in order to achieve the DevOps Engineer Expert one.Study for your examThe AZ-400 Exam contains about 40-60 questions; however, the number can vary depending on the exam. with a duration of about 180 minutes. Candidates are required to have a passing score of 700/1000 to earn the certification.Structure of the exam      Single-choice questions(with YES/NO options) Cannot be reviewed, skipped, or returned to later.        Single-choice questions.        Multiple-choice questions.        Arrange in the right sequence questions.        Case-study with multiple questions.  Study planA typical study plan may last for a month at about 5 hours of daily study. The study plan depends on the level of experience candidates have on the various topics measured in the exam. Experienced DevOps professionals may spend less time compared to the beginner fellows.Study materialsThis exam measures your ability to accomplish the following technical tasks: develop an instrumentation strategy; develop a Site Reliability Engineering (SRE) strategy; develop a security and compliance plan; manage source control; facilitate communication and collaboration; define and implement continuous integration; and define and implement continuous delivery and release management strategy. The following materials should be helpful.Microsoft learn: Azure DevOps Learning Path      Azure Devops Labs: get hands-on experience using various Azure DevOps services to solve real-life scenario business problems.        Azure DevOps documentation        Azure documentation        Practice question materialsThe listed links below are very helpful and may tell you what the questions may look like. All the objectives of this exam are covered so you’ll be ready for any question on the exam :        AZ-400: Designing and Implementing Microsoft DevOps Solutions Microsoft Official Practice Test    Whizlabs-AZ-400 Practice test    ExamTopics-AZ-400 Practice test  Scheduling and Taking your examIt is important you arrive at your test center 15-30 minutes before your scheduled appointment time. This will give you adequate time to complete the necessary sign-in procedures. If you arrive more than 15 minutes late for an exam and are refused admission, payments are due for the exam and delivery fees.Be prepared to show two (2) valid forms of personal identification, a National Identity card, Drivers license, or an International Passport.TipsMark questions whose answers you are not sure about for review, and revisit them when done with the others.Don’t leave any questions unanswered and make sure to submit before proceeding to the next section.Case-study questions come after the first section.Make sure you keep to time.You will receive your transcript upon submission in less than an hour, and an acclaim badge upon passing the exam in your mailbox. Share your badge on social networks like LinkedIn, Twitter, or Facebook.Voila, congratulations, you are now a Microsoft Certified: Azure DevOps Expert, you deserve it. Keep your skills sharp and transfer your knowledge to real work situations.Thank you for reading, I hope this helps you prepare and pass the AZ-400 examination.You can view my Azure DevOps badges here 😊  Microsoft Certified: Azure Developer Associate  Microsoft Certified: DevOps Engineer ExpertReference  Microsoft Certified: DevOps Engineer Expert  Azure DevOps Labs  2021 Pearson VUE Value of IT Certification  Introduction to the AZ-400: Designing and Implementing Microsoft DevOps Solutions Exam"
  },
  
  {
    "title": "C# | TDD Example using xUnit and Moq",
    "url": "/posts/C-TDD-Example-using-xUnit-and-Moq/",
    "categories": "C#, TDD",
    "tags": "microsoft, csharp, c#, tdd, unittest",
    "date": "2023-09-07 00:00:00 +0200",
    





    
    "snippet": "In Test-Driven Development (TDD), we write tests before writing the actual code. This example demonstrates how to create unit tests in C# using xUnit, how to use the Moq framework for mocking, and ...",
    "content": "In Test-Driven Development (TDD), we write tests before writing the actual code. This example demonstrates how to create unit tests in C# using xUnit, how to use the Moq framework for mocking, and how to refactor tests using Fact and Theory attributes.PrerequisitesBefore we start, make sure you have the following installed:  Visual Studio or Visual Studio Code (or any C# IDE of your choice)  xUnit.net testing framework  Moq mocking frameworkScenarioSuppose we are building a simple library that calculates the total price of items in a shopping cart.Step 1: Set Up the ProjectCreate a new C# project and add references to the xUnit and Moq libraries.Step 2: Write the Initial TestLet’s start by writing a test for our shopping cart. Create a test class, and write a Fact that checks if the cart’s total price is calculated correctly:using System;using Xunit;public class ShoppingCartTests{    [Fact]    public void CalculateTotalPrice_CartWithItems_ReturnsTotalPrice()    {        // Arrange        var cart = new ShoppingCart();        cart.AddItem(new Item(\"Item 1\", 10.0));        cart.AddItem(new Item(\"Item 2\", 15.0));        // Act        var totalPrice = cart.CalculateTotalPrice();        // Assert        Assert.Equal(25.0, totalPrice);    }}Step 3: Write the Initial CodeNow, create the ShoppingCart class and implement the CalculateTotalPrice method:public class ShoppingCart{    private List&lt;Item&gt; items = new List&lt;Item&gt;();    public void AddItem(Item item)    {        items.Add(item);    }    public double CalculateTotalPrice()    {        return items.Sum(item =&gt; item.Price);    }}Step 4: Mocking with MoqSuppose our ShoppingCart class depends on an external data source (e.g., a database). To test it, we can use Moq to mock this dependency. Create an interface for the data source, implement it, and inject it into the ShoppingCart:public interface IDataSource{    List&lt;Item&gt; GetItems();}public class Database : IDataSource{    public List&lt;Item&gt; GetItems()    {        // Simulate database call        return new List&lt;Item&gt;        {            new Item(\"Item 1\", 10.0),            new Item(\"Item 2\", 15.0)        };    }}public class ShoppingCart{    private IDataSource dataSource;    public ShoppingCart(IDataSource dataSource)    {        this.dataSource = dataSource;    }    // ...}Step 5: Refactor the Test with TheoryInstead of Fact, let’s refactor the test using Theory. This allows us to use data from multiple test cases. For instance, we can test the CalculateTotalPrice method with different sets of items:[Theory][InlineData(new[] { 10.0, 15.0 }, 25.0)][InlineData(new[] { 5.0, 7.0, 8.0 }, 20.0)]public void CalculateTotalPrice_CartWithItems_ReturnsTotalPrice(double[] itemPrices, double expectedTotalPrice){    // Arrange    var cart = new ShoppingCart(CreateMockDataSource(itemPrices));    // Act    var totalPrice = cart.CalculateTotalPrice();    // Assert    Assert.Equal(expectedTotalPrice, totalPrice);}Here, we’re using Theory to run the test with different sets of item prices. The CreateMockDataSource function sets up a Moq mock of the IDataSource interface.What Next?  This example demonstrates how to use TDD with xUnit, Moq for mocking, and how to refactor tests using Fact and Theory attributes. By following TDD, you can ensure that your code is thoroughly tested and that it meets the specified requirements."
  },
  
  {
    "title": "Azure | Azure Functions By Example",
    "url": "/posts/Azure-Functions-By-Example/",
    "categories": "Azure, Azure Functions",
    "tags": "microsoft, azure, azurefunctions",
    "date": "2023-09-06 00:00:00 +0200",
    





    
    "snippet": "Microsoft Azure Functions ExampleIn this example, we’ll create a simple Azure Functions application in C#. Azure Functions is a serverless compute service that allows you to run event-triggered cod...",
    "content": "Microsoft Azure Functions ExampleIn this example, we’ll create a simple Azure Functions application in C#. Azure Functions is a serverless compute service that allows you to run event-triggered code without managing infrastructure. We will create a function that responds to an HTTP request.PrerequisitesBefore you begin, make sure you have the following prerequisites:  Microsoft Azure account.  Azure Functions Extension installed in Visual Studio or Visual Studio Code.  .NET Core SDK installed.Create an Azure Function      Create a New Function Project:    In your development environment, create a new Azure Functions project using the Azure Functions extension. You can choose the template that suits your needs.        Add a New Function:    Add a new function to your project. Select “HTTP trigger” as the template. This will create a function that responds to HTTP requests.        Implement the Function:    In the generated function code, you can implement your logic. Here’s a simple example that responds with “Hello, Azure Functions!” when the function is triggered:    using System;using Microsoft.AspNetCore.Http;using Microsoft.AspNetCore.Mvc;using Microsoft.Azure.WebJobs;using Microsoft.Azure.WebJobs.Extensions.Http;using Microsoft.Extensions.Logging;public static class HelloWorldFunction{    [FunctionName(\"HelloWorld\")]    public static async Task&lt;IActionResult&gt; Run(        [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req,        ILogger log)    {        log.LogInformation(\"C# HTTP trigger function processed a request.\");        return new OkObjectResult(\"Hello, Azure Functions!\");    }}            Run the Function Locally:    You can test your function locally by running it in your development environment. Use the Azure Functions CLI to start the local runtime.    func start            Deploy to Azure:    Once your function is working as expected, you can deploy it to Azure. Use the Azure Functions extension to publish your project to Azure.        Test the Function in Azure:    After deployment, you can test the function by navigating to its URL. You’ll receive “Hello, Azure Functions!” as a response.  What Next?This example demonstrates a basic Azure Functions application in C#. Azure Functions are a powerful way to build serverless applications that respond to various triggers. You can extend this example by adding more functions, integrating with other Azure services, and handling more complex scenarios.Azure Functions are flexible and can be used for a wide range of use cases, including data processing, automation, and API endpoints."
  },
  
  {
    "title": "C# | Create .Net custom template using GitHub Packages Registry",
    "url": "/posts/Create-.Net-custom-template-using-GitHub-Packages-Registry/",
    "categories": "C#, Nuget Package",
    "tags": "microsoft, csharp, c#, github, nugetpackage, aspnetcore",
    "date": "2023-08-21 00:00:00 +0200",
    





    
    "snippet": "Introduction.NET gives us opportunity to create custom template for future use and GitHub packages registry is most popular now a days to host custom template. In this article, I will show you how ...",
    "content": "Introduction.NET gives us opportunity to create custom template for future use and GitHub packages registry is most popular now a days to host custom template. In this article, I will show you how to create .net custom template using GitHub packages registry.Tools and Technology uses  Visual Studio 2022  .NET 6  C#  GitHubImplementationStep 1: Create a personal access token (PAT) from GitHub  Login into you GitHub  Go to settings -&gt; Developer Settings -&gt; Personal Access Tokens  Click “Generate new token” button  Type Note for the token, expiration days      Select scope for the token – here I have selected repo, write:packages, delete:packages as shown below.        Now click “Generate Token” at the bottom of the panel      Copy the token and store the token for further use because you cannot find it later      Step 2: Add Nuget Source in visual studio  Type the following command to add sourcedotnet nuget add source https://nuget.pkg.github.com/hbolajraf/index.json --name github-hbolajraf --username hbolajraf --password &lt;Your personal Access Token&gt;      You will see a source is added in C:\\Users\\hbolajraf\\AppData\\Roaming\\NuGet\\NuGet.Config file    You can add source from visual studio Tools -&gt; Options -&gt; NuGet Package Manager -&gt; Package Sources  Restart visual studio to get new nuget package sourceStep 3: Create template for your application  Create a project or multiple projects using a solution file.  Here, I have created a clean architecture template with a solution file and multiple projects      Create a folder name – “.template.config” in the root directory of your application.        Create a file template.json in .template.config folder.  Add the following content to template.json filetemplate.json    {      \"$schema\": \"http://json.schemastore.org/template\",      \"author\": \"hbolajraf Hasan\",      \"classifications\": [        \"dotnet\",        \"CleanArchitecture\"      ],      \"name\": \"Clean Architecture project template\",      \"description\": \"Project template to create project using Clean Architecture\",      \"identity\": \"CleanArchitecture\",      \"shortName\": \"CleanArchitecture\",      \"sourceName\": \"CleanArch\",      \"tags\": {        \"language\": \"C#\",        \"type\": \"project\"      },      \"symbols\": {        \"Framework\": {          \"type\": \"parameter\",          \"description\": \"The target framework for the project.\",          \"datatype\": \"choice\",          \"choices\": [            {              \"choice\": \"net6.0\"            },            {              \"choice\": \"net5.0\"          }          ],          \"defaultValue\": \"net6.0\",          \"replaces\": \"{TargetFramework}\"        }      }    }Step 4: Install and create template locally (Optional)  Go to one where “.template.config” folder exists.  Now run the following command. Don’t forgot to add space “.” at the end.dotnet new --install .  You will see in the output that template is created. You will see Short Name of template which is used to install template.  Now go to the directory where you want to install template and type the following command.dotnet new CleanArchitecture    #or,dotnet new CleanArchitecture --forceHere CleanArchitecture is short name of the template  To create template by another name type as follows.dotnet new CleanArchitecture -o LocationNow projects name will be Location instead of CleanArch as mentioned in the previous json file.  Now go to the same directory to uninstall the template and type the following command.dotnet new --uninstall .```console    ## Step 5: Packing a template into a NuGet Package (nupkg file)*   Create a .csproj file one directory up of “.template.config” folder.*   In my case the folder structure as follows        ![](/posts/2022/temp-04.PNG)    **Add the following content in TemplatePack.csproj project.****TemplatePack.csproj**```xml         &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;           &lt;PropertyGroup&gt;             &lt;PackageType&gt;Template&lt;/PackageType&gt;             &lt;PackageVersion&gt;1.0.0&lt;/PackageVersion&gt;             &lt;PackageId&gt;hbolajraf.CleanArchitecture.Templates&lt;/PackageId&gt;             &lt;Title&gt;Clean Architecture Template&lt;/Title&gt;             &lt;Authors&gt;hbolajraf Hasan&lt;/Authors&gt;             &lt;Description&gt;Clean Architecture Template&lt;/Description&gt;             &lt;PackageTags&gt;dotnet-new;templates;clean-architecture&lt;/PackageTags&gt;            &lt;TargetFramework&gt;netstandard2.0&lt;/TargetFramework&gt;             &lt;IncludeContentInPack&gt;true&lt;/IncludeContentInPack&gt;             &lt;IncludeBuildOutput&gt;false&lt;/IncludeBuildOutput&gt;             &lt;ContentTargetFolders&gt;content&lt;/ContentTargetFolders&gt;             &lt;NoWarn&gt;$(NoWarn);NU5128&lt;/NoWarn&gt;             &lt;NoDefaultExcludes&gt;true&lt;/NoDefaultExcludes&gt;         &lt;RepositoryUrl&gt;https://github.com/hbolajraf/public-packages&lt;/RepositoryUrl&gt;          &lt;/PropertyGroup&gt;           &lt;ItemGroup&gt;         &lt;Content Include=\"CleanArchitecture\\**\\*\" Exclude=\"CleanArchitecture\\**\\bin\\**;CleanArchitecture\\**\\obj\\**\" /&gt;         &lt;Compile Remove=\"..\\**\\*\" /&gt;           &lt;/ItemGroup&gt;       &lt;/Project&gt;   To create package go to the directory where TemplatePack.csproj file exists and type the following command.dotnet pack  You will hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg file is created in bin/Debug folder.Step 6: Now push the package to github package registry  Go to the directory where hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg is exists.      Type the following command to push the package in github package registry    dotnet nuget push .\\hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg –api-key  --source github-hbolajraf    Here, “github-hbolajraf” is a github source which we have added in step – 2.  Now login your github and you will see a template is uploaded to your package registry.Step 7: Download template and install in local machine  Run the following command to install in local machinedotnet new --install  hbolajraf.CleanArchitecture.Templateshbolajraf.CleanArchitecture.Templates is package Id.output:The following template packages will be installed:   hbolajraf.CleanArchitecture.TemplatesSuccess: hbolajraf.CleanArchitecture.Templates::1.0.0 installed the following templates:Template Name                        Short Name         Language  Tags-----------------------------------  -----------------  --------  ------------------------Clean Architecture project template  CleanArchitecture  [C#]      dotnet/CleanArchitecture  Now go to the directory where you want to regenerate the template and type the following command.dotnet new CleanArchitectureHere CleanArchitecture is the short name of the templateNote  To see installed template in locally use the following command. You will also see how to uninstall the particular template.dotnet new --uninstall  To uninstall a particular template from local machine, use the following command.dotnet new --uninstall hbolajraf.CleanArchitecture.Templatesdotnet new –uninstall &lt;package id&gt; Source code"
  },
  
  {
    "title": "GitHub | Deploy .net core NuGet Packages in GitHub Packages Registry",
    "url": "/posts/Deploy-.net-core-NuGet-Packages-in-GitHub-Packages-Registry/",
    "categories": "GitHub, Nuget Package",
    "tags": "microsoft, csharp, c#, nuget, nugetpackage, aspnetcore, github",
    "date": "2023-08-19 00:00:00 +0200",
    





    
    "snippet": "IntroductionGitHub packages registries is most popular now a days. It offers different packages registries for most used package managers, such as NuGet, npm, Docker etc. In this article, I will sh...",
    "content": "IntroductionGitHub packages registries is most popular now a days. It offers different packages registries for most used package managers, such as NuGet, npm, Docker etc. In this article, I will show you how to host a .net core NuGet Package in GitHub Packages Registry.Tools and Technology uses  Visual Studio 2022  .NET 6  C#  GitHubImplementationStep 1: Create a personal access token (PAT) from GitHub  Login into you GitHub  Go to settings -&gt; Developer Settings -&gt; Personal Access Tokens  Click “Generate new token” button  Type Note for the token, expiration days      Select scope for the token – here I have selected repo, write:packages, delete:packages as shown below.        Now click “Generate Toke” at the bottom of the panel      Copy the token and store the token for further use because you cannot find it later      Step 2: Add Nuget Source in visual studio  Type the following command to add sourcedotnet nuget add source https://nuget.pkg.github.com/hbolajraf/index.json --name github-hbolajraf --username hbolajraf --password &lt;Your personal Access Token&gt;      You will see a source is added in C:\\Users\\hbolajraf\\AppData\\Roaming\\NuGet\\NuGet.Config file    Optional: You can also add source from visual studio Tools -&gt; Options -&gt; NuGet Package Manager -&gt; Package Sources  Restart visual studio to get new nuget package sourceStep 3: Create a class library to publish in GitHub Packages  Create a class library name – ‘CryptoEngine”  Create a class CryptoGenerator as follows    using System.Security.Cryptography;    using System.Text;        namespace CryptoEngine    {        public class CryptoGenerator        {            public static string GenerateSha256Hash(string plainText)            {                // Create a SHA256                   using (SHA256 sha256Hash = SHA256.Create())                {                    // ComputeHash - returns byte array                      byte[] bytes = sha256Hash.ComputeHash(Encoding.UTF8.GetBytes(plainText));                        // Convert byte array to a string                       StringBuilder builder = new StringBuilder();                    for (int i = 0; i &lt; bytes.Length; i++)                    {                        builder.Append(bytes[i].ToString(\"x2\"));                    }                    return builder.ToString();                }            }        }    }  Click right button on class library project -&gt; Package -&gt; General  Mark “Produce a package file during build operations”  Type Package ID, Package Version, Authors, Company, Product, Description  Type repository URL – A github repository and save  Now you will see the csproj file as followsCryptoEngine.csproj    &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;          &lt;PropertyGroup&gt;        &lt;TargetFramework&gt;net6.0&lt;/TargetFramework&gt;        &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;        &lt;Nullable&gt;enable&lt;/Nullable&gt;        &lt;GeneratePackageOnBuild&gt;True&lt;/GeneratePackageOnBuild&gt;        &lt;PackageId&gt;hbolajraf.CryptoEngine&lt;/PackageId&gt;        &lt;Version&gt;1.0.0&lt;/Version&gt;        &lt;Authors&gt;hbolajraf hasan&lt;/Authors&gt;        &lt;Company&gt;hbolajraf.NET&lt;/Company&gt;        &lt;Product&gt;CryptoEngine&lt;/Product&gt;        &lt;Description&gt;Chipper text generator&lt;/Description&gt;        &lt;RepositoryUrl&gt;https://github.com/hbolajraf/public-packages&lt;/RepositoryUrl&gt;      &lt;/PropertyGroup&gt;        &lt;/Project&gt;Step 4: Create a NuGet Package  Click right button on project and select Pack  A NuGet package will be generated in bin/Debug folder – In this case the nuget package name is hbolajraf.CryptoEngine.1.0.0.nupkg  Or, Go to the directory where .csproj file exists and right the following command to generate nuget packagedotnet packStep 5: Push NuGet package to GitHub Package Registry  Go to the directory where package generated – bin/Debug in this case.  Type following commanddotnet nuget push .\\hbolajraf.CryptoEngine.1.0.0.nupkg --api-key &lt;your github access token&gt; --source github-hbolajrafHere github-hbolajraf is my nuget source name for visual studio. Already added in step 2.  Now login to your Github account and go to Packages tab, you will see a package is uploaded. In this case package name is hbolajraf.CryptoEngineStep 6: Use already uploaded package in a project  If Nuget package source is not added, add it using step – 2  Go to package manager console  Select Package Source as “github-hbolajraf” and type following commandPM&gt; Install-Package hbolajraf.CryptoEngine  Or right button on project -&gt; Manage Nuget Packages  Select Package source “github-hbolajraf”  Browse and install package hbolajraf.CryptoEngineSource code"
  },
  
  {
    "title": "C# | Create Nuget Package using .NET Standard",
    "url": "/posts/Create-Nuget-Package-using-.NET-Standard/",
    "categories": "C#, Nuget Package",
    "tags": "microsoft, csharp, c#, nuget, nugetpackage",
    "date": "2023-08-17 00:00:00 +0200",
    





    
    "snippet": "Tools and technologies used  Visual Studio 2022  .NET Standard 2.1  Nuget.exeImplementationNew Project CreationUnder Visual Studio create a new Project Class Library and use .NET Standard 2.1 as ta...",
    "content": "Tools and technologies used  Visual Studio 2022  .NET Standard 2.1  Nuget.exeImplementationNew Project CreationUnder Visual Studio create a new Project Class Library and use .NET Standard 2.1 as target framework due to compatibility reason with latests versions of .NET CORE Frameworks.Use Nuget CLI to generate files1.Download Nuget.exe fileUse the following link to download the latests version of Nuget.exe file.2.Generate nuspec file Under the new project folder created befor, open a cmd console and run the comand bellow in order to generate the nuspec file.nuget spec NewProjectName.csprojThe result of the command should generate a new file which has the content below :&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;package &gt;  &lt;metadata&gt;    &lt;id&gt;$id$&lt;/id&gt;    &lt;version&gt;$version$&lt;/version&gt;    &lt;title&gt;$title$&lt;/title&gt;    &lt;authors&gt;$author$&lt;/authors&gt;    &lt;requireLicenseAcceptance&gt;false&lt;/requireLicenseAcceptance&gt;    &lt;license type=\"expression\"&gt;MIT&lt;/license&gt;    &lt;!-- &lt;icon&gt;icon.png&lt;/icon&gt; --&gt;    &lt;projectUrl&gt;http://project_url_here_or_delete_this_line/&lt;/projectUrl&gt;    &lt;description&gt;$description$&lt;/description&gt;    &lt;releaseNotes&gt;Summary of changes made in this release of the package.&lt;/releaseNotes&gt;    &lt;copyright&gt;$copyright$&lt;/copyright&gt;    &lt;tags&gt;Tag1 Tag2&lt;/tags&gt;  &lt;/metadata&gt;&lt;/package&gt;3.Generate nupkg fileYou have two solutions order to generate the nuget package file(nupkg)using the post-build event of the project  Under Visual Studio right click on the NewProjectName.crproj and select post-build event tab.After that put the command bellow and Build the solutionnuget pack \"$(ProjectPath)\" -Symbols -Properties Configuration=$(ConfigurationName) -IncludeReferencedProjects -OutputDirectory \"C:\\Dev\\nuget_packages\\NewProjectName\\\"using the Nuget CLI command  Under the cmd window tape the command bellow in order to generate the nuget packagenuget pack MyProject.csproj -properties Configuration=Release -OutputDirectory \"C:\\Dev\\nuget_packages\\NewProjectName\\\"In all cases the new nuget package file will be generated under the output directory : *C:\\Dev\\nuget_packages\\NewProjectName*What next ?Once you’ve created a package, which is a .nupkg file, you can publish it to the gallery of your choice(Artifactory, Azure artifacts or GitHub Package registry)"
  },
  
  {
    "title": "ASP.NET | Web APIs with Zipkin",
    "url": "/posts/ASP.NET-Web-APIs-with-Zipkin-and-C/",
    "categories": "ASP.NET, Web API",
    "tags": "microsoft, csharp, asp.net, webapi, zipkin",
    "date": "2023-08-09 00:00:00 +0200",
    





    
    "snippet": "Zipkin is a distributed tracing system that helps you monitor and troubleshoot microservices-based applications. This guide will walk you through the steps to integrate Zipkin with a C# Web API.Pre...",
    "content": "Zipkin is a distributed tracing system that helps you monitor and troubleshoot microservices-based applications. This guide will walk you through the steps to integrate Zipkin with a C# Web API.PrerequisitesBefore you start, make sure you have the following prerequisites:  .NET Core SDK installed on your system.  A C# Web API project that you want to instrument with Zipkin.  Zipkin server up and running (you can deploy Zipkin using Docker or as a standalone service).Steps to Integrate ZipkinStep 1: Install the Required PackagesYou need to install the necessary packages to enable Zipkin integration in your C# Web API project. Open your project in the terminal or command prompt and use the following command to install the required NuGet packages:dotnet add package OpenTracingdotnet add package OpenTracing.Contrib.NetCoredotnet add package JaegerStep 2: Configure Zipkin TracingIn your C# Web API project, you’ll need to configure Zipkin tracing. Create a configuration class or use your existing configuration:using Jaeger;using Jaeger.Reporters;using Jaeger.Samplers;using Jaeger.Senders;using OpenTracing;public static class ZipkinConfig{    public static ITracer ConfigureTracer(string serviceName)    {        var reporter = new RemoteReporter.Builder()            .WithSender(new UdpSender(\"your-zipkin-server-host\", 9411, 0))            .Build();        var sampler = new ConstSampler(true);        var tracer = new Tracer.Builder(serviceName)            .WithReporter(reporter)            .WithSampler(sampler)            .Build();        GlobalTracer.Register(tracer);        return tracer;    }}Step 3: Instrument Your Web APIIn your Web API controllers or middleware, you can use the ITracer to start and finish spans, which represent different parts of your API request:using System;using Microsoft.AspNetCore.Mvc;using OpenTracing;[Route(\"api/[controller]\")]public class MyController : ControllerBase{    private readonly ITracer _tracer;    public MyController(ITracer tracer)    {        _tracer = tracer;    }    [HttpGet]    public ActionResult&lt;string&gt; Get()    {        using (var scope = _tracer.BuildSpan(\"MyController.Get\").StartActive())        {            // Your API logic here            return \"Hello, World!\";        }    }}Step 4: Start Zipkin and Observe TracesStart your Zipkin server, and your C# Web API is now instrumented with Zipkin tracing. When you make requests to your API, you can use the Zipkin web UI to observe traces and troubleshoot issues in your microservices-based application.What Next?  That’s it! You’ve successfully integrated Zipkin with your C# Web API to monitor and trace requests across your microservices."
  },
  
  {
    "title": "SonarQube | Working with SonarLint and SonarQube in Visual Studio",
    "url": "/posts/Working-with-SonarLint-and-SonarQube-in-Visual-Studio/",
    "categories": "Best Practices, SonarQube",
    "tags": "microsoft, visualstudio, sonarqube, sonarlint",
    "date": "2023-08-05 00:00:00 +0200",
    





    
    "snippet": "Working with SonarLint and SonarQube in Visual StudioSonarLint and SonarQube are powerful tools for code quality and static code analysis in C# and other programming languages. They help you identi...",
    "content": "Working with SonarLint and SonarQube in Visual StudioSonarLint and SonarQube are powerful tools for code quality and static code analysis in C# and other programming languages. They help you identify and fix code issues and vulnerabilities. In this guide, we’ll walk you through how to set up and use SonarLint in Visual Studio and integrate it with SonarQube for more advanced analysis.Prerequisites      Visual Studio: Make sure you have Visual Studio installed on your machine. SonarLint works as a Visual Studio extension.        SonarQube Server: If you plan to use SonarQube for more advanced analysis, you’ll need access to a SonarQube server. You can install one locally or use a remote server.  Setting Up SonarLint1. Install SonarLint Extension:  Open Visual Studio.  Go to Extensions -&gt; Manage Extensions.  Search for “SonarLint” and install the extension.2. Binding to a SonarQube Server (Optional):  If you want to connect SonarLint to your SonarQube server for synchronized rules and quality profiles, go to Tools -&gt; Options -&gt; SonarLint.  Click “Connect to SonarQube” and provide the server URL and authentication details.3. Binding to SonarQube Projects (Optional):  If connected to a SonarQube server, you can bind your Visual Studio projects to SonarQube projects. This ensures that your code is analyzed using SonarQube rules.  Right-click on the project in Solution Explorer -&gt; SonarLint -&gt; Bind to SonarQube project.4. Analyzing Code:  SonarLint will automatically analyze your code in real-time as you work in Visual Studio.  Detected issues and suggestions will be highlighted in your code, and you can see details in the SonarLint window.Setting Up SonarQube IntegrationTo perform more advanced analysis and manage projects centrally, you can integrate SonarQube with Visual Studio.1. Install SonarQube Scanner for MSBuild:  Download and install the SonarQube Scanner for MSBuild.2. Configure SonarQube Server:  In your project’s root directory, create a sonar-project.properties file.  Configure it with your SonarQube server details.   sonar.host.url=http://your-sonarqube-server-url   sonar.login=your-auth-token   sonar.projectKey=unique-project-key3. Run Analysis:  Open a Command Prompt or PowerShell window and navigate to your project directory.  Run the following command to perform an analysis:   MSBuild.SonarQube.Runner.exe begin /k:\"your-project-key\"   MSBuild.exe   MSBuild.SonarQube.Runner.exe end4. View Results:  Visit your SonarQube server in a web browser to view the analysis results and manage your project.What Next?  With these steps, you can effectively use SonarLint for real-time code analysis within Visual Studio and integrate SonarQube for more advanced analysis, quality management, and reporting.You can consult the official SonarLint and SonarQube documentation for detailed setup and configuration instructions."
  },
  
  {
    "title": "Azure DevOps | Using Terraform",
    "url": "/posts/Azure-DevOps-Using-Terraform/",
    "categories": "Azure DevOps, Terraform",
    "tags": "microsoft, azuredevops, azure, terraform",
    "date": "2023-07-29 00:00:00 +0200",
    





    
    "snippet": "Using Terraform with Azure DevOpsAzure DevOps is a set of development tools and services for software development, while Terraform is a popular infrastructure as code (IaC) tool used to provision a...",
    "content": "Using Terraform with Azure DevOpsAzure DevOps is a set of development tools and services for software development, while Terraform is a popular infrastructure as code (IaC) tool used to provision and manage cloud resources. Combining the power of Terraform with Azure DevOps can help automate infrastructure deployment and management in your projects.In this guide, we’ll walk through the steps to set up and integrate Terraform with Azure DevOps.Prerequisites      Azure DevOps Account: You need an Azure DevOps account to get started. Sign up if you don’t have one.        Azure Subscription: Access to an Azure subscription to create and manage Azure resources.        Terraform: Install Terraform on your local machine.        Azure CLI: Install the Azure CLI.  Steps1. Create a New Azure DevOps Project      Log in to your Azure DevOps account.        Create a new project or select an existing one for your Terraform infrastructure.  2. Set Up a Git Repository      Create a Git repository within your Azure DevOps project to store your Terraform code.        Clone the repository to your local machine.  3. Terraform Configuration      Create a directory for your Terraform configurations within your Git repository.        Write your Terraform code for provisioning Azure resources in this directory.        Initialize your Terraform configuration:    terraform init            Create a main.tf file and define your Azure resources using the Azure Terraform Provider.        Use environment variables or other methods to securely store your Azure credentials and sensitive information.  4. Create an Azure DevOps Pipeline      In your Azure DevOps project, go to Pipelines and create a new pipeline.        Select the Git repository where your Terraform code is stored.        Choose a pipeline template based on your project’s needs. You can also create a YAML pipeline.        Configure the pipeline to execute Terraform commands (e.g., terraform init, terraform plan, and terraform apply) as tasks.  5. Securely Store Secrets  Use Azure DevOps Variable Groups or Azure Key Vault to securely manage and store secrets, such as Azure service principal credentials.6. Trigger the Pipeline      Commit your Terraform code to the Git repository.        Manually trigger the Azure DevOps pipeline or set up automatic triggers, such as on code commits or pull requests.  7. Monitor and Manage      Monitor the pipeline’s progress and logs through the Azure DevOps interface.        Leverage Azure DevOps features like approval gates and release environments for more advanced control and automation.  8. Cleanup      After you’re done with your resources, add Terraform code to destroy or deprovision them.        Run terraform destroy in your Azure DevOps pipeline when you’re ready to clean up your Azure resources.  By following these steps, you can set up a seamless integration between Terraform and Azure DevOps, automating your infrastructure provisioning and management processes.What Next?Using Terraform in conjunction with Azure DevOps can significantly enhance your infrastructure provisioning and management workflow. It allows for automation, version control, and collaboration, making your infrastructure as code projects more efficient and reliable."
  },
  
  {
    "title": "C# | Visual Studio Extensions for C# Developers",
    "url": "/posts/Visual-Studio-Extensions-for-C-Developers/",
    "categories": "Visual Studio, Tips And Tricks",
    "tags": "microsoft, visualstudio, tips&tricks",
    "date": "2023-07-24 00:00:00 +0200",
    





    
    "snippet": "Visual Studio extensions can significantly enhance your C# development workflow by adding features and tools that make coding, debugging, and project management more efficient. Here’s a list of ess...",
    "content": "Visual Studio extensions can significantly enhance your C# development workflow by adding features and tools that make coding, debugging, and project management more efficient. Here’s a list of essential extensions that every C# developer should consider installing.Coding Productivity      ReSharper: A powerful productivity tool that provides code analysis, quick-fixes, refactorings, and intelligent code completion.        Visual Studio IntelliCode: Utilizes AI to provide intelligent, context-aware code completion recommendations based on your coding patterns.        Visual Studio Live Share: Collaborative development tool that allows you to share your coding session with others in real-time.        CodeMaid: Helps maintain a cleaner codebase by organizing, formatting, and simplifying your code.  Debugging       OzCode: Advanced debugging tool that provides time-travel debugging, exceptional value tracking, and other debugging enhancements.        Debugger for Unity: If you’re working with Unity for game development, this extension adds debugging support for Unity projects.  Version Control  Visual Studio GitHub: Integrates GitHub with Visual Studio, providing seamless version control and code collaboration features.Code Analysis and Quality      SonarLint: A static code analysis tool that helps identify and fix code quality issues as you write your code.        Roslynator: Offers a wide range of code analyzers, refactorings, and code fixes based on the Roslyn compiler platform.  Project Management      NUnit Test Adapter: If you’re using NUnit for unit testing, this adapter allows you to run and debug NUnit tests within Visual Studio.        Visual Studio Installer Projects: Provides project templates for creating custom installation packages for your applications.        NuGet Package Manager: Manage NuGet packages directly within Visual Studio to easily add and update dependencies in your projects.  Documentation  GhostDoc: Simplifies the process of creating and maintaining code documentation by generating XML comments and helping with documentation standards.UI and Design  XAML Styler: Provides code formatting and styling for XAML markup, helping maintain clean and consistent UI code.Markdown Editing  Markdown Editor: If you’re writing documentation or READMEs in Markdown, this extension enhances the Markdown editing experience.Git Integration  Git Tools: Adds Git integration and makes it easier to manage Git repositories directly from Visual Studio.What Next?  These Visual Studio extensions can significantly improve your C# development experience. Install the ones that suit your needs and workflow to boost your productivity and code quality."
  },
  
  {
    "title": "Azure DevOps | Deploy Postman Tests in Azure DevOps Test Plans",
    "url": "/posts/Azure-DevOps-Deploy-Postman-Tests-in-Azure-DevOps-Test-Plans/",
    "categories": "Azure DevOps, Postman",
    "tags": "microsoft, azuredevops, azure, postman, tests",
    "date": "2023-07-22 00:00:00 +0200",
    





    
    "snippet": "Deploy Postman Tests in Azure DevOps Test PlansAzure DevOps allows you to automate the testing of your APIs and applications, and Postman is a popular tool for API testing. In this guide, we will w...",
    "content": "Deploy Postman Tests in Azure DevOps Test PlansAzure DevOps allows you to automate the testing of your APIs and applications, and Postman is a popular tool for API testing. In this guide, we will walk through the process of deploying Postman tests as part of your Azure DevOps pipeline.Prerequisites  An Azure DevOps account and a project set up.  Postman collection containing the tests you want to run.  A basic understanding of Azure DevOps pipelines.StepsStep 1: Add Postman Collection to Your Repository      Ensure your Postman collection is saved in a location that is accessible by your Azure DevOps repository. This can be the same repository or a shared location.        Commit the Postman collection to your repository, so it’s available for pipeline execution.  Step 2: Create an Azure DevOps Pipeline      In your Azure DevOps project, go to the “Pipelines” section.        Click on “New Pipeline” to create a new pipeline.        Select your repository as the source for the pipeline.        Choose a template or start with an “Empty job” if you want to configure your pipeline from scratch.  Step 3: Configure the Pipeline  In your pipeline YAML file, you can define a job to run Postman tests.jobs:- job: RunPostmanTests  steps:  - script: |      # Install Newman (Postman CLI)      npm install -g newman      # Run Postman collection      newman run path/to/your/postman_collection.json    displayName: 'Run Postman Tests'Make sure to replace path/to/your/postman_collection.json with the actual path to your Postman collection file.  You can also configure the pipeline to run these tests as part of a specific trigger, such as on every code commit or on a schedule.Step 4: Save and Trigger the Pipeline      Save your pipeline configuration.        Manually trigger the pipeline to verify that your Postman tests are executed.        Monitor the pipeline’s output for test results.  What Next?By deploying Postman tests within Azure DevOps, you can automate the testing of your APIs as part of your continuous integration and continuous delivery (CI/CD) process. This ensures that your API tests are consistently executed, helping you catch issues early in the development cycle."
  },
  
  {
    "title": "Azure DevOps | Running a Postman Collection using Newman Docker Image",
    "url": "/posts/Azure-DevOps-Running-a-Postman-Collection-using-Newman-Docker-Image/",
    "categories": "Azure DevOps, Postman",
    "tags": "microsoft, azuredevops, azure, postman, docker, newman",
    "date": "2023-07-20 00:00:00 +0200",
    





    
    "snippet": "Running a Postman Collection using Newman Docker ImageNewman is a command-line collection runner for Postman that allows you to automate and test your APIs. You can use the Newman Docker image to r...",
    "content": "Running a Postman Collection using Newman Docker ImageNewman is a command-line collection runner for Postman that allows you to automate and test your APIs. You can use the Newman Docker image to run Postman collections in a containerized environment.PrerequisitesBefore you begin, make sure you have Docker installed on your system.Step 1: Pull the Newman Docker ImageOpen your terminal and pull the Newman Docker image from Docker Hub using the following command:docker pull postman/newmanStep 2: Create a Postman CollectionCreate a Postman collection that includes the API requests you want to run. You can use the Postman app to create and export collections.Step 3: Export Your Postman CollectionExport your Postman collection as a JSON file. You can do this by opening the collection in Postman, clicking the “Export” button, and selecting “Collection v2” or “Collection v2.1” as the export format.Step 4: Run the Postman Collection using NewmanUse the Newman Docker image to run your Postman collection as follows:docker run -t postman/newman run &lt;path-to-collection-file.json&gt; --env-var VAR1=Value1 --env-var VAR2=Value2      &lt;path-to-collection-file.json&gt;: Replace this with the path to the Postman collection JSON file you exported in step 3.        --env-var VAR1=Value1 and --env-var VAR2=Value2: If your collection relies on environment variables, you can set them using the --env-var flag.  ExampleHere’s an example command to run a Postman collection named “my-api-tests.json” with environment variables:docker run -t postman/newman run my-api-tests.json --env-var API_URL=https://api.example.com --env-var API_KEY=your-api-keyAdditional OptionsYou can customize the Newman run using various Newman CLI options. For example, you can specify reporters, specify a folder for reports, and more.What Next?Using the Newman Docker image, you can easily automate the testing of your APIs by running Postman collections within a containerized environment. This is particularly useful for continuous integration and automated testing pipelines."
  },
  
  {
    "title": "C# | Using Entity Framework with PostgreSQL Database",
    "url": "/posts/C-Using-Entity-Framework-with-PostgreSQL-Database/",
    "categories": "C#, Entity Framework",
    "tags": "microsoft, csharp, c#, entityframework, postgresql, sql, database",
    "date": "2023-07-17 00:00:00 +0200",
    





    
    "snippet": "Using Entity Framework with PostgreSQL Database in C#In this guide, we will explore how to use C# Entity Framework Core to interact with a PostgreSQL database. Entity Framework Core is a powerful O...",
    "content": "Using Entity Framework with PostgreSQL Database in C#In this guide, we will explore how to use C# Entity Framework Core to interact with a PostgreSQL database. Entity Framework Core is a powerful Object-Relational Mapping (ORM) framework that simplifies database operations in C# applications. PostgreSQL is a robust, open-source relational database management system.Prerequisites  Basic knowledge of C# programming.  Visual Studio or Visual Studio Code.  .NET Core SDK installed.  PostgreSQL Database Server installed and running.SetupInstall Required PackagesTo use Entity Framework Core with PostgreSQL, you need to install the following NuGet packages:  Microsoft.EntityFrameworkCore  Npgsql.EntityFrameworkCore.PostgreSQLYou can install them using the NuGet Package Manager or the .csproj file.Database Connection StringMake sure to configure a connection string to your PostgreSQL database in your application’s configuration. The connection string typically looks like this:Server=myserver;Port=myport;Database=mydatabase;User Id=hbolajraf;Password=azerty_:=);Creating a ModelA model represents a database table. Create a C# class for your model, and annotate it with the [Key] attribute for the primary key and other attributes to define the table structure.public class Product{    [Key]    public int Id { get; set; }    public string Name { get; set; }    public decimal Price { get; set; }}Database ContextA database context represents a session with the database. Create a database context class that derives from DbContext and includes a DbSet for each model.public class AppDbContext : DbContext{    public AppDbContext(DbContextOptions&lt;AppDbContext&gt; options) : base(options) { }    public DbSet&lt;Product&gt; Products { get; set; }}MigrationEntity Framework Core uses migrations to create and update the database schema. Run the following commands to create and apply migrations:dotnet ef migrations add InitialCreatedotnet ef database updateThis will create the database schema based on your model.CRUD OperationsYou can now perform CRUD (Create, Read, Update, Delete) operations on your database using Entity Framework Core. Here are some examples:  Create:using (var context = new AppDbContext()){    var newProduct = new Product { Name = \"Tonic Bimo\", Price = 10.99 };    context.Products.Add(newProduct);    context.SaveChanges();}  Read:using (var context = new AppDbContext()){    var products = context.Products.ToList();}  Update:using (var context = new AppDbContext()){    var product = context.Products.Find(1);    if (product != null)    {        product.Price = 20.99;        context.SaveChanges();    }}  Delete:using (var context = new AppDbContext()){    var product = context.Products.Find(1);    if (product != null)    {        context.Products.Remove(product);        context.SaveChanges();    }}Querying the DatabaseEntity Framework Core provides a powerful query API. You can use LINQ queries to retrieve data from the database.using (var context = new AppDbContext()){    var expensiveProducts = context.Products        .Where(p =&gt; p.Price &gt; 20)        .ToList();}You can write complex queries and join multiple tables using LINQ.What Next?Entity Framework Core simplifies working with a PostgreSQL database in C# applications. With the steps outlined in this guide, you can create, read, update, and delete data, as well as perform complex queries, all with the power and flexibility of Entity Framework Core and the reliability of PostgreSQL."
  },
  
  {
    "title": "Azure DevOps | Installing Postman and Newman using npm",
    "url": "/posts/Azure-DevOps-Installing-Postman-and-Newman-using-npm/",
    "categories": "Azure DevOps, Postman",
    "tags": "microsoft, azuredevops, azure, postman, npm, newman",
    "date": "2023-07-14 00:00:00 +0200",
    





    
    "snippet": "Installing Postman and Newman with npmIn this guide, we will walk through the steps to install Postman and Newman using npm (Node Package Manager) and how to run a Postman collection using Newman.P...",
    "content": "Installing Postman and Newman with npmIn this guide, we will walk through the steps to install Postman and Newman using npm (Node Package Manager) and how to run a Postman collection using Newman.Prerequisites  Node.js and npm installed on your system.Install PostmanPostman provides a user-friendly graphical interface for creating and managing API requests and collections. To install Postman:      Open your terminal or command prompt.        Use npm to install the Postman command-line tool:    npm install -g postman            Once the installation is complete, you can run Postman by typing postman in your terminal.  Install NewmanNewman is a command-line collection runner for Postman that allows you to run and automate Postman collections. To install Newman:      Open your terminal or command prompt.        Use npm to install Newman globally:    npm install -g newman            Once the installation is complete, you can use Newman to run Postman collections from the command line.  Run a Postman CollectionNow that you have Postman and Newman installed, let’s run a Postman collection using Newman.      Export a Postman collection from the Postman application if you haven’t already. Save it as a JSON file (e.g., example_collection.json).        Open your terminal or command prompt.        Run the Postman collection using Newman:    newman run example_collection.json        Replace example_collection.json with the name of your Postman collection file.        Newman will execute the requests in your collection and provide a summary of the results, including the number of requests run and their status.  What Next?You’ve successfully installed Postman and Newman and used Newman to run a Postman collection from the command line. This is useful for automating API testing and integration into your CI/CD pipelines."
  },
  
  {
    "title": "C# | Entity Framework Generic Repository with SOLID Design Pattern",
    "url": "/posts/C-Entity-Framework-Generic-Repository-with-SOLID-Design-Pattern/",
    "categories": "C#, Entity Framework",
    "tags": "microsoft, csharp, c#, entityframework, solid, designpattern, repository",
    "date": "2023-07-09 00:00:00 +0200",
    





    
    "snippet": "Entity Framework Generic Repository with SOLID Design Pattern in C#In this guide, we’ll explore how to create a generic repository in a C# application using Entity Framework while adhering to SOLID...",
    "content": "Entity Framework Generic Repository with SOLID Design Pattern in C#In this guide, we’ll explore how to create a generic repository in a C# application using Entity Framework while adhering to SOLID design principles. A generic repository allows you to interact with the database in a more organized and reusable manner.PrerequisitesBefore you begin, ensure you have the following prerequisites:  Basic knowledge of C# and Entity Framework.  An existing C# project with Entity Framework setup.SOLID Design PrinciplesWe’ll focus on the following SOLID principles:  Single Responsibility Principle (SRP): Each class should have a single reason to change.  Open-Closed Principle (OCP): Software entities (classes, modules, functions) should be open for extension but closed for modification.  Liskov Substitution Principle (LSP): Subtypes must be substitutable for their base types.  Interface Segregation Principle (ISP): A client should not be forced to implement interfaces they do not use.  Dependency Inversion Principle (DIP): High-level modules should not depend on low-level modules. Both should depend on abstractions.Creating a Generic Repository  Create a Generic Repository Interface:    public interface IRepository&lt;T&gt; where T : class{    Task&lt;T&gt; GetByIdAsync(int id);    Task&lt;IEnumerable&lt;T&gt;&gt; GetAllAsync();    Task AddAsync(T entity);    Task UpdateAsync(T entity);    Task DeleteAsync(T entity);}        Implement the Generic Repository:    public class Repository&lt;T&gt; : IRepository&lt;T&gt; where T : class{    private readonly DbContext _context;    public Repository(DbContext context)    {        _context = context;    }    public async Task&lt;T&gt; GetByIdAsync(int id)    {        return await _context.Set&lt;T&gt;().FindAsync(id);    }    public async Task&lt;IEnumerable&lt;T&gt;&gt; GetAllAsync()    {        return await _context.Set&lt;T&gt;().ToListAsync();    }    public async Task AddAsync(T entity)    {        await _context.Set&lt;T&gt;().AddAsync(entity);    }    public async Task UpdateAsync(T entity)    {        _context.Set&lt;T&gt;().Update(entity);    }    public async Task DeleteAsync(T entity)    {        _context.Set&lt;T&gt;().Remove(entity);    }}      Using the Generic Repository      Dependency Injection:In your application’s startup or configuration, inject the repository into your services.    services.AddScoped(typeof(IRepository&lt;&gt;), typeof(Repository&lt;&gt;));            Using the Repository:In your services or controllers, use the generic repository to interact with the database.    public class MyService{    private readonly IRepository&lt;MyEntity&gt; _repository;    public MyService(IRepository&lt;MyEntity&gt; repository)    {        _repository = repository;    }    public async Task&lt;MyEntity&gt; GetEntityById(int id)    {        return await _repository.GetByIdAsync(id);    }    // Implement other methods as needed}            Apply SOLID Principles:Ensure your services adhere to SOLID principles, like separating concerns and following the Single Responsibility Principle, when implementing business logic.  What Next?Creating a generic repository using Entity Framework and adhering to SOLID design principles can make your C# application more maintainable and scalable. By using this pattern, you can easily extend and modify your data access layer without affecting the rest of your application.Remember to adapt the code and principles to your specific project’s needs and requirements."
  },
  
  {
    "title": "C# | Using the yield Keyword",
    "url": "/posts/C-Using-the-yield-Keyword/",
    "categories": "C#, Best Practices",
    "tags": "microsoft, csharp, c#, bestpractices",
    "date": "2023-07-08 00:00:00 +0200",
    





    
    "snippet": "Using the yield Keyword in C#In C#, the yield keyword is used to create an iterator. It enables you to efficiently process a sequence of data one element at a time, without having to generate the e...",
    "content": "Using the yield Keyword in C#In C#, the yield keyword is used to create an iterator. It enables you to efficiently process a sequence of data one element at a time, without having to generate the entire sequence in memory. This can be particularly useful for working with large data sets or when you want to generate elements on-the-fly.How yield WorksWhen you use yield in a method, it signals to the C# compiler that this method should be treated as an iterator. The method can then produce a sequence of values using the yield return statement. The iterator method, when called, will execute up to the first yield return statement and then pause. When the consumer requests the next element, the method continues execution from where it left off, generating the next value. This process continues until all values have been generated, and the iterator method is considered exhausted.Here is a simple example of how to use the yield keyword to create an iterator:using System;using System.Collections.Generic;public class IteratorDemo{    public static IEnumerable&lt;int&gt; GenerateNumbers()    {        for (int i = 0; i &lt; 10; i++)        {            yield return i;        }    }    public static void Main(string[] args)    {        foreach (var number in GenerateNumbers())        {            Console.WriteLine(number);        }    }}In this example, the GenerateNumbers method is an iterator that generates numbers from 0 to 9 using yield return. The foreach loop then iterates through the generated numbers.Benefits of yieldUsing yield provides several benefits:      Memory Efficiency: yield allows you to work with large data sets without loading the entire set into memory, which can save memory and improve performance.        Lazy Evaluation: Elements are generated on-the-fly as they are needed, enabling lazy evaluation and efficient resource utilization.        Simplified Code: yield can simplify the code for generating sequences, making it more readable and maintainable.        Deferred Execution: The execution of the iterator is deferred until you request elements, making it possible to generate data progressively.  Common Use Cases      File Parsing: When parsing large log files, you can use yield to read and process one line at a time.        Database Queries: yield can be used with databases to fetch and process records as needed.        Infinite Sequences: Create iterators for sequences that are theoretically infinite, such as a sequence of prime numbers.        Custom Collections: Implement custom collections with yield to provide efficient enumeration.  What Next?The yield keyword is a powerful feature in C# that allows you to work with sequences of data in a memory-efficient and convenient way. It is especially useful when dealing with large data sets or when you want to defer the execution of data generation until it is needed."
  },
  
  {
    "title": "C# | Using yield with Entity Framework",
    "url": "/posts/C-Using-yield-with-Entity-Framework/",
    "categories": "C#, Best Practices",
    "tags": "microsoft, csharp, c#, bestpractices",
    "date": "2023-07-07 00:00:00 +0200",
    





    
    "snippet": "Using yield in C# with Entity FrameworkIn C#, the yield keyword is used to create an iterator. It allows you to return a sequence of values one at a time, which is particularly useful when working ...",
    "content": "Using yield in C# with Entity FrameworkIn C#, the yield keyword is used to create an iterator. It allows you to return a sequence of values one at a time, which is particularly useful when working with large data sets or when you want to generate values lazily. In this guide, we’ll explore how to use yield with Entity Framework to retrieve and manipulate data efficiently.Understanding yieldThe yield keyword is often used in C# when defining an iterator method. It allows you to return a sequence of values without having to load the entire set into memory at once. Instead, it generates each value on-the-fly as requested.  An iterator method uses the yield return statement to produce each item in the sequence.  The calling code can iterate over the sequence using a foreach loop or other enumeration methods.  The method’s state is preserved between calls, so it continues where it left off.Using yield with Entity FrameworkEntity Framework is an Object-Relational Mapping (ORM) framework that allows you to work with databases using C#. You can combine yield with Entity Framework to efficiently retrieve and process data from a database.Here’s how to use yield with Entity Framework:      Create an Entity Framework Data Context: Define an Entity Framework data context that connects to your database.        Define a Query Method: Create a method that returns an IEnumerable&lt;T&gt; using the yield keyword. This method will represent your query.        Use the Query Method: Call the query method to retrieve data. Since it uses yield, the data will be streamed one item at a time, reducing memory usage.  Example: Retrieving Data with yield and Entity FrameworkLet’s see an example of how to use yield with Entity Framework to retrieve a list of products from a database.public class Product{    public int ProductId { get; set; }    public string Name { get; set; }    public decimal Price { get; set; }}public class MyDbContext : DbContext{    public DbSet&lt;Product&gt; Products { get; set; }}public class ProductRepository{    private readonly MyDbContext dbContext;    public ProductRepository(MyDbContext context)    {        dbContext = context;    }    public IEnumerable&lt;Product&gt; GetProducts()    {        foreach (var product in dbContext.Products)        {            yield return product;        }    }}In this example, the GetProducts method uses yield to stream the products from the database one at a time, reducing memory consumption.What Next?Using the yield keyword with Entity Framework can help you efficiently work with large data sets from a database by streaming data one item at a time. This approach can lead to improved performance and reduced memory usage when working with data in C# applications."
  },
  
  {
    "title": "Azure devops | Creating free account",
    "url": "/posts/Creating-free-account/",
    "categories": "Azure, Azure devops",
    "tags": "microsoft, azuredevops",
    "date": "2023-07-05 00:00:00 +0200",
    





    
    "snippet": "Create Azure devops free accountAs of my last knowledge update in January 2022, Azure DevOps Services, which provides a set of development tools, including version control, build automation, releas...",
    "content": "Create Azure devops free accountAs of my last knowledge update in January 2022, Azure DevOps Services, which provides a set of development tools, including version control, build automation, release management, and more, offers a free tier that you can use to get started. Follow below steps in order to create a free Azure DevOps account.1. Visit the Azure DevOps Services Website: Open your web browser and go to the Azure DevOps Services website at https://azure.com/devops.2. Sign Up for Azure DevOps:      Click on the “Get started for free” or “Start free” button on the homepage.        You’ll be prompted to sign in with your Microsoft account (formerly known as Live ID). If you don’t have one, you can create a new Microsoft account.        Follow the sign-up process, which may include verifying your email and providing some basic information.  3. Set Up Your Organization:      Once you’ve signed in, you’ll need to create an organization. An organization in Azure DevOps is a way to group your projects and manage access.        Provide a unique name for your organization, and choose your region and time zone.        Click “Continue.”  4. Choose a Free Plan:      Azure DevOps offers a free plan that includes up to 5 users and unlimited stakeholders. You can select this free plan during the setup process.        You can also choose to start a free trial of Azure DevOps, which may include additional features beyond the free tier.  5. Configure Additional Settings:  Follow the setup wizard to configure additional settings for your organization. This may include choosing a project management process (Agile, Scrum, etc.), and you can also set up repositories, pipelines, boards, and other services.6. Start Using Azure DevOps:  After completing the setup, you’ll be directed to your Azure DevOps organization. From there, you can create projects, repositories, and start managing your software development process.What Next?  Please note that while Azure DevOps offers a free tier, there may be some limitations on the number of users and resources in the free plan. Be sure to check the Azure DevOps pricing and documentation for the most up-to-date information on what’s included in the free tier.Keep in mind that the process or offerings might have changed since my last update in January 2022, so I recommend visiting the Azure DevOps website for the most current information and to set up your free account."
  },
  
  {
    "title": "C# | Best Practices",
    "url": "/posts/C-Best-Practices/",
    "categories": "C#, Best Practices",
    "tags": "microsoft, csharp, c#, bestpractices, tips&tricks",
    "date": "2023-07-02 00:00:00 +0200",
    





    
    "snippet": "C# Best PracticesThese best practices are designed to help you write clean, efficient, and maintainable C# code.1. Follow Naming Conventions  Use PascalCase for class names, method names, and prope...",
    "content": "C# Best PracticesThese best practices are designed to help you write clean, efficient, and maintainable C# code.1. Follow Naming Conventions  Use PascalCase for class names, method names, and properties (e.g., MyClass, MyMethod, MyProperty).  Use camelCase for local variables and method parameters (e.g., myVariable, myParameter).  Use ALL_CAPS for constants (e.g., MY_CONSTANT).2. Use Meaningful Names  Choose descriptive and meaningful names for your variables, classes, and methods.  Avoid abbreviations and single-letter variable names unless they are widely accepted (e.g., i, j, k for loop counters).3. Organize Your Code  Use regions and comments to clearly structure your code into logical sections.  Organize your files into namespaces that reflect the functionality of your code.4. Follow the DRY Principle (Don’t Repeat Yourself)  Refactor code to eliminate duplication. If you find the same code in multiple places, create a reusable method or class.5. Use Exception Handling Wisely  Only catch exceptions when you can handle them appropriately.  Use specific exception types rather than catching Exception for better error handling.6. Use Code Documentation  Document your code using XML comments for classes, methods, and properties.  Provide clear and concise explanations of what the code does and how to use it.7. Keep Methods Small and Focused  Aim for methods that do one thing and do it well.  If a method is too long, consider breaking it into smaller, more focused methods.8. Use Dependency Injection  Favor dependency injection over hardcoding dependencies in your classes.  Use interfaces to define contracts and make your code more testable.9. Write Unit Tests  Create unit tests for your code to ensure it functions as expected.  Use a testing framework like MSTest, NUnit, or xUnit.10. Use Source Control  Use a version control system like Git to track changes to your code.  Commit and push code regularly to ensure a history of changes.11. Optimize Performance  Profile your code to identify performance bottlenecks.  Use appropriate data structures and algorithms for efficient processing.12. Keep an Eye on Security  Avoid raw SQL queries and use parameterized queries to prevent SQL injection.  Validate and sanitize user inputs to protect against security vulnerabilities.13. Follow SOLID Principles  Strive to adhere to the SOLID principles: Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, and Dependency Inversion.14. Review Code  Conduct code reviews with peers to catch issues early and share knowledge.  Use code analysis tools and linters to automate code review processes.15. Stay Up-to-Date  Keep up with the latest C# features and best practices by reading blogs, books, and attending conferences.What Next?  By following these best practices, you can write C# code that is easier to read, maintain, and extend.Remember that good coding practices evolve, so always be open to learning and adapting to new techniques and tools."
  },
  
  {
    "title": "C# | Asynchronous programming with [async | await | Task]",
    "url": "/posts/C-Asynchronous-programming-with-async-await-and-Task/",
    "categories": "C#, Best Practices",
    "tags": "microsoft, csharp, c#, bestpractices",
    "date": "2023-07-01 00:00:00 +0200",
    





    
    "snippet": "Using await and async Task in C#In C#, asynchronous programming is used to improve the responsiveness of applications by allowing tasks to run concurrently without blocking the main thread. The awa...",
    "content": "Using await and async Task in C#In C#, asynchronous programming is used to improve the responsiveness of applications by allowing tasks to run concurrently without blocking the main thread. The await and async keywords play a crucial role in achieving this. This guide will show you how to use await and async Task effectively.IntroductionAsynchronous programming in C# is essential for tasks that may take a significant amount of time, such as I/O-bound operations or network requests. By using await and async, you can ensure that your application remains responsive while waiting for these tasks to complete.Using async Task      Define an async Method: To use await, define an async method within a class, typically returning a Task or Task&lt;T&gt;.    public async Task MyAsyncMethod(){    // Asynchronous code here}            Call the async Method: In another method, you can call your async method with the await keyword.    await MyAsyncMethod();      The await KeywordThe await keyword is used within an async method to pause execution until the awaited task completes. It allows the calling thread to continue other work without blocking.async Task MyAsyncMethod(){    var result = await SomeAsyncTask();    // Code after the await will execute when SomeAsyncTask is completed.}Exception HandlingTo handle exceptions in asynchronous methods, you can use standard try-catch blocks. When an exception is thrown in an async method, it’s captured and propagated as part of the Task.try{    await SomeAsyncMethod();}catch (Exception ex){    // Handle the exception}Cancellation TokensTo cancel an asynchronous operation, you can use CancellationToken. Pass a CancellationToken to the asynchronous method, and check for cancellation within the method.async Task MyAsyncMethod(CancellationToken cancellationToken){    // Check for cancellation    cancellationToken.ThrowIfCancellationRequested();    // Continue with the operation}Real-world ExampleHere’s an example of a common real-world scenario: making an HTTP request asynchronously.public async Task&lt;string&gt; FetchDataAsync(string url){    using (var httpClient = new HttpClient())    {        var response = await httpClient.GetAsync(url);        response.EnsureSuccessStatusCode();        return await response.Content.ReadAsStringAsync();    }}What Next?Using await and async Task in C# allows you to write responsive and efficient applications, especially when dealing with I/O-bound or long-running tasks. It enables you to keep your application responsive and improve the overall user experience by allowing multiple tasks to run concurrently without blocking the main thread."
  },
  
  {
    "title": "C# | Web Api's Tips and Tricks",
    "url": "/posts/C-Web-Api's-Tips-and-Tricks/",
    "categories": "C#, Tips And Tricks",
    "tags": "microsoft, csharp, c#, webapi, tips&tricks",
    "date": "2023-06-22 00:00:00 +0200",
    





    
    "snippet": "Web Api’s Tips and TricksBuilding Web APIs in C# is a powerful way to create scalable and efficient backend services. Here are some tips and tricks to help you get the most out of your C# Web API d...",
    "content": "Web Api’s Tips and TricksBuilding Web APIs in C# is a powerful way to create scalable and efficient backend services. Here are some tips and tricks to help you get the most out of your C# Web API development.1. Use ASP.NET Core  Start with ASP.NET Core for building Web APIs. It’s a cross-platform, high-performance framework with excellent support for RESTful services.2. RESTful Routes  Follow RESTful principles when designing your API endpoints. Use HTTP methods (GET, POST, PUT, DELETE) for CRUD operations and use nouns to represent resources.3. Model Validation  Leverage model validation attributes (e.g., [Required], [MaxLength], [RegularExpression]) to validate incoming data, ensuring data integrity and security.4. Versioning  Consider versioning your API from the beginning to maintain backward compatibility as your API evolves. You can use URL versioning, header versioning, or content negotiation for versioning.5. Use DTOs  Data Transfer Objects (DTOs) are essential for decoupling your API from your database models. They allow you to control what data is exposed and simplify data transformation.6. Dependency Injection  Leverage ASP.NET Core’s built-in dependency injection to manage the lifecycle of your services. This promotes loose coupling and testability.7. Middleware  ASP.NET Core middleware allows you to insert custom processing logic into the request/response pipeline. You can use it for tasks like authentication, logging, and exception handling.8. Authentication and Authorization  Implement secure authentication and authorization mechanisms, such as JWT (JSON Web Tokens) or OAuth, to protect your API endpoints.9. Pagination  For endpoints that return large datasets, implement pagination to improve performance and usability. Use query parameters like page and pageSize to control data retrieval.10. Logging and Error Handling  Set up comprehensive logging to track API usage and errors. Implement global exception handling to provide meaningful error responses to clients.11. Caching  Use response caching and distributed caching to reduce server load and improve response times for frequently accessed data.12. API Documentation  Create clear and comprehensive API documentation using tools like Swagger or OpenAPI to help clients understand how to interact with your API.13. Testing  Adopt a testing strategy that includes unit tests and integration tests to ensure the reliability and correctness of your API.14. Security  Protect your API from common security threats, such as SQL injection and cross-site scripting (XSS), by validating and sanitizing user inputs.15. Performance Optimization  Optimize your API for performance by using techniques like asynchronous programming, minimizing database queries, and reducing unnecessary data transfer.16. Rate Limiting  Implement rate limiting to prevent abuse of your API by limiting the number of requests a client can make in a given time frame.17. Continuous Integration and Deployment (CI/CD)  Set up CI/CD pipelines to automate the build, testing, and deployment of your Web API, ensuring a smooth release process."
  },
  
  {
    "title": "C# | Common Errors",
    "url": "/posts/C-Common-Errors/",
    "categories": "C#, Best Practices",
    "tags": "microsoft, csharp, c#, bestpractices",
    "date": "2023-06-18 00:00:00 +0200",
    





    
    "snippet": "Common Errors in C#C# is a powerful programming language, but like any language, it has its share of common errors that developers may encounter. Understanding these errors and their solutions can ...",
    "content": "Common Errors in C#C# is a powerful programming language, but like any language, it has its share of common errors that developers may encounter. Understanding these errors and their solutions can help improve your coding skills and productivity.NullReferenceExceptionDescription: This error occurs when you try to access a member (method or property) of an object that is currently set to null.Common Causes:  Accessing an uninitialized object.  Accessing a property or method of an object after it has been set to null.  Solution: Ensure that the object is properly initialized before accessing its members. Use null checks (if (obj != null)) or use the null-conditional operator (obj?.Method()).IndexOutOfRangeExceptionDescription: This error occurs when you attempt to access an element of an array or collection using an index that is out of its bounds.Common Causes:  Accessing an array or collection with an index that is too large or too small.  Solution: Check the length of the array or collection before accessing elements and make sure the index is within the valid range.ArgumentExceptionDescription: This error is thrown when an argument provided to a method is not valid.Common Causes:  Passing invalid or unexpected arguments to a method.  Using incorrect argument types or values.  Solution: Ensure that you are passing valid arguments to methods. Read documentation and method signatures to understand the expected arguments.FileNotFoundExceptionDescription: This error is raised when an attempt to access a file fails because the specified file does not exist.Common Causes:  Providing an incorrect or non-existent file path.  Solution: Verify that the file exists at the specified path or handle the exception to provide appropriate feedback to the user.Syntax ErrorsDescription: Syntax errors occur when your code does not conform to the C# language syntax rules.Common Causes:  Mismatched parentheses, brackets, or curly braces.  Misspelled keywords or identifiers.  Incorrect use of operators.  Solution: Carefully review the code and correct the syntax errors indicated by the compiler.Unhandled ExceptionsDescription: Unhandled exceptions cause the application to crash when they are not properly caught and handled in your code.Common Causes:  Failing to use try-catch blocks to handle exceptions.  Not anticipating and handling specific exceptions that can occur in your code.  Solution: Use try-catch blocks to catch and handle exceptions or use higher-level exception handling mechanisms to gracefully handle errors.Resource LeaksDescription: Resource leaks occur when you do not properly release resources like file handles, database connections, or memory.Common Causes:  Failing to close or dispose of resources when they are no longer needed.  Not using using statements for disposable objects.  Solution: Always release resources explicitly or use using statements to ensure resources are properly cleaned up.What Next?Understanding and addressing these common C# errors will help you write more robust and reliable code. Learning to diagnose and fix errors is an essential skill for any C# developer."
  },
  
  {
    "title": "C# | Tips and tricks",
    "url": "/posts/C-Tips-and-tricks/",
    "categories": "C#, Tips And Tricks",
    "tags": "microsoft, csharp, c#, bestpractices, tips&tricks",
    "date": "2023-06-16 00:00:00 +0200",
    





    
    "snippet": "C# tips and tricksC# is a versatile programming language that offers many features and techniques to make your coding more efficient and maintainable. In this document, we’ll explore some useful ti...",
    "content": "C# tips and tricksC# is a versatile programming language that offers many features and techniques to make your coding more efficient and maintainable. In this document, we’ll explore some useful tips and tricks for C# development.1. String InterpolationString interpolation allows you to embed expressions directly within string literals. It’s a cleaner and more readable way to concatenate strings and variables.string name = \"Hassan\";int age = 35;string message = $\"Hello, {name}! You are {age} years old.\";2. Null Conditional OperatorThe null-conditional operator (?.) simplifies null checks, making your code more concise and less error-prone.int? length = text?.Length;3. DeconstructionDeconstruction allows you to assign values from a tuple or an object to separate variables in a single line.var (x, y) = GetCoordinates();4. Pattern MatchingPattern matching simplifies conditional statements by checking for specific patterns in data, making your code more readable.if (obj is int number){    // Use 'number' as an int}5. Local FunctionsLocal functions are functions defined within another method, making your code more modular and improving encapsulation.int Calculate(int a, int b){    int Add(int x, int y) =&gt; x + y;    return Add(a, b);}6. LINQ (Language Integrated Query)LINQ allows for elegant and efficient querying of collections and databases.var result = from person in people             where person.Age &gt; 35             select person.Name;7. Ternary OperatorThe ternary operator is a concise way to write simple conditional expressions.string result = (condition) ? \"True\" : \"False\";8. Using StatementThe using statement simplifies resource management, ensuring that disposable objects are properly disposed of when no longer needed.using (var stream = new FileStream(\"file.txt\", FileMode.Open)){    // Work with the file stream}9. Async/AwaitAsync and await make asynchronous programming more readable and maintainable.async Task&lt;string&gt; DownloadAsync(string url){    var data = await DownloadDataAsync(url);    return Encoding.UTF8.GetString(data);}10. Extension MethodsYou can add new methods to existing types using extension methods, enhancing code reusability.public static class StringExtensions{    public static bool IsNullOrEmpty(this string value)    {        return string.IsNullOrEmpty(value);    }}What Next?  These are just a few of the many tips and tricks that can help you become a more proficient C# developer. As you continue to work with C#, explore its vast ecosystem to improve your skills and productivity."
  },
  
  {
    "title": "C# | Entity Framework Issues and Troubleshooting",
    "url": "/posts/C-Entity-Framework-Issues-and-Troubleshooting/",
    "categories": "C#, Entity Framework",
    "tags": "microsoft, csharp, c#, entityframework, bestpractices",
    "date": "2023-06-11 00:00:00 +0200",
    





    
    "snippet": "Entity Framework Issues and TroubleshootingEntity Framework is a powerful ORM (Object-Relational Mapping) framework for C# applications, but it can sometimes lead to issues that developers need to ...",
    "content": "Entity Framework Issues and TroubleshootingEntity Framework is a powerful ORM (Object-Relational Mapping) framework for C# applications, but it can sometimes lead to issues that developers need to address. Here are some common Entity Framework issues and tips on how to troubleshoot them.1. Connection and Configuration Issues1.1 Unable to Connect to Database  Issue: Entity Framework cannot establish a connection to the database.  Troubleshooting:          Check the connection string in your appsettings.json or web.config file.      Ensure the database server is running.      Verify your database server’s firewall rules.      1.2 Configuration Problems  Issue: Incorrect configuration of Entity Framework.  Troubleshooting:          Double-check your OnConfiguring method in the DbContext class.      Verify that the Entity Framework version matches your project.      2. Query and Performance Issues2.1 Slow Queries  Issue: Queries executed by Entity Framework are slow.  Troubleshooting:          Use SQL Server Profiler to analyze generated SQL queries.      Consider using indexing on database columns.      Optimize your LINQ queries for better performance.      2.2 N+1 Query Problem  Issue: The N+1 query problem occurs when Entity Framework generates too many SQL queries.  Troubleshooting:          Use eager loading with .Include() or .ThenInclude() to fetch related data.      Avoid lazy loading when it’s not necessary.      3. Data Migrations and Schema Changes3.1 Migration Failures  Issue: Data migration or schema update fails.  Troubleshooting:          Check the migration script for errors.      Ensure that the data model matches the database schema.      3.2 Model Changes Not Reflected  Issue: Changes to your data model are not reflected in the database.  Troubleshooting:          Create a new migration using Add-Migration in the Package Manager Console.      Update the database using Update-Database.      4. DbContext and Entity State Management4.1 DbContext Lifetime  Issue: DbContext lifetime management problems.  Troubleshooting:          Use a scoped or request-specific DbContext for web applications.      Dispose of DbContext instances properly.      4.2 Entity State Not Updating  Issue: Entity state is not being updated after changes.  Troubleshooting:          Call SaveChanges() after modifying entities.      Check for validation errors that may prevent changes from being saved.      5. Miscellaneous Issues5.1 Exception Handling  Issue: Unhandled exceptions during Entity Framework operations.  Troubleshooting: Implement exception handling in your code to gracefully handle errors.5.2 Asynchronous Code  Issue: Incorrect use of asynchronous code with Entity Framework.  Troubleshooting: Ensure you are using asynchronous methods properly, and don’t mix synchronous and asynchronous calls.What Next?Remember that Entity Framework issues can vary depending on the project, database, and configuration. When facing problems, check documentation, online forums, and communities for more specific guidance.This guide should help you identify and troubleshoot common issues when working with Entity Framework in C# applications. If you encounter any other issues, don’t hesitate to seek assistance from the Entity Framework community or forums."
  },
  
  {
    "title": "SQL | Database Query Optimization with Examples",
    "url": "/posts/SQL-Database-Query-Optimization-with-Examples/",
    "categories": "SQL, Optimization",
    "tags": "microsoft, sql, database, optimization",
    "date": "2023-06-03 00:00:00 +0200",
    





    
    "snippet": "Database Query Optimization with ExamplesDatabase query optimization is crucial for improving the performance of your application and reducing the response time of database queries. In this guide, ...",
    "content": "Database Query Optimization with ExamplesDatabase query optimization is crucial for improving the performance of your application and reducing the response time of database queries. In this guide, we’ll explore various optimization techniques with practical examples.IndexingIndexes help the database engine locate and retrieve rows from tables more efficiently. Using indexes can significantly speed up queries.Example:-- Create an index on the 'email' column of the 'users' tableCREATE INDEX idx_users_email ON users(email);-- Query using the indexed columnSELECT * FROM users WHERE email = 'user@example.com';Query RewritingRewriting queries can lead to more optimized execution plans, reducing the query’s execution time.Example:-- Original querySELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31';-- Optimized query using inequalitiesSELECT * FROM orders WHERE order_date &gt;= '2023-01-01' AND order_date &lt;= '2023-12-31';Avoiding SELECT *Avoiding SELECT * and specifying only the necessary columns reduces the amount of data transferred from the database to the application, improving query performance.Example:-- Avoid SELECT *SELECT order_id, order_date, total_amount FROM orders WHERE customer_id = 123;Use of LIMIT and OFFSETWhen you don’t need to retrieve all matching rows, using LIMIT and OFFSET can reduce the amount of data processed and improve query performance.Example:-- Retrieve 10 results starting from the 20th rowSELECT * FROM products LIMIT 10 OFFSET 20;NormalizationNormalization is the process of organizing data in a database to eliminate redundancy and improve data integrity. It can help reduce the size of the data and optimize query performance.Example:Consider a denormalized table:-- Denormalized tableCREATE TABLE orders (    order_id INT PRIMARY KEY,    customer_name VARCHAR(255),    product_name VARCHAR(255),    total_amount DECIMAL);Normalize the table to separate customers and products:-- Customers tableCREATE TABLE customers (    customer_id INT PRIMARY KEY,    customer_name VARCHAR(255));-- Products tableCREATE TABLE products (    product_id INT PRIMARY KEY,    product_name VARCHAR(255));-- Normalized orders tableCREATE TABLE orders (    order_id INT PRIMARY KEY,    customer_id INT,    product_id INT,    total_amount DECIMAL);What Next?By following normalization principles, you can improve data integrity and reduce data redundancy.Optimizing database queries is an ongoing process that depends on the specific database system and application requirements. These techniques and examples provide a foundation for improving query performance, but it’s essential to analyze query execution plans and monitor database performance to achieve the best results."
  },
  
  {
    "title": "Kubernetes | Helm Commands with YAML File Examples",
    "url": "/posts/Kubernetes-Helm-Commands-with-YAML-File-Examples/",
    "categories": "Kubernetes, Helm",
    "tags": "microsoft, kubernetes, helm, k8s",
    "date": "2023-05-25 00:00:00 +0200",
    





    
    "snippet": "Helm Commands with YAML File ExamplesHelm is a powerful package manager for Kubernetes that simplifies the deployment and management of containerized applications. In this guide, we’ll explore some...",
    "content": "Helm Commands with YAML File ExamplesHelm is a powerful package manager for Kubernetes that simplifies the deployment and management of containerized applications. In this guide, we’ll explore some common Helm commands and provide detailed examples of Helm Chart YAML files.Helm CommandsInitialize a Helm ChartTo create a new Helm chart, you can use the following command:helm create mychartThis command generates the necessary directory structure and files for your chart.Installing a ChartTo install a Helm chart, you can use the following command:helm install my-release ./mychartHere, my-release is the name you give to the release, and ./mychart is the path to your Helm chart.Upgrading a ChartTo upgrade a Helm release, you can use the following command:helm upgrade my-release ./mychartThis command is used to apply changes to a deployed release.Uninstalling a ChartTo uninstall a Helm release, you can use the following command:helm uninstall my-releaseThis command removes the release and associated resources.Helm Chart YAML ExamplesChart.yamlThe Chart.yaml file provides metadata about your Helm chart. Here’s an example:apiVersion: v2name: mychartdescription: A Helm chart for my applicationversion: 0.1.0appVersion: 1.0.0values.yamlThe values.yaml file contains configuration values for your Helm chart. Here’s an example:replicaCount: 1image:  repository: nginx  tag: stable  pullPolicy: IfNotPresentservice:  name: mychart-service  type: ClusterIP  port: 80Deployment.yamlThe Deployment.yaml file is part of your Helm chart’s templates and defines a Kubernetes Deployment. Here’s an example:apiVersion: apps/v1kind: Deploymentmetadata:  name: spec:  replicas:   template:    spec:      containers:        - name:           image: \":\"          ports:            - containerPort: 80Service.yamlThe Service.yaml file defines a Kubernetes Service for your application. Here’s an example:apiVersion: v1kind: Servicemetadata:  name: spec:  selector:    app:   ports:    - port:       targetPort: 80What Next?These are just a few examples of Helm commands and YAML files used in Helm charts. Helm makes it easier to package, deploy, and manage Kubernetes applications, allowing you to define and version your application configurations in a structured way."
  },
  
  {
    "title": "SQL | Database Query Optimization with Common Table Expressions",
    "url": "/posts/SQL-Database-Query-Optimization-with-Common-Table-Expressions/",
    "categories": "SQL, Optimization",
    "tags": "microsoft, sql, database, optimization, cte",
    "date": "2023-05-18 00:00:00 +0200",
    





    
    "snippet": "Database Query Optimization with Common Table Expressions (CTE)Common Table Expressions (CTEs) are a valuable tool for optimizing database queries, particularly when dealing with complex queries or...",
    "content": "Database Query Optimization with Common Table Expressions (CTE)Common Table Expressions (CTEs) are a valuable tool for optimizing database queries, particularly when dealing with complex queries or large datasets. They allow you to break down your query into smaller, more manageable parts. In this guide, we’ll explore how to use CTEs for query optimization with examples.What is a Common Table Expression (CTE)?A Common Table Expression is a temporary result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. CTEs help improve query readability and maintainability by breaking down complex queries into smaller logical units.Query Optimization with CTEsCTEs can significantly enhance query performance by allowing the database optimizer to better understand and optimize your query. They can eliminate redundancy and make your SQL code more elegant.To optimize your queries with CTEs:  Use them for recursive queries.  Organize and structure your SQL code for clarity.  Reduce duplicated subqueries.ExamplesExample 1: Recursive CTERecursive CTEs are particularly useful when working with hierarchical data structures, such as organizational charts or comment threads.Suppose you have a table named Employee with columns EmployeeID and ManagerID. You can use a CTE to retrieve all employees reporting to a specific manager, including their subordinates.WITH RecursiveEmployeeCTE AS (  SELECT EmployeeID, EmployeeName, ManagerID  FROM Employee  WHERE EmployeeID = @ManagerID    UNION ALL    SELECT e.EmployeeID, e.EmployeeName, e.ManagerID  FROM Employee AS e  INNER JOIN RecursiveEmployeeCTE AS r ON e.ManagerID = r.EmployeeID)SELECT EmployeeID, EmployeeNameFROM RecursiveEmployeeCTE;Example 2: Hierarchical DataConsider a table named Category with columns CategoryID and ParentCategoryID. You can use a CTE to retrieve all categories in a hierarchical structure.WITH RecursiveCategoryCTE AS (  SELECT CategoryID, CategoryName, ParentCategoryID  FROM Category  WHERE ParentCategoryID IS NULL    UNION ALL    SELECT c.CategoryID, c.CategoryName, c.ParentCategoryID  FROM Category AS c  INNER JOIN RecursiveCategoryCTE AS r ON c.ParentCategoryID = r.CategoryID)SELECT CategoryID, CategoryNameFROM RecursiveCategoryCTE;What Next?Common Table Expressions (CTEs) are a powerful tool for database query optimization. They enhance the readability and maintainability of your SQL code while improving performance, especially in cases involving recursive or hierarchical data. By breaking down complex queries into smaller, manageable units, CTEs can make your database queries more efficient and elegant."
  }
  
]

