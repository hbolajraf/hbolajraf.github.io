[ { "title": "Azure | Azure API Management", "url": "/posts/Azure-API-Management/", "categories": "Azure, API Management", "tags": "microsoft, csharp, c#, Azure, api", "date": "2023-11-04 00:00:00 +0100", "snippet": "Azure API ManagementAzure API Management is a comprehensive solution for publishing, managing, securing, and analyzing APIs. It allows organizations to expose their services as APIs to internal and...", "content": "Azure API ManagementAzure API Management is a comprehensive solution for publishing, managing, securing, and analyzing APIs. It allows organizations to expose their services as APIs to internal and external developers, making it easier to consume and manage APIs efficiently. In this guide, we will explore the key concepts of Azure API Management along with examples.Key Concepts1. APIs : APIs represent the services that you want to expose, whether they are web services, functions, or other endpoints.2. API Products : API Products group APIs and define their usage policies. For example, you can create a “Basic” product with limited access and a “Premium” product with more features.3. Operations : Operations define how to interact with an API, including HTTP methods (GET, POST, etc.) and request/response transformations.4. Policies : Policies are rules applied to APIs or operations. They can include rate limiting, authentication, caching, and more.5. Developer Portal : Azure API Management provides a developer portal where developers can discover and test your APIs.Getting Started Create an Azure API Management Service: In the Azure Portal, create an API Management service. Import or Create APIs: You can import APIs from existing services or create a new API within Azure API Management. Configure API Products: Define API products with usage policies, like rate limits or subscription requirements. Secure Your APIs: Implement authentication and authorization policies to secure your APIs. Creating an APILet’s create a simple API for weather information. Create an API: In Azure API Management, add a new API and define its backend URL, such as https://weatherapi.hbolajraf.com. Define Operations: Add operations like GET /weather/{city} to retrieve weather information for a specific city. Test in the Developer Portal: Use the developer portal to test the API operations. Securing an APITo secure your API, you can require an API key or OAuth token. Authentication: Configure policies to require API keys for access. You can generate API keys for developers in your developer portal. OAuth 2.0: If you want to use OAuth 2.0, configure OAuth settings and allow developers to obtain tokens for your API. Analyzing API UsageAzure API Management provides analytics and monitoring capabilities. Analytics: Monitor API usage, error rates, and response times through Azure Monitor. Reports: Access detailed reports on API traffic and usage patterns. Alerts: Set up alerts to be notified when unusual activity occurs. Use case exampleIn this example, we will demonstrate how to consume an API hosted in Azure API Management from a C# application.Prerequisites Azure API Management instance set up with an API. Visual Studio or any C# development environment. Azure SDK for .NET installed.Step 1: Obtain API Management Subscription KeyBefore you can call an API in Azure API Management, you need to obtain a subscription key. Here’s how: In the Azure Portal, navigate to your API Management instance. Under the “APIs” section, select the API you want to access. In the API’s settings, go to the “Security” tab. Generate a new subscription key or use an existing one. Step 2: Create a C# Console ApplicationCreate a new C# console application in your development environment.Step 3: Install Required PackagesIn your C# project, install the Microsoft.Azure.Management.ApiManagement and Microsoft.Azure.Management.ApiManagement.Fluent NuGet packages. These packages allow you to interact with Azure API Management.Install-Package Microsoft.Azure.Management.ApiManagementInstall-Package Microsoft.Azure.Management.ApiManagement.FluentStep 4: Write C# Code to Consume the APIHere is a sample C# code that demonstrates how to consume the weather API from Azure API Management using the subscription key obtained in Step 1:using Microsoft.Azure.Management.ApiManagement;using Microsoft.Azure.Management.ApiManagement.Models;using Microsoft.Azure.Management.Fluent;using Microsoft.Azure.Management.ResourceManager.Fluent;using Microsoft.Azure.Services.AppAuthentication;using Microsoft.Rest.Azure.Authentication;using System;class Program{ static async System.Threading.Tasks.Task Main(string[] args) { string clientId = \"your-client-id\"; string clientSecret = \"your-client-secret\"; string tenantId = \"your-tenant-id\"; string subscriptionId = \"your-subscription-id\"; string resourceGroupName = \"your-resource-group\"; string serviceName = \"your-api-management-service\"; string apiKey = \"your-subscription-key\"; var serviceClientCredentials = ApplicationTokenProvider.LoginSilentAsync( tenantId, clientId, clientSecret).Result; var azure = Azure.Authenticate(serviceClientCredentials).WithSubscription(subscriptionId); var apiManagementClient = new ApiManagementClient(serviceClientCredentials) { SubscriptionId = subscriptionId }; var apiManagement = await azure.ApiManagementServices.GetByResourceGroupAsync(resourceGroupName, serviceName); var baseUri = $\"https://weatherapi.hbolajraf.com\"; var operationResponse = await apiManagementClient.HttpOperations.ListByServiceAsync(resourceGroupName, serviceName); if (operationResponse != null) { var operation = operationResponse.First(); var request = new HttpRequestMessage { Method = HttpMethod.Get, RequestUri = new Uri(new Uri(baseUri), operation.UrlTemplate), }; request.Headers.Add(\"Ocp-Apim-Subscription-Key\", apiKey); using (var response = await httpClient.SendAsync(request)) { var content = await response.Content.ReadAsStringAsync(); Console.WriteLine($\"Response: {content}\"); } } }}Replace the placeholders ( your-client-id, your-client-secret, your-tenant-id, your-subscription-id, etc.) with your actual Azure and API Management service details.Step 5: Run the C# ApplicationBuild and run your C# application. It will call the API hosted in Azure API Management using the subscription key and display the response.What Next? Azure API Management simplifies the process of publishing, managing, and securing APIs. It offers a developer-friendly experience with the developer portal and extensive analytics to help you make data-driven decisions about your APIs.The Use Case example demonstrates how to consume an API in Azure API Management using a C# application. You can adapt this code to fit your specific use case and integrate it into your applications as needed." }, { "title": "Docker | Tips and Tricks", "url": "/posts/Docker-Tips-and-Tricks/", "categories": "Docker, Tips And Tricks", "tags": "microsoft, docker, tips&tricks", "date": "2023-10-26 00:00:00 +0200", "snippet": "Docker is a powerful tool for containerization, allowing you to package and run applications with their dependencies in isolated containers. Here are some tips and tricks for using Docker effective...", "content": "Docker is a powerful tool for containerization, allowing you to package and run applications with their dependencies in isolated containers. Here are some tips and tricks for using Docker effectively.Installation and Setup1. Install Docker To get started, install Docker by following the official installation instructions for your operating system on the Docker website.2. Use Docker Compose Docker Compose is a tool for defining and running multi-container Docker applications. It simplifies the process of managing complex applications with multiple containers.Basic Docker Commands3. Pull an Image Use docker pull to download Docker images from a registry. For example, docker pull ubuntu will pull the Ubuntu image.4. List Images To list all downloaded images, use docker images or docker image ls.5. Run a Container Start a new container with docker run. For example, docker run -it ubuntu bash runs an interactive Ubuntu container.6. Attach to a Running Container To attach to a running container, use docker exec -it &lt;container_name&gt; bash.7. Stop and Remove Containers Use docker stop &lt;container_id&gt; to stop a running container. To remove a stopped container, use docker rm &lt;container_id&gt;.8. View Container Logs View container logs with docker logs &lt;container_id&gt;.9. Naming Containers When running containers, provide a --name flag to give them human-readable names.Advanced Docker Commands10. Build a Docker Image Create a Docker image from a Dockerfile using docker build. For example, docker build -t my-image:1.0 . builds an image from the current directory.11. Docker Registry Login Log in to a Docker registry using docker login. This is necessary for pushing images to a private registry.12. Push Images to a Registry Push your Docker images to a registry with docker push. For example, docker push my-image:1.0 pushes an image to the registry.13. Docker Network Create custom Docker networks to connect containers. Use docker network create to create a network and --network to specify it when running containers.14. Volume Mounting Share data between your host and container by using volume mounts with the -v or --volume flag. For example, docker run -v /host/path:/container/path.15. Docker Compose for Multi-Container Apps Use a docker-compose.yml file to define and run multi-container applications. Run them with docker-compose up.Docker Security16. Limit Container Capabilities Reduce a container’s capabilities by using the --cap-drop and --cap-add flags in the docker run command.17. Scan Images for Vulnerabilities Utilize tools like Clair or Trivy to scan your Docker images for known vulnerabilities before deploying them.18. Regularly Update Images Keep your base images up to date, as they may contain security patches. Use the latest base images from the official repositories.Docker Cleanup19. Remove Dangling Images Remove unused images with docker image prune.20. Clean Up Containers Remove all stopped containers with docker container prune.What Next? Remember to consult the Docker documentation and community resources for additional tips and best practices when working with Docker containers." }, { "title": "GitHub | Using Copilot with Visual Studio Code", "url": "/posts/GitHub-Using-Copilot-with-Visual-Studio-Code/", "categories": "GitHub, Copilot", "tags": "microsoft, github, copilot, zipkin", "date": "2023-10-15 00:00:00 +0200", "snippet": "Using GitHub Copilot with Visual Studio CodeGitHub Copilot is an AI-powered pair programmer developed by GitHub and OpenAI. It helps developers write code faster by providing suggestions and auto-c...", "content": "Using GitHub Copilot with Visual Studio CodeGitHub Copilot is an AI-powered pair programmer developed by GitHub and OpenAI. It helps developers write code faster by providing suggestions and auto-completions. In this guide, we’ll explore how to set up GitHub Copilot with Visual Studio Code and provide some examples of how to use it effectively.PrerequisitesBefore you start, ensure you have the following: Visual Studio Code installed. A GitHub Copilot subscription or access to the GitHub Copilot technical preview.Installing GitHub Copilot Open Visual Studio Code. Go to the Extensions view by clicking on the square icon on the sidebar or using Ctrl+Shift+X. Search for “GitHub Copilot” and click “Install” next to the GitHub Copilot extension. Follow the on-screen instructions to set up GitHub Copilot.Using GitHub CopilotGitHub Copilot works alongside you as you write code, providing code completions and suggestions. Here are some key features:Code CompletionsAs you type, GitHub Copilot suggests code completions based on what you’re writing. You can accept suggestions by pressing Tab or Enter.Code SuggestionsGitHub Copilot offers helpful code suggestions when you’re stuck or need guidance. You can use the /// trigger to see suggestions for comments.ExamplesExample 1: Basic Code CompletionsLet’s say you’re writing a Python function to calculate the square of a number. With GitHub Copilot installed, as you start typing, it provides code completions.# Start typing a functiondef calculate_square(number):GitHub Copilot will suggest completing the function like this:# Copilot suggestiondef calculate_square(number): \"\"\"Calculate the square of a number.\"\"\" return number ** 2Simply accept the suggestion by pressing Tab, and you have a complete function to calculate the square of a number.Example 2: Code Suggestions for CommentsGitHub Copilot is great at helping with code comments. If you’re documenting a function, use the /// trigger to get suggestions.def divide_numbers(a, b): ///GitHub Copilot will provide a comment suggestion like this:def divide_numbers(a, b): \"\"\"Divide two numbers and return the result.\"\"\"What Next?By accepting the suggestion, you can quickly add descriptive comments to your code.GitHub Copilot is a powerful tool that can save you time and help you write high-quality code more efficiently. Experiment with it and let it assist you in various programming languages and scenarios." }, { "title": "Azure | Build .Net IoT App using C#", "url": "/posts/Azure-Build-.Net-IoT-App-using-C/", "categories": "Azure, IoT", "tags": "microsoft, csharp, c#, azure, iot", "date": "2023-10-04 00:00:00 +0200", "snippet": "Build .Net IoT App using C#Azure IoT provides a robust platform for building Internet of Things (IoT) solutions. Here are some C# examples to help you get started with Azure IoT services.Azure IoT ...", "content": "Build .Net IoT App using C#Azure IoT provides a robust platform for building Internet of Things (IoT) solutions. Here are some C# examples to help you get started with Azure IoT services.Azure IoT HubAzure IoT Hub is a fully managed service that enables reliable and secure communication between IoT devices and the cloud. Here’s how to interact with it using C#:Send Telemetry Datausing Microsoft.Azure.Devices.Client;using System.Text;using System.Threading.Tasks;string deviceConnectionString = \"Your Device Connection String\";using var deviceClient = DeviceClient.CreateFromConnectionString(deviceConnectionString, TransportType.Mqtt);var telemetryData = new{ temperature = 25.5, humidity = 60};var messageString = JsonConvert.SerializeObject(telemetryData);var message = new Message(Encoding.UTF8.GetBytes(messageString));await deviceClient.SendEventAsync(message);Receive Cloud-to-Device Messagesusing Microsoft.Azure.Devices.Client;using System.Text;using System.Threading.Tasks;string deviceConnectionString = \"Your Device Connection String\";using var deviceClient = DeviceClient.CreateFromConnectionString(deviceConnectionString, TransportType.Mqtt);Message receivedMessage = await deviceClient.ReceiveAsync();if (receivedMessage != null){ var messageData = Encoding.ASCII.GetString(receivedMessage.GetBytes()); Console.WriteLine($\"Received message: {messageData}\"); await deviceClient.CompleteAsync(receivedMessage);}Azure IoT Device Provisioning ServiceAzure IoT Device Provisioning Service (DPS) simplifies the initial setup of IoT devices. Here’s how to use it with C#:Register a Device with DPSusing Microsoft.Azure.Devices.Provisioning.Service;using Microsoft.Azure.Devices.Shared;using System.Threading.Tasks;string idScope = \"Your DPS ID Scope\";string registrationId = \"Your Device Registration ID\";string primaryKey = \"Your Device Primary Key\";var provisioningServiceClient = ProvisioningServiceClient.CreateFromConnectionString($\"HostName={idScope};SharedAccessKeyName=provisioningserviceowner;SharedAccessKey={primaryKey}\");var individualEnrollment = new IndividualEnrollment(registrationId){ Attestation = new TpmAttestation(), ProvisioningStatus = ProvisioningStatus.Enabled, DeviceID = \"Your Device ID\"};await provisioningServiceClient.CreateOrUpdateIndividualEnrollmentAsync(individualEnrollment);Azure IoT EdgeAzure IoT Edge extends IoT Hub to edge devices. You can run code and manage devices on the edge. Here’s how to get started with C#:Create an IoT Edge Moduleusing Microsoft.Azure.Devices;using Microsoft.Azure.Devices.Edge.Agent.Core;using Microsoft.Azure.Devices.Edge.ModuleUtil;string connectionString = \"Your IoT Hub Connection String\";string deviceId = \"Your Device ID\";string moduleId = \"Your Module ID\";var edgeAgentModule = new DockerModule( \"mcr.microsoft.com/azureiotedge-agent:1.0\", Constants.EdgeAgentModuleName, Constants.EdgeRuntimeContainerName, new DockerConfig(\"linux/amd64\", new DockerLoggingConfig(), new DockerRestartPolicy(0, \"never\")), null);var deploymentConfig = new DeploymentConfig(\"1.0\");await ModuleUtil.DeployModuleAsync(connectionString, deviceId, moduleId, edgeAgentModule, deploymentConfig);What Next?These examples cover some of the essential tasks you can perform using C# with Azure IoT services. For more advanced scenarios and detailed documentation, refer to the Azure IoT documentation.Make sure to replace placeholders like “Your Device Connection String” or “Your DPS ID Scope” with your actual Azure IoT service information." }, { "title": "Git | Tips and Tricks", "url": "/posts/Git-Tips-and-Tricks/", "categories": "GitHub, Git", "tags": "git, github, sourcecontrol", "date": "2023-10-01 00:00:00 +0200", "snippet": "Git Tips and TricksGit is a powerful version control system that can make your development workflow more efficient. Here are some tips and tricks to help you get the most out of Git.Configure GitBe...", "content": "Git Tips and TricksGit is a powerful version control system that can make your development workflow more efficient. Here are some tips and tricks to help you get the most out of Git.Configure GitBefore you start using Git, it’s a good idea to configure it with your name and email address. This information will be associated with your commits.git config --global user.name \"hbolajraf\"git config --global user.email \"hassan.bolajraf@gmail.com\"You can also set other configurations, such as your preferred text editor and default branch.Basic Commands1. Initialize a Repository: To start a new Git repository, use git init in your project directory.2. Clone a Repository: To clone a repository from a remote URL, use git clone &lt;URL&gt;.3. Commit Changes: After making changes, use git commit -m \"Your commit message\" to save them.4. Check the Status: Use git status to see the status of your working directory.BranchingBranches are essential for managing different lines of development.1. Create a Branch: Use git branch &lt;branch_name&gt; to create a new branch.2. Switch Branches: To switch to a different branch, use git checkout &lt;branch_name&gt;.3. Merge Branches: Merge changes from one branch into another with git merge &lt;branch_name&gt;.4. Delete Branch: Use git branch -d &lt;branch_name&gt; to delete a branch.StashingStashing is useful when you need to save your changes temporarily.1. Stash Changes: Use git stash to save your changes.2. Apply Stash: To reapply your changes, use git stash apply.3. List Stashes: See a list of stashes with git stash list.Interactive RebaseInteractive rebase allows you to modify commit history.1. Rebase Interactive: Use git rebase -i HEAD~n to interactively rebase the last n commits.2. Edit Commits: Change “pick” to “edit” to modify a commit.3. Amend Commits: Use git commit --amend to edit the current commit.Git AliasesGit aliases let you create shortcuts for Git commands.1. Create an Alias: Add an alias to your global Git configuration.git config --global alias.co checkout2. Usage: Now, you can use git co as a shorthand for git checkout.Git HooksGit hooks are scripts that run automatically on certain Git events.1. Pre-Commit Hook: Create a .git/hooks/pre-commit script to run actions before a commit.2. Post-Receive Hook: In a server’s Git repository, create a hooks/post-receive script to perform actions after receiving a push.Ignoring FilesYou can specify files or patterns to ignore using a .gitignore file.1. Create .gitignore: Create a file named .gitignore and list the files, directories, or patterns you want to ignore.2. Example .gitignore:# Ignore build artifactsbin/obj/# Ignore log files*.log# Ignore a specific directorydocs/What Next?These tips and tricks will help you become more proficient with Git, making your version control tasks more efficient and your development process smoother." }, { "title": "C# | Configure QoS within API Gateway using ocelot and Polly", "url": "/posts/Configure-QoS-within-API-Gateway-using-ocelot-and-Polly/", "categories": "C#, Web API", "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, polly, webapi, c#, gateway, qos", "date": "2023-09-27 00:00:00 +0200", "snippet": "IntroductionAPI Gateway is an entry point for backend application. It maintains routing, authentication, logging, service discovery etc. Ocelot is used to design and develop API gateway for .net ba...", "content": "IntroductionAPI Gateway is an entry point for backend application. It maintains routing, authentication, logging, service discovery etc. Ocelot is used to design and develop API gateway for .net based application. QoS is generally configured in API gateway which provides different priorities for different applications, users or traffic. In this article, we will configure and discuss Quality of Services (QoS) using ocelot and Polly on ASP.NET Core web API project.What is Quality of Service (QoS)?QoS provides different priorities to different applications, users or data flow. We have already mentioned, Ocelot is used to design API Gateway and Ocelot uses Polly to achieve QoS.The QoSOptions node contains three important properties. ExceptionsAllowedBeforeBreakingThis value must greater than 0. It means that the circuit breaker will break after a certain number of exceptions occur. For example: DurationOfBreakThis value specifies how long the circuit breaker will stay open after it is tripped. The unit of this value is milliseconds. For example: 5000 means 5 seconds TimeoutValueThis value specifies that a request will automatically be timed out if it takes more than this value. The unit of this value is milliseconds as well. For example: 3000 means 3 seconds. Tools and Technology used Visual Studio 2022 .NET 6.0 In Memory Database Entity Framework ASP.NET Core Web API C# Ocelot and MMLib.SwaggerForOcelotImplementationStep 1: Create solution and projects. Create a solution name QoS. Add 2 new web api projects, name – Catalog.API, BFF.WebHere, BFF.Web project will act as API Gateway.Step 2: Install nuget packages. Install following nuget packages in Catalog.API PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemory PM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServer PM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools PM&gt; Install-Package Microsoft.VisualStudio.Web.CodeGeneration.Design Install following nuget packages in BFF.Web PM&gt; Install-Package MMLib.SwaggerForOcelot PM&gt; Install-Package Ocelot PM&gt; Install-Package Ocelot.Provider.PollyStep 3: Organize Catalog.API Create CatalogItem model in Model folderCatalogItem.cs using System.ComponentModel.DataAnnotations; using System.ComponentModel.DataAnnotations.Schema; namespace Catalog.API.Model { public class CatalogItem { [Key] [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } public decimal Price { get; set; } public int AvailableStock { get; set; } public int RestockThreshold { get; set; } } } Create DbContext class as CatalogContext in Db folderCatalogContext.cs using Catalog.API.Model; using Microsoft.EntityFrameworkCore; namespace Catalog.API.Db { public class CatalogContext : DbContext { public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options) { } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { base.OnConfiguring(optionsBuilder); } public DbSet&lt;CatalogItem&gt; CatalogItems { get; set; } } } Create SeedDataProvider class in Db folderSeedDataProvider.cs using Catalog.API.Model; namespace Catalog.API.Db { public class SeedDataProvider { public static void Initialize(CatalogContext catalogContext) { if(!catalogContext.CatalogItems.Any()) { var catalogs = new List&lt;CatalogItem&gt; { new CatalogItem { Name = \"T-Shirt\", Description = \"Cats Eye T-Shirt\", Price = 1000, AvailableStock = 100, RestockThreshold = 10 }, new CatalogItem { Name = \"Samsung Mobile\", Description = \"Samsung A30\", Price = 30000, AvailableStock = 100, RestockThreshold = 5 }, new CatalogItem { Name = \"Meril Beauty Soap\", Description = \"Beauty Soap\", Price = 40, AvailableStock = 500, RestockThreshold = 20 } }; catalogContext.CatalogItems.AddRange(catalogs); catalogContext.SaveChanges(); } } } } Modify Program class as follows.Program.cs using Catalog.API.Db; using Microsoft.EntityFrameworkCore; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\")); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); // For Seed data generation using(var scope = app.Services.CreateScope()) { var services = scope.ServiceProvider; var db = services.GetRequiredService&lt;CatalogContext&gt;(); SeedDataProvider.Initialize(db); } // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run();Here, the following line is used to configure in memory database builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\"));The following code snippet is used to initialize seed data using(var scope = app.Services.CreateScope()) { var services = scope.ServiceProvider; var db = services.GetRequiredService&lt;CatalogContext&gt;(); SeedDataProvider.Initialize(db); } Add CatalogItemsController class in Controllers folder as follows.CatalogItemsController.cs using Catalog.API.Db; using Catalog.API.Model; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; namespace Catalog.API.Controllers { [Route(\"api/[controller]\")] [ApiController] public class CatalogItemsController : ControllerBase { private readonly CatalogContext _context; private static int _count = 0; public CatalogItemsController(CatalogContext context) { _context = context; } // GET: api/CatalogItems [HttpGet] public async Task&lt;ActionResult&lt;IEnumerable&lt;CatalogItem&gt;&gt;&gt; GetCatalogItems() { _count++; if(_count &lt;= 3) { // Sleep for 4 seconds Thread.Sleep(4000); } if (_context.CatalogItems == null) { return NotFound(); } return await _context.CatalogItems.ToListAsync(); } // GET: api/CatalogItems/5 [HttpGet(\"{id}\")] public async Task&lt;ActionResult&lt;CatalogItem&gt;&gt; GetCatalogItem(int id) { if (_context.CatalogItems == null) { return NotFound(); } var catalogItem = await _context.CatalogItems.FindAsync(id); if (catalogItem == null) { return NotFound(); } return catalogItem; } // PUT: api/CatalogItems/5 // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPut(\"{id}\")] public async Task&lt;IActionResult&gt; PutCatalogItem(int id, CatalogItem catalogItem) { if (id != catalogItem.Id) { return BadRequest(); } _context.Entry(catalogItem).State = EntityState.Modified; try { await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!CatalogItemExists(id)) { return NotFound(); } else { throw; } } return NoContent(); } // POST: api/CatalogItems // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPost] public async Task&lt;ActionResult&lt;CatalogItem&gt;&gt; PostCatalogItem(CatalogItem catalogItem) { if (_context.CatalogItems == null) { return Problem(\"Entity set 'CatalogContext.CatalogItems' is null.\"); } _context.CatalogItems.Add(catalogItem); await _context.SaveChangesAsync(); return CreatedAtAction(\"GetCatalogItem\", new { id = catalogItem.Id }, catalogItem); } // DELETE: api/CatalogItems/5 [HttpDelete(\"{id}\")] public async Task&lt;IActionResult&gt; DeleteCatalogItem(int id) { if (_context.CatalogItems == null) { return NotFound(); } var catalogItem = await _context.CatalogItems.FindAsync(id); if (catalogItem == null) { return NotFound(); } _context.CatalogItems.Remove(catalogItem); await _context.SaveChangesAsync(); return NoContent(); } private bool CatalogItemExists(int id) { return (_context.CatalogItems?.Any(e =&gt; e.Id == id)).GetValueOrDefault(); } } }Step 4: Organize BFF.WebIn this stage, we are going to configure a gateway to communicate with other services using ocelot. Create a folder name - Routes.dev in root directory and add the following files. ocelot.catalog-api.json, ocelot.global.json, ocelot.SwaggerEndPoints.json in Routes.dev folder. Now modify the json files as follows. ocelot.catalog-api.json { \"Routes\": [ { \"DownstreamPathTemplate\": \"/{everything}\", \"DownstreamScheme\": \"https\", \"SwaggerKey\": \"catalog-api\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": \"7282\" } ], \"UpstreamPathTemplate\": \"/catalog/{everything}\", \"UpstreamHttpMethod\": [ \"GET\", \"POST\", \"PUT\", \"DELETE\" ], \"QoSOptions\": { \"ExceptionsAllowedBeforeBreaking\": 2, \"DurationOfBreak\": 5000, \"TimeoutValue\": 3000 } } ] }QoSOptions section of the above file basically configure QoS for Catalog service. The above configuration means that if the server does not response for 3 minutes, it will throw timeout exception. If the server throws two exceptions, it will not be accessible for five minutes.ocelot.global.json { \"GlobalConfiguration\": { \"BaseUrl\": \"https://localhost:7205\" //\"ServiceDiscoveryProvider\": { // \"Host\": \"localhost\", // \"Port\": 7205 //} } }ocelot.SwaggerEndPoints.json { \"SwaggerEndPoints\": [ { \"Key\": \"bffweb\", \"TransformByOcelotConfig\": false, \"Config\": [ { \"Name\": \"BFF.Web\", \"Version\": \"1.0\", \"Url\": \"https://localhost:7205/swagger/v1/swagger.json\" } ] }, { \"Key\": \"catalog-api\", \"TransformByOcelotConfig\": true, \"Config\": [ { \"Name\": \"Catalog.API\", \"Version\": \"1.0\", \"Url\": \"https://localhost:7205/catalog/swagger/v1/swagger.json\" } ] } ] }Note: I have added the following code block in CatalogItemController to produce timeout manually. [HttpGet] public async Task&lt;ActionResult&lt;IEnumerable&lt;CatalogItem&gt;&gt;&gt; GetCatalogItems() { _count++; if(_count &lt;= 3) { // Sleep for 4 seconds Thread.Sleep(4000); } if (_context.CatalogItems == null) { return NotFound(); } return await _context.CatalogItems.ToListAsync(); }Step 5: Run and test the applicationNow run both web projects. In the BFF.Web, select Catalog.API-1.0 from swagger definition (“Select a definition on the top right corner”) and execute the api (CatalogItems) as follows.When we visit the first time (or quickly second time), it tells us that circuit is breaking for 5000 ms. Look at the console of BFF.Web.Then, the second time (quickly) it tells us that the circuit is open, and we cannot visit the service for 3 seconds as follows.After 3 seconds, the service is accessible. If you execute now, you will see the output like below.Source code" }, { "title": "C# | Search AD entry by ObjectSid using Novell Directory Ldap Nuget package", "url": "/posts/Search-AD-entry-by-ObjectSid-using-Novell-Directory-Ldap-Nuget-package/", "categories": "C#, Active Directory", "tags": "ldap, microsoft, c#, dotnet, activedirectory", "date": "2023-09-25 00:00:00 +0200", "snippet": "SID stands for security identifier, a unique string that Windows Server automatically assigns to each computer, user and group in order to mark and clearly distinguish them.Windows SID Format :SIDs...", "content": "SID stands for security identifier, a unique string that Windows Server automatically assigns to each computer, user and group in order to mark and clearly distinguish them.Windows SID Format :SIDs always follow the same structure, with values separated by dashes:Novell Directory LdapNovell Directory Ldap nuget package allows you to connect to the active directory in order to perform CRUD actions on all AD objects, among other Users Groups and computers. it support both platforms windows and linux, so even if you are under kubernetes clusters on docker containers you will be able to manage AD objects.C# code practical caseThis exemple is based on a project that was created based on .Net 6 under Visual studio 2022 and with a docker support to manage Ad Objects within a linux docker container.The nuget package version that was used are the following :&lt;PackageReference Include=\"Novell.Directory.Ldap.NETStandard\" Version=\"3.6.0\" /&gt;When we will process a search within the AD for a dedicated entry we will use a string ObjectSid instead of a bytes value. To do that we will create a helper method that will return the string value based on the input ObjectSid bytes value as bellow :1. Add Helper method to convert ObjectSID Byte value in string format public static string ConvertSidByteValueToStringValue(Byte[] sidBytes) { short sSubAuthorityCount = 0; StringBuilder strSid = new StringBuilder(); strSid.Append(\"S-\"); try { // Add SID revision. strSid.Append(sidBytes[0].ToString()); sSubAuthorityCount = Convert.ToInt16(sidBytes[1]); // Next six bytes are SID authority value. if (sidBytes[2] != 0 || sidBytes[3] != 0) { string strAuth = String.Format(\"0x{0:2x}{1:2x}{2:2x}{3:2x}{4:2x}{5:2x}\", (Int16)sidBytes[2], (Int16)sidBytes[3], (Int16)sidBytes[4], (Int16)sidBytes[5], (Int16)sidBytes[6], (Int16)sidBytes[7]); strSid.Append(\"-\"); strSid.Append(strAuth); } else { Int64 iVal = sidBytes[7] + (sidBytes[6] &lt;&lt; 8) + (sidBytes[5] &lt;&lt; 16) + (sidBytes[4] &lt;&lt; 24); strSid.Append(\"-\"); strSid.Append(iVal.ToString()); } // Get sub authority count... int idxAuth = 0; for (int i = 0; i &lt; sSubAuthorityCount; i++) { idxAuth = 8 + i * 4; UInt32 iSubAuth = BitConverter.ToUInt32(sidBytes, idxAuth); strSid.Append(\"-\"); strSid.Append(iSubAuth.ToString()); } } catch (Exception ex) { Trace.TraceWarning(ex.Message); throw; } return strSid.ToString(); }This method will get ObjectSid information parts from the array of Bytes input value as bellow :byte[0] - Revision Levelbyte[1] - count of Sub-Authoritiesbyte[2 - 7] - 48 bit Authority(big-endian)byte[8 +] - n Sub-Authorities, 32 bits And after that we return a String ObjectSID on the format bellow :S-{Revision}-{Authority}-{ SubAuthority1}-{ SubAuthority2}...-{ SubAuthorityN}2. Call Active Directory to retrieve User informations based on objectSid string valueAdding Novell directiveusing Novell.Directory.Ldap;.....Adding the search method to get the User entry object/// Exemple of Dn Value : OU=User,OU=Accounts,DC=USA,DC=NY /// Exemple of objectSidStringValue Value : S-1-5-15-420314761-778715008-4547327-1845947 /// Exemple of objectCategory Value : User or Group or Computer /// Exemple of OutputProps Values : \"description\",\"lastLogon\",\"email\",\"name\", \"login\" public List&lt;Attributes&lt;string, string&gt;&gt;? GetLdapEntryByObjectSid(string Dn, string objectSidStringValue, string objectCategory, string[] OutputProps) { var ldapConnectionOptions = new LdapConnectionOptions(); ldapConnectionOptions.UseSsl(); var ldapConx = new LdapConnection(ldapConnectionOptions); List&lt;Attributes&lt;string, string&gt;&gt; listAttributes = new List&lt;Attributes&lt;string, string&gt;&gt;(); string Filter = $\"(&amp;(objectSid={objectSidStringValue})(objectCategory={objectCategory}))\"; if (!string.IsNullOrEmpty(Filter)) { var searchQuery = ldapConx.Search(Dn, Novell.Directory.Ldap.LdapConnection.ScopeSub, Filter, OutputProps, false); if (searchQuery != null) { while (searchQuery.HasMore()) { LdapEntry nextEntry = null; nextEntry = searchQuery.Next(); //SID Case if (OutputProps.Contains(\"objectSid\")) { //Get Sid Property Bytes value to be converted into string clear value var objectSid = nextEntry.GetAttributeSet().FirstOrDefault(e =&gt; e.Key.Equals(\"objectSid\")); if (objectSid.Value != null) { var sidStringFormat = ConvertSidByteValueToStringValue(objectSid.Value.ByteValue); listAttributes.Add(new Attributes&lt;string, string&gt;(objectSid.Key, sidStringFormat)); } } listAttributes.AddRange(nextEntry.GetAttributeSet() .Where(e =&gt; !e.Key.Equals(\"objectSid\")) .Select(e =&gt; new Attributes&lt;string, string&gt;(e.Key, e.Value.StringValue)) .ToList()); } } } return listAttributes; }The GetLdapEntryByObjectSid method will process a searchQuery within ldap based on the objectSid filter and also the objectCategory in this example User filter. As mentioned the objectSid within Active directory are persisted as a byte so we will call our Helper method to convert it into a string value: var sidStringFormat = ConvertSidByteValueToStringValue(objectSid.Value.ByteValue);The returned list of properties will contains all properties as string readable values, so we can use them also to process a new search based on other property.Links : Novell Directory - GitHub repositoryNovell Directory - Nuget package" }, { "title": "C# | Configure Swagger on api gateway using ocelot in asp.net core application", "url": "/posts/Configure-Swagger-For-Ocelot-Gateway/", "categories": "C#, Web API", "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, webapi, c#, gateway, swagger", "date": "2023-09-24 00:00:00 +0200", "snippet": "IntroductionSwagger configuration on API gateway is not as simple as you are configure normal application. You have to configure it in different way. In this article I will create an API gateway us...", "content": "IntroductionSwagger configuration on API gateway is not as simple as you are configure normal application. You have to configure it in different way. In this article I will create an API gateway using ocelot and asp.net core application and show you how to configure swagger on API gateway.Tools and technologies used Visual Studio 2022 .NET 6.0 In Memory Database Entity Framework ASP.NET Core Web API C# Ocelot and MMLib.SwaggerForOcelotImplementationStep 1: Create solution and projects. Create a solution name APIGateway Add 4 new web api project, name - Catalog.API, Location.API, Ordering.API and BFF.Web in the solution.Here, BFF.Web project will act as API Gateway.Step 2: Install nuget packages. Install following nuget package in Catalog.API Project PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools Install following nuget package in Ordering.API Project PM&gt; Install-Package Microsoft.EntityFrameworkCorePM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools Install following nuget packages in BFF.Web Project PM&gt; Install-Package OcelotPM&gt; Install-Package Ocelot.Provider.PollyPM&gt; Install-Package Ocelot.Cache.CacheManagerPM&gt; Install-Package MMLib.SwaggerForOcelot Step 3: Organize Catalog.API Project Create a Product model class in Catalog.API/Model folderProduct.cs using System.ComponentModel.DataAnnotations; using System.ComponentModel.DataAnnotations.Schema; namespace Catalog.API.Model { public class Product { [Key] [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } public decimal Price { get; set; } public int AvailableStock { get; set; } public int RestockThreshold { get; set; } } } Create a CatalogContext class in Catalog.API/Db folderCatalogContext.cs using Catalog.API.Model; using Microsoft.EntityFrameworkCore; namespace Catalog.API.Db { public class CatalogContext : DbContext { public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options) { } protected override void OnModelCreating(ModelBuilder modelBuilder) { } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { base.OnConfiguring(optionsBuilder); } public DbSet&lt;Product&gt; Products { get; set; } } } Modify Program.cs file as follows using Catalog.API.Db; using Microsoft.EntityFrameworkCore; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\")); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); Create a conroller class name ProductsController in Catalog.API/Controllers folderCatalogController.cs using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Http; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; using Catalog.API.Db; using Catalog.API.Model; namespace Catalog.API.Controllers { [Route(\"api/[controller]\")] [ApiController] public class ProductsController : ControllerBase { private readonly CatalogContext _context; public ProductsController(CatalogContext context) { _context = context; } // GET: api/Products [HttpGet(\"GetAll\")] public async Task&lt;ActionResult&lt;IEnumerable&lt;Product&gt;&gt;&gt; GetProducts() { return await _context.Products.ToListAsync(); } // GET: api/Products/5 [HttpGet(\"{id}\")] public async Task&lt;ActionResult&lt;Product&gt;&gt; GetProduct(int id) { var product = await _context.Products.FindAsync(id); if (product == null) { return NotFound(); } return product; } // PUT: api/Products/5 // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPut(\"Edit/{id}\")] public async Task&lt;IActionResult&gt; PutProduct(int id, Product product) { if (id != product.Id) { return BadRequest(); } _context.Entry(product).State = EntityState.Modified; try { await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!ProductExists(id)) { return NotFound(); } else { throw; } } return NoContent(); } // POST: api/Products // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPost(\"Add\")] public async Task&lt;ActionResult&lt;Product&gt;&gt; PostProduct(Product product) { _context.Products.Add(product); await _context.SaveChangesAsync(); return CreatedAtAction(\"GetProduct\", new { id = product.Id }, product); } // DELETE: api/Products/5 [HttpDelete(\"Delete/{id}\")] public async Task&lt;IActionResult&gt; DeleteProduct(int id) { var product = await _context.Products.FindAsync(id); if (product == null) { return NotFound(); } _context.Products.Remove(product); await _context.SaveChangesAsync(); return NoContent(); } private bool ProductExists(int id) { return _context.Products.Any(e =&gt; e.Id == id); } } }Step 4: Organize Ordering.API Project Create a Order model class in Ordering.API/Model folderOrder.cs namespace Ordering.API.Models { public class Order { public int Id { get; set; } public string Address { get; set; } public DateTime OrderDate { get; set; } public string Comments { get; set; } } } Create a OrderingContext class in Ordering.API/Db folderOrderingContext.cs using Microsoft.EntityFrameworkCore; using Ordering.API.Models; namespace Ordering.API.Db { public class OrderingContext : DbContext { public OrderingContext(DbContextOptions&lt;OrderingContext&gt; options) : base(options) { } public DbSet&lt;Ordering.API.Models.Order&gt; Order { get; set; } } } Modify Program.cs file as follows using Microsoft.EntityFrameworkCore; using Ordering.API.Db; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); builder.Services.AddDbContext&lt;OrderingContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\")); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); Create a conroller class name OrdersController in Ordering.API/Controllers folderOrdersController.cs using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Http; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; using Ordering.API.Db; using Ordering.API.Models; namespace Ordering.API.Controllers { [Route(\"api/[controller]\")] [ApiController] public class OrdersController : ControllerBase { private readonly OrderingContext _context; public OrdersController(OrderingContext context) { _context = context; } // GET: api/Orders [HttpGet(\"GetAll\")] public async Task&lt;ActionResult&lt;IEnumerable&lt;Order&gt;&gt;&gt; GetOrder() { return await _context.Order.ToListAsync(); } // GET: api/Orders/5 [HttpGet(\"{id}\")] public async Task&lt;ActionResult&lt;Order&gt;&gt; GetOrder(int id) { var order = await _context.Order.FindAsync(id); if (order == null) { return NotFound(); } return order; } // PUT: api/Orders/5 // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPut(\"Edit/{id}\")] public async Task&lt;IActionResult&gt; PutOrder(int id, Order order) { if (id != order.Id) { return BadRequest(); } _context.Entry(order).State = EntityState.Modified; try { await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!OrderExists(id)) { return NotFound(); } else { throw; } } return NoContent(); } // POST: api/Orders // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPost(\"Add\")] public async Task&lt;ActionResult&lt;Order&gt;&gt; PostOrder(Order order) { _context.Order.Add(order); await _context.SaveChangesAsync(); return CreatedAtAction(\"GetOrder\", new { id = order.Id }, order); } // DELETE: api/Orders/5 [HttpDelete(\"Delete/{id}\")] public async Task&lt;IActionResult&gt; DeleteOrder(int id) { var order = await _context.Order.FindAsync(id); if (order == null) { return NotFound(); } _context.Order.Remove(order); await _context.SaveChangesAsync(); return NoContent(); } private bool OrderExists(int id) { return _context.Order.Any(e =&gt; e.Id == id); } } }Step 5: Organize Location.API Project Create CountriesController in Location.API/Controllers folder using Microsoft.AspNetCore.Mvc; namespace Location.API.Controllers { [ApiController] [Route(\"api/[controller]\")] public class CountriesController : ControllerBase { [HttpGet(\"GetAll\")] public IEnumerable&lt;string&gt; Get() { return new string[] {\"America\",\"Bangladesh\", \"Canada\" }; } } }Step 6: Organize BFF.Web (API Gateway) Project Create a folder name Routes and add the following files in that folderocelot.catalog.api.json { \"Routes\": [ { \"DownstreamPathTemplate\": \"/{everything}\", \"DownstreamScheme\": \"https\", \"SwaggerKey\": \"catalog\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": \"7282\" } ], \"UpstreamPathTemplate\": \"/catalog/{everything}\", \"UpstreamHttpMethod\": [ \"GET\", \"POST\", \"PUT\", \"DELETE\" ] } ] }ocelot.location.api.json { \"Routes\": [ { \"DownstreamPathTemplate\": \"/{everything}\", \"DownstreamScheme\": \"https\", \"SwaggerKey\": \"location\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": \"7003\" } ], \"UpstreamPathTemplate\": \"/location/{everything}\", \"UpstreamHttpMethod\": [ \"GET\", \"POST\", \"PUT\", \"DELETE\" ] } ] }ocelot.ordering.api.json { \"Routes\": [ { \"DownstreamPathTemplate\": \"/{everything}\", \"DownstreamScheme\": \"https\", \"SwaggerKey\": \"ordering\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": \"7126\" } ], \"UpstreamPathTemplate\": \"/ordering/{everything}\", \"UpstreamHttpMethod\": [ \"GET\", \"POST\", \"PUT\", \"DELETE\" ] } ] }ocelot.global.json { \"GlobalConfiguration\": { \"BaseUrl\": \"http://localhost:5205\" } }ocelot.SwaggerEndPoints.json { \"SwaggerEndPoints\": [ { \"Key\": \"bffweb\", \"TransformByOcelotConfig\": false, \"Config\": [ { \"Name\": \"BFF.Web\", \"Version\": \"1.0\", \"Url\": \"http://localhost:5205/swagger/v1/swagger.json\" } ] }, { \"Key\": \"location\", \"TransformByOcelotConfig\": true, \"Config\": [ { \"Name\": \"Location.API\", \"Version\": \"1.0\", \"Url\": \"http://localhost:5205/location/swagger/v1/swagger.json\" } ] }, { \"Key\": \"catalog\", \"TransformByOcelotConfig\": true, \"Config\": [ { \"Name\": \"Catalog.API\", \"Version\": \"1.0\", \"Url\": \"http://localhost:5205/catalog/swagger/v1/swagger.json\" } ] }, { \"Key\": \"ordering\", \"TransformByOcelotConfig\": true, \"Config\": [ { \"Name\": \"Ordering.API\", \"Version\": \"1.0\", \"Url\": \"http://localhost:5205/catalog/swagger/v1/swagger.json\" } ] } ] } Add AlterUpstream Class in Config FolderAlterUpstream.cs using Newtonsoft.Json; using Newtonsoft.Json.Linq; namespace BFF.Web.Config { public class AlterUpstream { public static string AlterUpstreamSwaggerJson(HttpContext context, string swaggerJson) { var swagger = JObject.Parse(swaggerJson); // ... alter upstream json return swagger.ToString(Formatting.Indented); } } } Modify Program.cs file as followsProgram.cs using BFF.Web.Config; using MMLib.SwaggerForOcelot.DependencyInjection; using Ocelot.DependencyInjection; using Ocelot.Middleware; using Ocelot.Provider.Polly; var builder = WebApplication.CreateBuilder(args); var routes = \"Routes\"; builder.Configuration.AddOcelotWithSwaggerSupport(options =&gt; { options.Folder = routes; }); builder.Services.AddOcelot(builder.Configuration).AddPolly(); builder.Services.AddSwaggerForOcelot(builder.Configuration); var environment = Environment.GetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\"); builder.Configuration.SetBasePath(Directory.GetCurrentDirectory()) .AddOcelot(routes, builder.Environment) .AddEnvironmentVariables(); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); // Swagger for ocelot builder.Services.AddSwaggerGen(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.UseSwaggerForOcelotUI(options =&gt; { options.PathToSwaggerGenerator = \"/swagger/docs\"; options.ReConfigureUpstreamSwaggerJson = AlterUpstream.AlterUpstreamSwaggerJson; }).UseOcelot().Wait(); app.MapControllers(); app.Run();Step 7: Run and Test application Now run multiple (all) projects and test application using postman. Check all end point using api gateway and swagger using the following URLhttps://localhost:7205/swagger/index.html Select Swagger definition from top right corner of BFFSource code" }, { "title": "Azure DevOps | Running JMeter Test Collection using JMeter Docker Image", "url": "/posts/Azure-DevOps-Running-JMeter-Test-Collection-using-JMeter-Docker-Image/", "categories": "Azure DevOps, JMeter", "tags": "microsoft, azuredevops, azure, jmeter, docker", "date": "2023-09-18 00:00:00 +0200", "snippet": "Running JMeter Test Collection using JMeter Docker ImageJMeter is a popular open-source tool for performance testing and load testing of web applications. Running JMeter tests using Docker is a con...", "content": "Running JMeter Test Collection using JMeter Docker ImageJMeter is a popular open-source tool for performance testing and load testing of web applications. Running JMeter tests using Docker is a convenient way to ensure consistency and isolation. In this guide, we’ll show you how to run a JMeter test collection using a JMeter Docker image.PrerequisitesBefore you begin, ensure that you have the following prerequisites: Docker installed on your machine.Steps Pull the JMeter Docker Image: You can pull the official JMeter Docker image from Docker Hub: docker pull justb4/jmeter:latest This will download the latest JMeter Docker image to your local machine. Prepare Your JMeter Test Collection: Create a directory that contains your JMeter test plan files (.jmx). You can organize your collection of test plans in this directory. Run JMeter Tests using the Docker Image: You can run your JMeter test collection by mounting the test plan directory to the Docker container and specifying the JMX file to execute. Replace YOUR_TEST_DIRECTORY and YOUR_TEST_FILE.jmx with your actual test directory and JMX file. docker run -it --rm -v /path/to/YOUR_TEST_DIRECTORY:/mnt/jmeter -w /mnt/jmeter justb4/jmeter -n -t /mnt/jmeter/YOUR_TEST_FILE.jmx -it - Runs the container in interactive mode. --rm - Removes the container when it stops. -v /path/to/YOUR_TEST_DIRECTORY:/mnt/jmeter - Mounts your test directory to the container at /mnt/jmeter. -w /mnt/jmeter - Sets the working directory to /mnt/jmeter within the container. justb4/jmeter - Specifies the JMeter Docker image. -n - Runs JMeter in non-GUI mode. -t /mnt/jmeter/YOUR_TEST_FILE.jmx - Specifies the JMX file to execute. View Test Results: After the test is completed, you can view the results in the console output. You can also configure JMeter to save the test results in various formats, such as CSV or XML, by adding appropriate listeners to your JMX file. What Next?Running JMeter test collections using a Docker image simplifies the setup and execution process, making it easier to perform load testing and performance testing on your web applications. You can easily automate and integrate this process into your CI/CD pipelines for continuous performance monitoring.Remember to replace YOUR_TEST_DIRECTORY and YOUR_TEST_FILE.jmx with your actual test collection directory and JMX file, and customize any additional parameters as needed for your specific testing requirements." }, { "title": "C# | Implementing API gateway using ocelot in asp.net core application", "url": "/posts/Implementing-Ocelot-Gateway/", "categories": "C#, Web API", "tags": "microsoft, aspnetcore, csharp, microservices, ocelot, webapi, c#, gateway", "date": "2023-09-13 00:00:00 +0200", "snippet": "API Gateway is an API management tools that sits between a client application and backend application. It agregates different services, maintain load balancing and work as reverse proxy. Ocelot is ...", "content": "API Gateway is an API management tools that sits between a client application and backend application. It agregates different services, maintain load balancing and work as reverse proxy. Ocelot is an api managment tool which is very powerful and best fit for .net application.Tools and technologies used Visual Studio 2022 .NET 6.0 In Memory Database Entity Framework ASP.NET Core Web API C# OcelotImplementationStep 1: Create solution and projects. Create a solution name APIGateway Add 4 new web api project, name - Catalog.API, Location.API, Ordering.API and BFF.Web in the solution.Here, BFF.Web project will act as API Gateway.Step 2: Install nuget packages. Install following nuget package in Catalog.API Project PM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools Install following nuget package in Ordering.API Project PM&gt; Install-Package Microsoft.EntityFrameworkCorePM&gt; Install-Package Microsoft.EntityFrameworkCore.InMemoryPM&gt; Install-Package Microsoft.EntityFrameworkCore.SqlServerPM&gt; Install-Package Microsoft.EntityFrameworkCore.Tools Install following nuget packages in BFF.Web Project PM&gt; Install-Package OcelotPM&gt; Install-Package Ocelot.Cache.CacheManager Step 3: Organize Catalog.API Project Create a Product model class in Catalog.API/Model folderProduct.cs using System.ComponentModel.DataAnnotations; using System.ComponentModel.DataAnnotations.Schema; namespace Catalog.API.Model { public class Product { [Key] [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } public decimal Price { get; set; } public int AvailableStock { get; set; } public int RestockThreshold { get; set; } } } Create a CatalogContext class in Catalog.API/Db folderCatalogContext.cs using Catalog.API.Model; using Microsoft.EntityFrameworkCore; namespace Catalog.API.Db { public class CatalogContext : DbContext { public CatalogContext(DbContextOptions&lt;CatalogContext&gt; options) : base(options) { } protected override void OnModelCreating(ModelBuilder modelBuilder) { } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { base.OnConfiguring(optionsBuilder); } public DbSet&lt;Product&gt; Products { get; set; } } } Modify Program.cs file as follows using Catalog.API.Db; using Microsoft.EntityFrameworkCore; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); builder.Services.AddDbContext&lt;CatalogContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\")); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); Create a conroller class name ProductsController in Catalog.API/Controllers folderCatalogContoller.cs using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Http; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; using Catalog.API.Db; using Catalog.API.Model; namespace Catalog.API.Controllers { [Route(\"api/[controller]\")] [ApiController] public class ProductsController : ControllerBase { private readonly CatalogContext _context; public ProductsController(CatalogContext context) { _context = context; } // GET: api/Products [HttpGet(\"GetAll\")] public async Task&lt;ActionResult&lt;IEnumerable&lt;Product&gt;&gt;&gt; GetProducts() { return await _context.Products.ToListAsync(); } // GET: api/Products/5 [HttpGet(\"{id}\")] public async Task&lt;ActionResult&lt;Product&gt;&gt; GetProduct(int id) { var product = await _context.Products.FindAsync(id); if (product == null) { return NotFound(); } return product; } // PUT: api/Products/5 // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPut(\"Edit/{id}\")] public async Task&lt;IActionResult&gt; PutProduct(int id, Product product) { if (id != product.Id) { return BadRequest(); } _context.Entry(product).State = EntityState.Modified; try { await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!ProductExists(id)) { return NotFound(); } else { throw; } } return NoContent(); } // POST: api/Products // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPost(\"Add\")] public async Task&lt;ActionResult&lt;Product&gt;&gt; PostProduct(Product product) { _context.Products.Add(product); await _context.SaveChangesAsync(); return CreatedAtAction(\"GetProduct\", new { id = product.Id }, product); } // DELETE: api/Products/5 [HttpDelete(\"Delete/{id}\")] public async Task&lt;IActionResult&gt; DeleteProduct(int id) { var product = await _context.Products.FindAsync(id); if (product == null) { return NotFound(); } _context.Products.Remove(product); await _context.SaveChangesAsync(); return NoContent(); } private bool ProductExists(int id) { return _context.Products.Any(e =&gt; e.Id == id); } } }Step 4: Organize Ordering.API Project Create a Order model class in Ordering.API/Model folderOrder.cs namespace Ordering.API.Models { public class Order { public int Id { get; set; } public string Address { get; set; } public DateTime OrderDate { get; set; } public string Comments { get; set; } } } Create a OrderingContext class in Ordering.API/Db folderOrderingContext.cs using Microsoft.EntityFrameworkCore; using Ordering.API.Models; namespace Ordering.API.Db { public class OrderingContext : DbContext { public OrderingContext(DbContextOptions&lt;OrderingContext&gt; options) : base(options) { } public DbSet&lt;Ordering.API.Models.Order&gt; Order { get; set; } } } Modify Program.cs file as follows using Microsoft.EntityFrameworkCore; using Ordering.API.Db; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); builder.Services.AddDbContext&lt;OrderingContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"CatalogDB\")); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); Create a conroller class name OrdersController in Ordering.API/Controllers folderOrdersController.cs using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Http; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; using Ordering.API.Db; using Ordering.API.Models; namespace Ordering.API.Controllers { [Route(\"api/[controller]\")] [ApiController] public class OrdersController : ControllerBase { private readonly OrderingContext _context; public OrdersController(OrderingContext context) { _context = context; } // GET: api/Orders [HttpGet(\"GetAll\")] public async Task&lt;ActionResult&lt;IEnumerable&lt;Order&gt;&gt;&gt; GetOrder() { return await _context.Order.ToListAsync(); } // GET: api/Orders/5 [HttpGet(\"{id}\")] public async Task&lt;ActionResult&lt;Order&gt;&gt; GetOrder(int id) { var order = await _context.Order.FindAsync(id); if (order == null) { return NotFound(); } return order; } // PUT: api/Orders/5 // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPut(\"Edit/{id}\")] public async Task&lt;IActionResult&gt; PutOrder(int id, Order order) { if (id != order.Id) { return BadRequest(); } _context.Entry(order).State = EntityState.Modified; try { await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!OrderExists(id)) { return NotFound(); } else { throw; } } return NoContent(); } // POST: api/Orders // To protect from overposting attacks, see https://go.microsoft.com/fwlink/?linkid=2123754 [HttpPost(\"Add\")] public async Task&lt;ActionResult&lt;Order&gt;&gt; PostOrder(Order order) { _context.Order.Add(order); await _context.SaveChangesAsync(); return CreatedAtAction(\"GetOrder\", new { id = order.Id }, order); } // DELETE: api/Orders/5 [HttpDelete(\"Delete/{id}\")] public async Task&lt;IActionResult&gt; DeleteOrder(int id) { var order = await _context.Order.FindAsync(id); if (order == null) { return NotFound(); } _context.Order.Remove(order); await _context.SaveChangesAsync(); return NoContent(); } private bool OrderExists(int id) { return _context.Order.Any(e =&gt; e.Id == id); } } }Step 5: Organize Location.API Project Create CountriesController in Location.API/Controllers folder using Microsoft.AspNetCore.Mvc; namespace Location.API.Controllers { [ApiController] [Route(\"api/[controller]\")] public class CountriesController : ControllerBase { [HttpGet(\"GetAll\")] public IEnumerable&lt;string&gt; Get() { return new string[] {\"Morrocco\",\"Spain\", \"Portugal\" }; } } }Step 6: Organize BFF.Web (API Gateway) Project Add a configuraton file for api gateway. I keep this file name - ocelot.json. Add this file in the root directory.ocelot.json { //---Location Service: Start ----------// \"Routes\": [ { \"DownstreamPathTemplate\": \"/api/Countries/GetAll\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7003 } ], // Configure caching // the cache will expire after 30 seconds. \"FileCacheOptions\": { \"TtlSeconds\": 30, \"Region\": \"countriescaching\" }, \"UpstreamPathTemplate\": \"/Countries/GetAll\", \"UpstreamHttpMethod\": [ \"Get\" ], // Enable case sensative Routing/URL \"RouteIsCaseSensitive\": true }, //---Location Service: End ----------// // Catalog Services //------------------// { \"DownstreamPathTemplate\": \"/api/Products/GetAll\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7282 } ], // Implement rate limiting // maximum admitted 1 per 5s. \"RateLimitOptions\": { \"ClientWhitelist\": [ // This is an array used to specify the clients that should not be affected by the rate-limiting ], \"EnableRateLimiting\": true, \"Period\": \"5s\", \"PeriodTimespan\": 1, \"Limit\": 1 }, \"UpstreamPathTemplate\": \"/Products/GetAll\", \"UpstreamHttpMethod\": [ \"Get\" ] }, { \"DownstreamPathTemplate\": \"/api/Products/Add\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7282 } ], \"UpstreamPathTemplate\": \"/Products/Add\", \"UpstreamHttpMethod\": [ \"Post\" ] }, { \"DownstreamPathTemplate\": \"/api/Products/{id}\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7282 } ], \"UpstreamPathTemplate\": \"/Products/{id}\", \"UpstreamHttpMethod\": [ \"Get\" ] }, { \"DownstreamPathTemplate\": \"/api/Products/Edit/{id}\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7282 } ], \"UpstreamPathTemplate\": \"/Products/Edit/{id}\", \"UpstreamHttpMethod\": [ \"Put\" ] }, { \"DownstreamPathTemplate\": \"/api/Products/Delete/{id}\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7282 } ], \"UpstreamPathTemplate\": \"/Products/Delete/{id}\", \"UpstreamHttpMethod\": [ \"Delete\" ] }, //---Catalog service : End ------------// //---Ordering Service: Start ----------// // Catch All Routing { \"DownstreamPathTemplate\": \"/{url}\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7126 } ], \"UpstreamPathTemplate\": \"/{url}\", \"UpstreamHttpMethod\": [ \"Get\", \"Post\", \"Put\", \"Delete\" ] } ], //---Ordering Service: End ----------// //https://localhost:7282/api/Products/GetAll \"GlobalConfiguration\": { // enable request correleation id to capture request information \"RequestIdKey\": \"X-Correlation-Id\", \"BaseUrl\": \"https://localhost:7205/\" } } Modify Program.cs file as followsProgram.cs using Ocelot.DependencyInjection; using Ocelot.Middleware; using Ocelot.Cache.CacheManager; var builder = WebApplication.CreateBuilder(args); var environment = Environment.GetEnvironmentVariable(\"ASPNETCORE_ENVIRONMENT\"); builder.Configuration.SetBasePath(Directory.GetCurrentDirectory()) .AddJsonFile(\"ocelot.json\", optional: false, reloadOnChange: true) .AddEnvironmentVariables(); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); // Swagger for ocelot //builder.Services.AddSwaggerForOcelot(builder.Configuration); //builder.Services.AddSwaggerForOcelot(); builder.Services.AddSwaggerGen(); //For ocelot builder.Services.AddOcelot() // Added for caching .AddCacheManager(x =&gt; { x.WithDictionaryHandle(); }); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); //app.UseSwaggerForOcelotUI(); } app.UseOcelot(); app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run();Step 7: Run and Test application Now run multiple (all) projects and test application using postman. You have to check application using api gateway. For Location service, test using https://localhost:7205/Countries/GetAll For Catalog service, test using https://localhost:7205/Products/{endpoints} For Ordering service, test using https://localhost:7205/api/Orders/{endpoints}Note:Configure caching in api gateway Added the following section in ocelot.json file. \"FileCacheOptions\": { \"TtlSeconds\": 30, \"Region\": \"countriescaching\" } Added the following code block in the the Program.cs file as follows //For ocelotbuilder.Services.AddOcelot() // Added for caching .AddCacheManager(x =&gt; { x.WithDictionaryHandle(); }); Enable case sensative URL Add following blocks in ocelot.json for case sensative URL // Enable case sensative Routing/URL \"RouteIsCaseSensitive\": true Implement rate limiting Add following blocks in ocelot.json for rate limiting // Implement rate limiting // maximum admitted 1 per 5s. \"RateLimitOptions\": { \"ClientWhitelist\": [ // This is an array used to specify the clients that should not be affected by the rate-limiting ], \"EnableRateLimiting\": true, \"Period\": \"5s\", \"PeriodTimespan\": 1, \"Limit\": 1 }Catch all routing Add following blocks in ocelot.json for catch all routing { \"DownstreamPathTemplate\": \"/{url}\", \"DownstreamScheme\": \"https\", \"DownstreamHostAndPorts\": [ { \"Host\": \"localhost\", \"Port\": 7126 } ], \"UpstreamPathTemplate\": \"/{url}\", \"UpstreamHttpMethod\": [ \"Get\", \"Post\", \"Put\", \"Delete\" ] }Source code" }, { "title": "Certification | AZ-400 Certification Path", "url": "/posts/AZ-400-Certification-Path/", "categories": "Microsoft, Certification", "tags": "certification, microsoft, azuredevops, dotnet", "date": "2023-09-10 00:00:00 +0200", "snippet": "Microsoft certification paths changesBoth the MCSD and MCT certifications were discontinued by Microsoft in 2017. They were replaced by newer certification programs that reflect the changing demand...", "content": "Microsoft certification paths changesBoth the MCSD and MCT certifications were discontinued by Microsoft in 2017. They were replaced by newer certification programs that reflect the changing demands of the tech industry and the evolving needs of professionals.The MCSD certification was replaced by the Azure Developer Associate certification, which is a newer certification program that is designed for developers who want to demonstrate their expertise in buildingand deploying cloud-based solutions using Microsoft Azure. The MCT certification was replaced by the Microsoft Certified: Learning Consultant certification, which is a program that recognizes individuals who have demonstrated expertise in designing and delivering training programs on Microsoft technologies.Microsoft offers a wide range of certification programs that are designed to validate the skills of professionals in a variety of different roles. Some of the most popular certification programs offered by Microsoft by role include: Developers: Microsoft offers several certification programs that are designed for developers, including the Azure Developer Associate certification, the Microsoft Certified: Power Platform Developer Associate certification, and noth the Microsoft Certified: Azure Solutions Architect Expert certification and DevOps Engineer Expert. IT Administrators: Microsoft offers several certification programs that are designed for IT administrators, including the Microsoft Certified: Azure Administrator Associate certification, the Microsoft Certified: Azure Security Engineer Associate certification, and the Microsoft Certified: Windows Server 2016 certification. Data Professionals: Microsoft offers several certification programs that are designed for data professionals, including the Microsoft Certified: Data Analyst Associate certification, the Microsoft Certified: Data Scientist Associate certification, and the Microsoft Certified: Azure Data Engineer Associate certification. Business Professionals: Microsoft offers several certification programs that are designed for business professionals, including the Microsoft Certified: Power Platform Fundamentals certification, the Microsoft Certified: Dynamics 365 Fundamentals certification, and the Microsoft Certified: Dynamics 365 Marketing certification. Educators: Microsoft offers several certification programs that are designed for educators, including the Microsoft Certified: Educator certification, the Microsoft Certified: Trainer certification, and the Microsoft Certified: Learning Consultant certification. We will focus in this post on Developers role. So, actually for a dotnet developer the best Microsoft certification to achieve is by following the DevOps Engineer Expert Certification path. Currently the different missions of a dotnet developer require some experience on the devops side for the build and release parts and as well as on the Microsoft Azure part too.Describe certificate pathAccording to The 2021 Pearson VUE Value of IT Certification survey 91% of certified IT professional get more professional credibility, 76% are more valuable to their employers, and 28% of the surveyed candidates reported a salary increase as a benefit of getting certified.Azure DevOps Expert Certification is a three-stars certification that falls under the Microsoft expert-level certifications.AZ-400 Exam PrerequisiteA candidate for this exam must have written either or both of the assoiate Microsoft certification in the schema bellow. Note that the Exam AZ-900: Microsoft Azure Fundamentals are optional and it open to you to pass it. From my part i choose to write the Azure Developer Associate because of my experience as a software developer. For Devops engineer it more recommended to pass the Administrator Associate certification in order to achieve the DevOps Engineer Expert one.Study for your examThe AZ-400 Exam contains about 40-60 questions; however, the number can vary depending on the exam. with a duration of about 180 minutes. Candidates are required to have a passing score of 700/1000 to earn the certification.Structure of the exam Single-choice questions(with YES/NO options) Cannot be reviewed, skipped, or returned to later. Single-choice questions. Multiple-choice questions. Arrange in the right sequence questions. Case-study with multiple questions. Study planA typical study plan may last for a month at about 5 hours of daily study. The study plan depends on the level of experience candidates have on the various topics measured in the exam. Experienced DevOps professionals may spend less time compared to the beginner fellows.Study materialsThis exam measures your ability to accomplish the following technical tasks: develop an instrumentation strategy; develop a Site Reliability Engineering (SRE) strategy; develop a security and compliance plan; manage source control; facilitate communication and collaboration; define and implement continuous integration; and define and implement continuous delivery and release management strategy. The following materials should be helpful.Microsoft learn: Azure DevOps Learning Path Azure Devops Labs: get hands-on experience using various Azure DevOps services to solve real-life scenario business problems. Azure DevOps documentation Azure documentation Practice question materialsThe listed links below are very helpful and may tell you what the questions may look like. All the objectives of this exam are covered so you’ll be ready for any question on the exam. AZ-400: Designing and Implementing Microsoft DevOps Solutions Microsoft Official Practice TestWhizlabs-AZ-400 Practice testExamTopics-AZ-400 Practice testScheduling and Taking your examIt is important you arrive at your test center 15-30 minutes before your scheduled appointment time. This will give you adequate time to complete the necessary sign-in procedures. If you arrive more than 15 minutes late for an exam and are refused admission, payments are due for the exam and delivery fees.Be prepared to show two (2) valid forms of personal identification, a National Identity card, Drivers license, or an International Passport.TipsMark questions whose answers you are not sure about for review, and revisit them when done with the others.Don’t leave any questions unanswered and make sure to submit before proceeding to the next section.Case-study questions come after the first section.Make sure you keep to time.You will receive your transcript upon submission in less than an hour, and an acclaim badge upon passing the exam in your mailbox. Share your badge on social networks like LinkedIn, Twitter, or Facebook.Voila, congratulations, you are now a Microsoft Certified: Azure DevOps Expert, you deserve it. Keep your skills sharp and transfer your knowledge to real work situations.Thank you for reading, I hope this helps you prepare and pass the AZ-400 examination.You can view my Azure DevOps badges here 😊Microsoft Certified: Azure Developer AssociateMicrosoft Certified: DevOps Engineer ExpertReferenceMicrosoft Certified: DevOps Engineer ExpertAzure DevOps Labs2021 Pearson VUE Value of IT CertificationIntroduction to the AZ-400: Designing and Implementing Microsoft DevOps Solutions Exam" }, { "title": "C# | TDD Example using xUnit and Moq", "url": "/posts/C-TDD-Example-using-xUnit-and-Moq/", "categories": "C#, TDD", "tags": "microsoft, csharp, c#, tdd, unittest", "date": "2023-09-07 00:00:00 +0200", "snippet": "In Test-Driven Development (TDD), we write tests before writing the actual code. This example demonstrates how to create unit tests in C# using xUnit, how to use the Moq framework for mocking, and ...", "content": "In Test-Driven Development (TDD), we write tests before writing the actual code. This example demonstrates how to create unit tests in C# using xUnit, how to use the Moq framework for mocking, and how to refactor tests using Fact and Theory attributes.PrerequisitesBefore we start, make sure you have the following installed: Visual Studio or Visual Studio Code (or any C# IDE of your choice) xUnit.net testing framework Moq mocking frameworkScenarioSuppose we are building a simple library that calculates the total price of items in a shopping cart.Step 1: Set Up the ProjectCreate a new C# project and add references to the xUnit and Moq libraries.Step 2: Write the Initial TestLet’s start by writing a test for our shopping cart. Create a test class, and write a Fact that checks if the cart’s total price is calculated correctly:using System;using Xunit;public class ShoppingCartTests{ [Fact] public void CalculateTotalPrice_CartWithItems_ReturnsTotalPrice() { // Arrange var cart = new ShoppingCart(); cart.AddItem(new Item(\"Item 1\", 10.0)); cart.AddItem(new Item(\"Item 2\", 15.0)); // Act var totalPrice = cart.CalculateTotalPrice(); // Assert Assert.Equal(25.0, totalPrice); }}Step 3: Write the Initial CodeNow, create the ShoppingCart class and implement the CalculateTotalPrice method:public class ShoppingCart{ private List&lt;Item&gt; items = new List&lt;Item&gt;(); public void AddItem(Item item) { items.Add(item); } public double CalculateTotalPrice() { return items.Sum(item =&gt; item.Price); }}Step 4: Mocking with MoqSuppose our ShoppingCart class depends on an external data source (e.g., a database). To test it, we can use Moq to mock this dependency. Create an interface for the data source, implement it, and inject it into the ShoppingCart:public interface IDataSource{ List&lt;Item&gt; GetItems();}public class Database : IDataSource{ public List&lt;Item&gt; GetItems() { // Simulate database call return new List&lt;Item&gt; { new Item(\"Item 1\", 10.0), new Item(\"Item 2\", 15.0) }; }}public class ShoppingCart{ private IDataSource dataSource; public ShoppingCart(IDataSource dataSource) { this.dataSource = dataSource; } // ...}Step 5: Refactor the Test with TheoryInstead of Fact, let’s refactor the test using Theory. This allows us to use data from multiple test cases. For instance, we can test the CalculateTotalPrice method with different sets of items:[Theory][InlineData(new[] { 10.0, 15.0 }, 25.0)][InlineData(new[] { 5.0, 7.0, 8.0 }, 20.0)]public void CalculateTotalPrice_CartWithItems_ReturnsTotalPrice(double[] itemPrices, double expectedTotalPrice){ // Arrange var cart = new ShoppingCart(CreateMockDataSource(itemPrices)); // Act var totalPrice = cart.CalculateTotalPrice(); // Assert Assert.Equal(expectedTotalPrice, totalPrice);}Here, we’re using Theory to run the test with different sets of item prices. The CreateMockDataSource function sets up a Moq mock of the IDataSource interface.What Next? This example demonstrates how to use TDD with xUnit, Moq for mocking, and how to refactor tests using Fact and Theory attributes. By following TDD, you can ensure that your code is thoroughly tested and that it meets the specified requirements." }, { "title": "Azure | Azure Functions By Example", "url": "/posts/Azure-Functions-By-Example/", "categories": "Azure, Azure Functions", "tags": "microsoft, azure, azurefunctions", "date": "2023-09-06 00:00:00 +0200", "snippet": "Microsoft Azure Functions ExampleIn this example, we’ll create a simple Azure Functions application in C#. Azure Functions is a serverless compute service that allows you to run event-triggered cod...", "content": "Microsoft Azure Functions ExampleIn this example, we’ll create a simple Azure Functions application in C#. Azure Functions is a serverless compute service that allows you to run event-triggered code without managing infrastructure. We will create a function that responds to an HTTP request.PrerequisitesBefore you begin, make sure you have the following prerequisites: Microsoft Azure account. Azure Functions Extension installed in Visual Studio or Visual Studio Code. .NET Core SDK installed.Create an Azure Function Create a New Function Project: In your development environment, create a new Azure Functions project using the Azure Functions extension. You can choose the template that suits your needs. Add a New Function: Add a new function to your project. Select “HTTP trigger” as the template. This will create a function that responds to HTTP requests. Implement the Function: In the generated function code, you can implement your logic. Here’s a simple example that responds with “Hello, Azure Functions!” when the function is triggered: using System;using Microsoft.AspNetCore.Http;using Microsoft.AspNetCore.Mvc;using Microsoft.Azure.WebJobs;using Microsoft.Azure.WebJobs.Extensions.Http;using Microsoft.Extensions.Logging;public static class HelloWorldFunction{ [FunctionName(\"HelloWorld\")] public static async Task&lt;IActionResult&gt; Run( [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req, ILogger log) { log.LogInformation(\"C# HTTP trigger function processed a request.\"); return new OkObjectResult(\"Hello, Azure Functions!\"); }} Run the Function Locally: You can test your function locally by running it in your development environment. Use the Azure Functions CLI to start the local runtime. func start Deploy to Azure: Once your function is working as expected, you can deploy it to Azure. Use the Azure Functions extension to publish your project to Azure. Test the Function in Azure: After deployment, you can test the function by navigating to its URL. You’ll receive “Hello, Azure Functions!” as a response. What Next?This example demonstrates a basic Azure Functions application in C#. Azure Functions are a powerful way to build serverless applications that respond to various triggers. You can extend this example by adding more functions, integrating with other Azure services, and handling more complex scenarios.Azure Functions are flexible and can be used for a wide range of use cases, including data processing, automation, and API endpoints." }, { "title": "C# | Create .Net custom template using GitHub Packages Registry", "url": "/posts/Create-.Net-custom-template-using-GitHub-Packages-Registry/", "categories": "C#, Nuget Package", "tags": "microsoft, csharp, c#, github, nugetpackage, aspnetcore", "date": "2023-08-21 00:00:00 +0200", "snippet": "Introduction.NET gives us opportunity to create custom template for future use and GitHub packages registry is most popular now a days to host custom template. In this article, I will show you how ...", "content": "Introduction.NET gives us opportunity to create custom template for future use and GitHub packages registry is most popular now a days to host custom template. In this article, I will show you how to create .net custom template using GitHub packages registry.Tools and Technology uses Visual Studio 2022 .NET 6 C# GitHubImplementationStep 1: Create a personal access token (PAT) from GitHub Login into you GitHub Go to settings -&gt; Developer Settings -&gt; Personal Access Tokens Click “Generate new token” button Type Note for the token, expiration days Select scope for the token – here I have selected repo, write:packages, delete:packages as shown below. Now click “Generate Token” at the bottom of the panel Copy the token and store the token for further use because you cannot find it later Step 2: Add Nuget Source in visual studio Type the following command to add sourcedotnet nuget add source https://nuget.pkg.github.com/hbolajraf/index.json --name github-hbolajraf --username hbolajraf --password &lt;Your personal Access Token&gt; You will see a source is added in C:\\Users\\hbolajraf\\AppData\\Roaming\\NuGet\\NuGet.Config file You can add source from visual studio Tools -&gt; Options -&gt; NuGet Package Manager -&gt; Package Sources Restart visual studio to get new nuget package sourceStep 3: Create template for your application Create a project or multiple projects using a solution file. Here, I have created a clean architecture template with a solution file and multiple projects Create a folder name – “.template.config” in the root directory of your application. Create a file template.json in .template.config folder. Add the following content to template.json filetemplate.json { \"$schema\": \"http://json.schemastore.org/template\", \"author\": \"hbolajraf Hasan\", \"classifications\": [ \"dotnet\", \"CleanArchitecture\" ], \"name\": \"Clean Architecture project template\", \"description\": \"Project template to create project using Clean Architecture\", \"identity\": \"CleanArchitecture\", \"shortName\": \"CleanArchitecture\", \"sourceName\": \"CleanArch\", \"tags\": { \"language\": \"C#\", \"type\": \"project\" }, \"symbols\": { \"Framework\": { \"type\": \"parameter\", \"description\": \"The target framework for the project.\", \"datatype\": \"choice\", \"choices\": [ { \"choice\": \"net6.0\" }, { \"choice\": \"net5.0\" } ], \"defaultValue\": \"net6.0\", \"replaces\": \"{TargetFramework}\" } } }Step 4: Install and create template locally (Optional) Go to one where “.template.config” folder exists. Now run the following command. Don’t forgot to add space “.” at the end.dotnet new --install . You will see in the output that template is created. You will see Short Name of template which is used to install template. Now go to the directory where you want to install template and type the following command.dotnet new CleanArchitecture #or,dotnet new CleanArchitecture --forceHere CleanArchitecture is short name of the template To create template by another name type as follows.dotnet new CleanArchitecture -o LocationNow projects name will be Location instead of CleanArch as mentioned in the previous json file. Now go to the same directory to uninstall the template and type the following command.dotnet new --uninstall .```console ## Step 5: Packing a template into a NuGet Package (nupkg file)* Create a .csproj file one directory up of “.template.config” folder.* In my case the folder structure as follows ![](/posts/2022/temp-04.PNG) **Add the following content in TemplatePack.csproj project.****TemplatePack.csproj**```xml &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt; &lt;PropertyGroup&gt; &lt;PackageType&gt;Template&lt;/PackageType&gt; &lt;PackageVersion&gt;1.0.0&lt;/PackageVersion&gt; &lt;PackageId&gt;hbolajraf.CleanArchitecture.Templates&lt;/PackageId&gt; &lt;Title&gt;Clean Architecture Template&lt;/Title&gt; &lt;Authors&gt;hbolajraf Hasan&lt;/Authors&gt; &lt;Description&gt;Clean Architecture Template&lt;/Description&gt; &lt;PackageTags&gt;dotnet-new;templates;clean-architecture&lt;/PackageTags&gt; &lt;TargetFramework&gt;netstandard2.0&lt;/TargetFramework&gt; &lt;IncludeContentInPack&gt;true&lt;/IncludeContentInPack&gt; &lt;IncludeBuildOutput&gt;false&lt;/IncludeBuildOutput&gt; &lt;ContentTargetFolders&gt;content&lt;/ContentTargetFolders&gt; &lt;NoWarn&gt;$(NoWarn);NU5128&lt;/NoWarn&gt; &lt;NoDefaultExcludes&gt;true&lt;/NoDefaultExcludes&gt; &lt;RepositoryUrl&gt;https://github.com/hbolajraf/public-packages&lt;/RepositoryUrl&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;Content Include=\"CleanArchitecture\\**\\*\" Exclude=\"CleanArchitecture\\**\\bin\\**;CleanArchitecture\\**\\obj\\**\" /&gt; &lt;Compile Remove=\"..\\**\\*\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; To create package go to the directory where TemplatePack.csproj file exists and type the following command.dotnet pack You will hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg file is created in bin/Debug folder.Step 6: Now push the package to github package registry Go to the directory where hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg is exists. Type the following command to push the package in github package registry dotnet nuget push .\\hbolajraf.CleanArchitecture.Templates.1.0.0.nupkg –api-key --source github-hbolajraf Here, “github-hbolajraf” is a github source which we have added in step – 2. Now login your github and you will see a template is uploaded to your package registry.Step 7: Download template and install in local machine Run the following command to install in local machinedotnet new --install hbolajraf.CleanArchitecture.Templateshbolajraf.CleanArchitecture.Templates is package Id.output:The following template packages will be installed: hbolajraf.CleanArchitecture.TemplatesSuccess: hbolajraf.CleanArchitecture.Templates::1.0.0 installed the following templates:Template Name Short Name Language Tags----------------------------------- ----------------- -------- ------------------------Clean Architecture project template CleanArchitecture [C#] dotnet/CleanArchitecture Now go to the directory where you want to regenerate the template and type the following command.dotnet new CleanArchitectureHere CleanArchitecture is the short name of the templateNote To see installed template in locally use the following command. You will also see how to uninstall the particular template.dotnet new --uninstall To uninstall a particular template from local machine, use the following command.dotnet new --uninstall hbolajraf.CleanArchitecture.Templatesdotnet new –uninstall &lt;package id&gt; Source code" }, { "title": "GitHub | Deploy .net core NuGet Packages in GitHub Packages Registry", "url": "/posts/Deploy-.net-core-NuGet-Packages-in-GitHub-Packages-Registry/", "categories": "GitHub, Nuget Package", "tags": "microsoft, csharp, c#, nuget, nugetpackage, aspnetcore, github", "date": "2023-08-19 00:00:00 +0200", "snippet": "IntroductionGitHub packages registries is most popular now a days. It offers different packages registries for most used package managers, such as NuGet, npm, Docker etc. In this article, I will sh...", "content": "IntroductionGitHub packages registries is most popular now a days. It offers different packages registries for most used package managers, such as NuGet, npm, Docker etc. In this article, I will show you how to host a .net core NuGet Package in GitHub Packages Registry.Tools and Technology uses Visual Studio 2022 .NET 6 C# GitHubImplementationStep 1: Create a personal access token (PAT) from GitHub Login into you GitHub Go to settings -&gt; Developer Settings -&gt; Personal Access Tokens Click “Generate new token” button Type Note for the token, expiration days Select scope for the token – here I have selected repo, write:packages, delete:packages as shown below. Now click “Generate Toke” at the bottom of the panel Copy the token and store the token for further use because you cannot find it later Step 2: Add Nuget Source in visual studio Type the following command to add sourcedotnet nuget add source https://nuget.pkg.github.com/hbolajraf/index.json --name github-hbolajraf --username hbolajraf --password &lt;Your personal Access Token&gt; You will see a source is added in C:\\Users\\hbolajraf\\AppData\\Roaming\\NuGet\\NuGet.Config file Optional: You can also add source from visual studio Tools -&gt; Options -&gt; NuGet Package Manager -&gt; Package Sources Restart visual studio to get new nuget package sourceStep 3: Create a class library to publish in GitHub Packages Create a class library name – ‘CryptoEngine” Create a class CryptoGenerator as follows using System.Security.Cryptography; using System.Text; namespace CryptoEngine { public class CryptoGenerator { public static string GenerateSha256Hash(string plainText) { // Create a SHA256 using (SHA256 sha256Hash = SHA256.Create()) { // ComputeHash - returns byte array byte[] bytes = sha256Hash.ComputeHash(Encoding.UTF8.GetBytes(plainText)); // Convert byte array to a string StringBuilder builder = new StringBuilder(); for (int i = 0; i &lt; bytes.Length; i++) { builder.Append(bytes[i].ToString(\"x2\")); } return builder.ToString(); } } } } Click right button on class library project -&gt; Package -&gt; General Mark “Produce a package file during build operations” Type Package ID, Package Version, Authors, Company, Product, Description Type repository URL – A github repository and save Now you will see the csproj file as followsCryptoEngine.csproj &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt; &lt;PropertyGroup&gt; &lt;TargetFramework&gt;net6.0&lt;/TargetFramework&gt; &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt; &lt;Nullable&gt;enable&lt;/Nullable&gt; &lt;GeneratePackageOnBuild&gt;True&lt;/GeneratePackageOnBuild&gt; &lt;PackageId&gt;hbolajraf.CryptoEngine&lt;/PackageId&gt; &lt;Version&gt;1.0.0&lt;/Version&gt; &lt;Authors&gt;hbolajraf hasan&lt;/Authors&gt; &lt;Company&gt;hbolajraf.NET&lt;/Company&gt; &lt;Product&gt;CryptoEngine&lt;/Product&gt; &lt;Description&gt;Chipper text generator&lt;/Description&gt; &lt;RepositoryUrl&gt;https://github.com/hbolajraf/public-packages&lt;/RepositoryUrl&gt; &lt;/PropertyGroup&gt; &lt;/Project&gt;Step 4: Create a NuGet Package Click right button on project and select Pack A NuGet package will be generated in bin/Debug folder – In this case the nuget package name is hbolajraf.CryptoEngine.1.0.0.nupkg Or, Go to the directory where .csproj file exists and right the following command to generate nuget packagedotnet packStep 5: Push NuGet package to GitHub Package Registry Go to the directory where package generated – bin/Debug in this case. Type following commanddotnet nuget push .\\hbolajraf.CryptoEngine.1.0.0.nupkg --api-key &lt;your github access token&gt; --source github-hbolajrafHere github-hbolajraf is my nuget source name for visual studio. Already added in step 2. Now login to your Github account and go to Packages tab, you will see a package is uploaded. In this case package name is hbolajraf.CryptoEngineStep 6: Use already uploaded package in a project If Nuget package source is not added, add it using step – 2 Go to package manager console Select Package Source as “github-hbolajraf” and type following commandPM&gt; Install-Package hbolajraf.CryptoEngine Or right button on project -&gt; Manage Nuget Packages Select Package source “github-hbolajraf” Browse and install package hbolajraf.CryptoEngineSource code" }, { "title": "C# | Create Nuget Package using .NET Standard", "url": "/posts/Create-Nuget-Package-using-.NET-Standard/", "categories": "C#, Nuget Package", "tags": "microsoft, csharp, c#, nuget, nugetpackage", "date": "2023-08-17 00:00:00 +0200", "snippet": "Tools and technologies used Visual Studio 2022 .NET Standard 2.1 Nuget.exeImplementationNew Project CreationUnder Visual Studio create a new Project Class Library and use .NET Standard 2.1 as ta...", "content": "Tools and technologies used Visual Studio 2022 .NET Standard 2.1 Nuget.exeImplementationNew Project CreationUnder Visual Studio create a new Project Class Library and use .NET Standard 2.1 as target framework due to compatibility reason with latests versions of .NET CORE Frameworks.Use Nuget CLI to generate files1.Download Nuget.exe fileUse the following link to download the latests version of Nuget.exe file.2.Generate nuspec file Under the new project folder created befor, open a cmd console and run the comand bellow in order to generate the nuspec file.nuget spec NewProjectName.csprojThe result of the command should generate a new file which has the content below :&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;package &gt; &lt;metadata&gt; &lt;id&gt;$id$&lt;/id&gt; &lt;version&gt;$version$&lt;/version&gt; &lt;title&gt;$title$&lt;/title&gt; &lt;authors&gt;$author$&lt;/authors&gt; &lt;requireLicenseAcceptance&gt;false&lt;/requireLicenseAcceptance&gt; &lt;license type=\"expression\"&gt;MIT&lt;/license&gt; &lt;!-- &lt;icon&gt;icon.png&lt;/icon&gt; --&gt; &lt;projectUrl&gt;http://project_url_here_or_delete_this_line/&lt;/projectUrl&gt; &lt;description&gt;$description$&lt;/description&gt; &lt;releaseNotes&gt;Summary of changes made in this release of the package.&lt;/releaseNotes&gt; &lt;copyright&gt;$copyright$&lt;/copyright&gt; &lt;tags&gt;Tag1 Tag2&lt;/tags&gt; &lt;/metadata&gt;&lt;/package&gt;3.Generate nupkg fileYou have two solutions order to generate the nuget package file(nupkg)using the post-build event of the project Under Visual Studio right click on the NewProjectName.crproj and select post-build event tab.After that put the command bellow and Build the solutionnuget pack \"$(ProjectPath)\" -Symbols -Properties Configuration=$(ConfigurationName) -IncludeReferencedProjects -OutputDirectory \"C:\\Dev\\nuget_packages\\NewProjectName\\\"using the Nuget CLI command Under the cmd window tape the command bellow in order to generate the nuget packagenuget pack MyProject.csproj -properties Configuration=Release -OutputDirectory \"C:\\Dev\\nuget_packages\\NewProjectName\\\"In all cases the new nuget package file will be generated under the output directory : *C:\\Dev\\nuget_packages\\NewProjectName*What next ?Once you’ve created a package, which is a .nupkg file, you can publish it to the gallery of your choice(Artifactory, Azure artifacts or GitHub Package registry)" }, { "title": "C# | ASP.NET Web APIs with Zipkin", "url": "/posts/ASP.NET-Web-APIs-with-Zipkin-and-C/", "categories": "C#, Web API", "tags": "microsoft, csharp, c#, webapi, zipkin", "date": "2023-08-09 00:00:00 +0200", "snippet": "ASP.NET Web APIs with Zipkin and C#Zipkin is a distributed tracing system that helps you monitor and troubleshoot microservices-based applications. This guide will walk you through the steps to int...", "content": "ASP.NET Web APIs with Zipkin and C#Zipkin is a distributed tracing system that helps you monitor and troubleshoot microservices-based applications. This guide will walk you through the steps to integrate Zipkin with a C# Web API.PrerequisitesBefore you start, make sure you have the following prerequisites: .NET Core SDK installed on your system. A C# Web API project that you want to instrument with Zipkin. Zipkin server up and running (you can deploy Zipkin using Docker or as a standalone service).Steps to Integrate ZipkinStep 1: Install the Required PackagesYou need to install the necessary packages to enable Zipkin integration in your C# Web API project. Open your project in the terminal or command prompt and use the following command to install the required NuGet packages:dotnet add package OpenTracingdotnet add package OpenTracing.Contrib.NetCoredotnet add package JaegerStep 2: Configure Zipkin TracingIn your C# Web API project, you’ll need to configure Zipkin tracing. Create a configuration class or use your existing configuration:using Jaeger;using Jaeger.Reporters;using Jaeger.Samplers;using Jaeger.Senders;using OpenTracing;public static class ZipkinConfig{ public static ITracer ConfigureTracer(string serviceName) { var reporter = new RemoteReporter.Builder() .WithSender(new UdpSender(\"your-zipkin-server-host\", 9411, 0)) .Build(); var sampler = new ConstSampler(true); var tracer = new Tracer.Builder(serviceName) .WithReporter(reporter) .WithSampler(sampler) .Build(); GlobalTracer.Register(tracer); return tracer; }}Step 3: Instrument Your Web APIIn your Web API controllers or middleware, you can use the ITracer to start and finish spans, which represent different parts of your API request:using System;using Microsoft.AspNetCore.Mvc;using OpenTracing;[Route(\"api/[controller]\")]public class MyController : ControllerBase{ private readonly ITracer _tracer; public MyController(ITracer tracer) { _tracer = tracer; } [HttpGet] public ActionResult&lt;string&gt; Get() { using (var scope = _tracer.BuildSpan(\"MyController.Get\").StartActive()) { // Your API logic here return \"Hello, World!\"; } }}Step 4: Start Zipkin and Observe TracesStart your Zipkin server, and your C# Web API is now instrumented with Zipkin tracing. When you make requests to your API, you can use the Zipkin web UI to observe traces and troubleshoot issues in your microservices-based application.What Next? That’s it! You’ve successfully integrated Zipkin with your C# Web API to monitor and trace requests across your microservices." }, { "title": "SonarQube | Working with SonarLint and SonarQube in Visual Studio", "url": "/posts/Working-with-SonarLint-and-SonarQube-in-Visual-Studio/", "categories": "Best Practices, SonarQube", "tags": "microsoft, visualstudio, sonarqube, sonarlint", "date": "2023-08-05 00:00:00 +0200", "snippet": "Working with SonarLint and SonarQube in Visual StudioSonarLint and SonarQube are powerful tools for code quality and static code analysis in C# and other programming languages. They help you identi...", "content": "Working with SonarLint and SonarQube in Visual StudioSonarLint and SonarQube are powerful tools for code quality and static code analysis in C# and other programming languages. They help you identify and fix code issues and vulnerabilities. In this guide, we’ll walk you through how to set up and use SonarLint in Visual Studio and integrate it with SonarQube for more advanced analysis.Prerequisites Visual Studio: Make sure you have Visual Studio installed on your machine. SonarLint works as a Visual Studio extension. SonarQube Server: If you plan to use SonarQube for more advanced analysis, you’ll need access to a SonarQube server. You can install one locally or use a remote server. Setting Up SonarLint1. Install SonarLint Extension: Open Visual Studio. Go to Extensions -&gt; Manage Extensions. Search for “SonarLint” and install the extension.2. Binding to a SonarQube Server (Optional): If you want to connect SonarLint to your SonarQube server for synchronized rules and quality profiles, go to Tools -&gt; Options -&gt; SonarLint. Click “Connect to SonarQube” and provide the server URL and authentication details.3. Binding to SonarQube Projects (Optional): If connected to a SonarQube server, you can bind your Visual Studio projects to SonarQube projects. This ensures that your code is analyzed using SonarQube rules. Right-click on the project in Solution Explorer -&gt; SonarLint -&gt; Bind to SonarQube project.4. Analyzing Code: SonarLint will automatically analyze your code in real-time as you work in Visual Studio. Detected issues and suggestions will be highlighted in your code, and you can see details in the SonarLint window.Setting Up SonarQube IntegrationTo perform more advanced analysis and manage projects centrally, you can integrate SonarQube with Visual Studio.1. Install SonarQube Scanner for MSBuild: Download and install the SonarQube Scanner for MSBuild.2. Configure SonarQube Server: In your project’s root directory, create a sonar-project.properties file. Configure it with your SonarQube server details. sonar.host.url=http://your-sonarqube-server-url sonar.login=your-auth-token sonar.projectKey=unique-project-key3. Run Analysis: Open a Command Prompt or PowerShell window and navigate to your project directory. Run the following command to perform an analysis: MSBuild.SonarQube.Runner.exe begin /k:\"your-project-key\" MSBuild.exe MSBuild.SonarQube.Runner.exe end4. View Results: Visit your SonarQube server in a web browser to view the analysis results and manage your project.What Next? With these steps, you can effectively use SonarLint for real-time code analysis within Visual Studio and integrate SonarQube for more advanced analysis, quality management, and reporting.You can consult the official SonarLint and SonarQube documentation for detailed setup and configuration instructions." }, { "title": "Azure DevOps | Using Terraform", "url": "/posts/Azure-DevOps-Using-Terraform/", "categories": "Azure DevOps, Terraform", "tags": "microsoft, azuredevops, azure, terraform", "date": "2023-07-29 00:00:00 +0200", "snippet": "Using Terraform with Azure DevOpsAzure DevOps is a set of development tools and services for software development, while Terraform is a popular infrastructure as code (IaC) tool used to provision a...", "content": "Using Terraform with Azure DevOpsAzure DevOps is a set of development tools and services for software development, while Terraform is a popular infrastructure as code (IaC) tool used to provision and manage cloud resources. Combining the power of Terraform with Azure DevOps can help automate infrastructure deployment and management in your projects.In this guide, we’ll walk through the steps to set up and integrate Terraform with Azure DevOps.Prerequisites Azure DevOps Account: You need an Azure DevOps account to get started. Sign up if you don’t have one. Azure Subscription: Access to an Azure subscription to create and manage Azure resources. Terraform: Install Terraform on your local machine. Azure CLI: Install the Azure CLI. Steps1. Create a New Azure DevOps Project Log in to your Azure DevOps account. Create a new project or select an existing one for your Terraform infrastructure. 2. Set Up a Git Repository Create a Git repository within your Azure DevOps project to store your Terraform code. Clone the repository to your local machine. 3. Terraform Configuration Create a directory for your Terraform configurations within your Git repository. Write your Terraform code for provisioning Azure resources in this directory. Initialize your Terraform configuration: terraform init Create a main.tf file and define your Azure resources using the Azure Terraform Provider. Use environment variables or other methods to securely store your Azure credentials and sensitive information. 4. Create an Azure DevOps Pipeline In your Azure DevOps project, go to Pipelines and create a new pipeline. Select the Git repository where your Terraform code is stored. Choose a pipeline template based on your project’s needs. You can also create a YAML pipeline. Configure the pipeline to execute Terraform commands (e.g., terraform init, terraform plan, and terraform apply) as tasks. 5. Securely Store Secrets Use Azure DevOps Variable Groups or Azure Key Vault to securely manage and store secrets, such as Azure service principal credentials.6. Trigger the Pipeline Commit your Terraform code to the Git repository. Manually trigger the Azure DevOps pipeline or set up automatic triggers, such as on code commits or pull requests. 7. Monitor and Manage Monitor the pipeline’s progress and logs through the Azure DevOps interface. Leverage Azure DevOps features like approval gates and release environments for more advanced control and automation. 8. Cleanup After you’re done with your resources, add Terraform code to destroy or deprovision them. Run terraform destroy in your Azure DevOps pipeline when you’re ready to clean up your Azure resources. By following these steps, you can set up a seamless integration between Terraform and Azure DevOps, automating your infrastructure provisioning and management processes.What Next?Using Terraform in conjunction with Azure DevOps can significantly enhance your infrastructure provisioning and management workflow. It allows for automation, version control, and collaboration, making your infrastructure as code projects more efficient and reliable." }, { "title": "C# | Visual Studio Extensions for C# Developers", "url": "/posts/Visual-Studio-Extensions-for-C-Developers/", "categories": "Visual Studio, Tips And Tricks", "tags": "microsoft, visualstudio, tips&tricks", "date": "2023-07-24 00:00:00 +0200", "snippet": "Visual Studio extensions can significantly enhance your C# development workflow by adding features and tools that make coding, debugging, and project management more efficient. Here’s a list of ess...", "content": "Visual Studio extensions can significantly enhance your C# development workflow by adding features and tools that make coding, debugging, and project management more efficient. Here’s a list of essential extensions that every C# developer should consider installing.Coding Productivity ReSharper: A powerful productivity tool that provides code analysis, quick-fixes, refactorings, and intelligent code completion. Visual Studio IntelliCode: Utilizes AI to provide intelligent, context-aware code completion recommendations based on your coding patterns. Visual Studio Live Share: Collaborative development tool that allows you to share your coding session with others in real-time. CodeMaid: Helps maintain a cleaner codebase by organizing, formatting, and simplifying your code. Debugging OzCode: Advanced debugging tool that provides time-travel debugging, exceptional value tracking, and other debugging enhancements. Debugger for Unity: If you’re working with Unity for game development, this extension adds debugging support for Unity projects. Version Control Visual Studio GitHub: Integrates GitHub with Visual Studio, providing seamless version control and code collaboration features.Code Analysis and Quality SonarLint: A static code analysis tool that helps identify and fix code quality issues as you write your code. Roslynator: Offers a wide range of code analyzers, refactorings, and code fixes based on the Roslyn compiler platform. Project Management NUnit Test Adapter: If you’re using NUnit for unit testing, this adapter allows you to run and debug NUnit tests within Visual Studio. Visual Studio Installer Projects: Provides project templates for creating custom installation packages for your applications. NuGet Package Manager: Manage NuGet packages directly within Visual Studio to easily add and update dependencies in your projects. Documentation GhostDoc: Simplifies the process of creating and maintaining code documentation by generating XML comments and helping with documentation standards.UI and Design XAML Styler: Provides code formatting and styling for XAML markup, helping maintain clean and consistent UI code.Markdown Editing Markdown Editor: If you’re writing documentation or READMEs in Markdown, this extension enhances the Markdown editing experience.Git Integration Git Tools: Adds Git integration and makes it easier to manage Git repositories directly from Visual Studio.What Next? These Visual Studio extensions can significantly improve your C# development experience. Install the ones that suit your needs and workflow to boost your productivity and code quality." }, { "title": "Azure DevOps | Deploy Postman Tests in Azure DevOps Test Plans", "url": "/posts/Azure-DevOps-Deploy-Postman-Tests-in-Azure-DevOps-Test-Plans/", "categories": "Azure DevOps, Postman", "tags": "microsoft, azuredevops, azure, postman, tests", "date": "2023-07-22 00:00:00 +0200", "snippet": "Deploy Postman Tests in Azure DevOps Test PlansAzure DevOps allows you to automate the testing of your APIs and applications, and Postman is a popular tool for API testing. In this guide, we will w...", "content": "Deploy Postman Tests in Azure DevOps Test PlansAzure DevOps allows you to automate the testing of your APIs and applications, and Postman is a popular tool for API testing. In this guide, we will walk through the process of deploying Postman tests as part of your Azure DevOps pipeline.Prerequisites An Azure DevOps account and a project set up. Postman collection containing the tests you want to run. A basic understanding of Azure DevOps pipelines.StepsStep 1: Add Postman Collection to Your Repository Ensure your Postman collection is saved in a location that is accessible by your Azure DevOps repository. This can be the same repository or a shared location. Commit the Postman collection to your repository, so it’s available for pipeline execution. Step 2: Create an Azure DevOps Pipeline In your Azure DevOps project, go to the “Pipelines” section. Click on “New Pipeline” to create a new pipeline. Select your repository as the source for the pipeline. Choose a template or start with an “Empty job” if you want to configure your pipeline from scratch. Step 3: Configure the Pipeline In your pipeline YAML file, you can define a job to run Postman tests.jobs:- job: RunPostmanTests steps: - script: | # Install Newman (Postman CLI) npm install -g newman # Run Postman collection newman run path/to/your/postman_collection.json displayName: 'Run Postman Tests'Make sure to replace path/to/your/postman_collection.json with the actual path to your Postman collection file. You can also configure the pipeline to run these tests as part of a specific trigger, such as on every code commit or on a schedule.Step 4: Save and Trigger the Pipeline Save your pipeline configuration. Manually trigger the pipeline to verify that your Postman tests are executed. Monitor the pipeline’s output for test results. What Next?By deploying Postman tests within Azure DevOps, you can automate the testing of your APIs as part of your continuous integration and continuous delivery (CI/CD) process. This ensures that your API tests are consistently executed, helping you catch issues early in the development cycle." }, { "title": "Azure DevOps | Running a Postman Collection using Newman Docker Image", "url": "/posts/Azure-DevOps-Running-a-Postman-Collection-using-Newman-Docker-Image/", "categories": "Azure DevOps, Postman", "tags": "microsoft, azuredevops, azure, postman, docker, newman", "date": "2023-07-20 00:00:00 +0200", "snippet": "Running a Postman Collection using Newman Docker ImageNewman is a command-line collection runner for Postman that allows you to automate and test your APIs. You can use the Newman Docker image to r...", "content": "Running a Postman Collection using Newman Docker ImageNewman is a command-line collection runner for Postman that allows you to automate and test your APIs. You can use the Newman Docker image to run Postman collections in a containerized environment.PrerequisitesBefore you begin, make sure you have Docker installed on your system.Step 1: Pull the Newman Docker ImageOpen your terminal and pull the Newman Docker image from Docker Hub using the following command:docker pull postman/newmanStep 2: Create a Postman CollectionCreate a Postman collection that includes the API requests you want to run. You can use the Postman app to create and export collections.Step 3: Export Your Postman CollectionExport your Postman collection as a JSON file. You can do this by opening the collection in Postman, clicking the “Export” button, and selecting “Collection v2” or “Collection v2.1” as the export format.Step 4: Run the Postman Collection using NewmanUse the Newman Docker image to run your Postman collection as follows:docker run -t postman/newman run &lt;path-to-collection-file.json&gt; --env-var VAR1=Value1 --env-var VAR2=Value2 &lt;path-to-collection-file.json&gt;: Replace this with the path to the Postman collection JSON file you exported in step 3. --env-var VAR1=Value1 and --env-var VAR2=Value2: If your collection relies on environment variables, you can set them using the --env-var flag. ExampleHere’s an example command to run a Postman collection named “my-api-tests.json” with environment variables:docker run -t postman/newman run my-api-tests.json --env-var API_URL=https://api.example.com --env-var API_KEY=your-api-keyAdditional OptionsYou can customize the Newman run using various Newman CLI options. For example, you can specify reporters, specify a folder for reports, and more.What Next?Using the Newman Docker image, you can easily automate the testing of your APIs by running Postman collections within a containerized environment. This is particularly useful for continuous integration and automated testing pipelines." }, { "title": "C# | Using Entity Framework with PostgreSQL Database", "url": "/posts/C-Using-Entity-Framework-with-PostgreSQL-Database/", "categories": "C#, Entity Framework", "tags": "microsoft, csharp, c#, entityframework, postgresql, sql, database", "date": "2023-07-17 00:00:00 +0200", "snippet": "Using Entity Framework with PostgreSQL Database in C#In this guide, we will explore how to use C# Entity Framework Core to interact with a PostgreSQL database. Entity Framework Core is a powerful O...", "content": "Using Entity Framework with PostgreSQL Database in C#In this guide, we will explore how to use C# Entity Framework Core to interact with a PostgreSQL database. Entity Framework Core is a powerful Object-Relational Mapping (ORM) framework that simplifies database operations in C# applications. PostgreSQL is a robust, open-source relational database management system.Prerequisites Basic knowledge of C# programming. Visual Studio or Visual Studio Code. .NET Core SDK installed. PostgreSQL Database Server installed and running.SetupInstall Required PackagesTo use Entity Framework Core with PostgreSQL, you need to install the following NuGet packages: Microsoft.EntityFrameworkCore Npgsql.EntityFrameworkCore.PostgreSQLYou can install them using the NuGet Package Manager or the .csproj file.Database Connection StringMake sure to configure a connection string to your PostgreSQL database in your application’s configuration. The connection string typically looks like this:Server=myserver;Port=myport;Database=mydatabase;User Id=hbolajraf;Password=azerty_:=);Creating a ModelA model represents a database table. Create a C# class for your model, and annotate it with the [Key] attribute for the primary key and other attributes to define the table structure.public class Product{ [Key] public int Id { get; set; } public string Name { get; set; } public decimal Price { get; set; }}Database ContextA database context represents a session with the database. Create a database context class that derives from DbContext and includes a DbSet for each model.public class AppDbContext : DbContext{ public AppDbContext(DbContextOptions&lt;AppDbContext&gt; options) : base(options) { } public DbSet&lt;Product&gt; Products { get; set; }}MigrationEntity Framework Core uses migrations to create and update the database schema. Run the following commands to create and apply migrations:dotnet ef migrations add InitialCreatedotnet ef database updateThis will create the database schema based on your model.CRUD OperationsYou can now perform CRUD (Create, Read, Update, Delete) operations on your database using Entity Framework Core. Here are some examples: Create:using (var context = new AppDbContext()){ var newProduct = new Product { Name = \"Tonic Bimo\", Price = 10.99 }; context.Products.Add(newProduct); context.SaveChanges();} Read:using (var context = new AppDbContext()){ var products = context.Products.ToList();} Update:using (var context = new AppDbContext()){ var product = context.Products.Find(1); if (product != null) { product.Price = 20.99; context.SaveChanges(); }} Delete:using (var context = new AppDbContext()){ var product = context.Products.Find(1); if (product != null) { context.Products.Remove(product); context.SaveChanges(); }}Querying the DatabaseEntity Framework Core provides a powerful query API. You can use LINQ queries to retrieve data from the database.using (var context = new AppDbContext()){ var expensiveProducts = context.Products .Where(p =&gt; p.Price &gt; 20) .ToList();}You can write complex queries and join multiple tables using LINQ.What Next?Entity Framework Core simplifies working with a PostgreSQL database in C# applications. With the steps outlined in this guide, you can create, read, update, and delete data, as well as perform complex queries, all with the power and flexibility of Entity Framework Core and the reliability of PostgreSQL." }, { "title": "Azure DevOps | Installing Postman and Newman using npm", "url": "/posts/Azure-DevOps-Installing-Postman-and-Newman-using-npm/", "categories": "Azure DevOps, Postman", "tags": "microsoft, azuredevops, azure, postman, npm, newman", "date": "2023-07-14 00:00:00 +0200", "snippet": "Installing Postman and Newman with npmIn this guide, we will walk through the steps to install Postman and Newman using npm (Node Package Manager) and how to run a Postman collection using Newman.P...", "content": "Installing Postman and Newman with npmIn this guide, we will walk through the steps to install Postman and Newman using npm (Node Package Manager) and how to run a Postman collection using Newman.Prerequisites Node.js and npm installed on your system.Install PostmanPostman provides a user-friendly graphical interface for creating and managing API requests and collections. To install Postman: Open your terminal or command prompt. Use npm to install the Postman command-line tool: npm install -g postman Once the installation is complete, you can run Postman by typing postman in your terminal. Install NewmanNewman is a command-line collection runner for Postman that allows you to run and automate Postman collections. To install Newman: Open your terminal or command prompt. Use npm to install Newman globally: npm install -g newman Once the installation is complete, you can use Newman to run Postman collections from the command line. Run a Postman CollectionNow that you have Postman and Newman installed, let’s run a Postman collection using Newman. Export a Postman collection from the Postman application if you haven’t already. Save it as a JSON file (e.g., example_collection.json). Open your terminal or command prompt. Run the Postman collection using Newman: newman run example_collection.json Replace example_collection.json with the name of your Postman collection file. Newman will execute the requests in your collection and provide a summary of the results, including the number of requests run and their status. What Next?You’ve successfully installed Postman and Newman and used Newman to run a Postman collection from the command line. This is useful for automating API testing and integration into your CI/CD pipelines." }, { "title": "C# | Entity Framework Generic Repository with SOLID Design Pattern", "url": "/posts/C-Entity-Framework-Generic-Repository-with-SOLID-Design-Pattern/", "categories": "C#, Entity Framework", "tags": "microsoft, csharp, c#, entityframework, solid, designpattern, repository", "date": "2023-07-09 00:00:00 +0200", "snippet": "Entity Framework Generic Repository with SOLID Design Pattern in C#In this guide, we’ll explore how to create a generic repository in a C# application using Entity Framework while adhering to SOLID...", "content": "Entity Framework Generic Repository with SOLID Design Pattern in C#In this guide, we’ll explore how to create a generic repository in a C# application using Entity Framework while adhering to SOLID design principles. A generic repository allows you to interact with the database in a more organized and reusable manner.PrerequisitesBefore you begin, ensure you have the following prerequisites: Basic knowledge of C# and Entity Framework. An existing C# project with Entity Framework setup.SOLID Design PrinciplesWe’ll focus on the following SOLID principles: Single Responsibility Principle (SRP): Each class should have a single reason to change. Open-Closed Principle (OCP): Software entities (classes, modules, functions) should be open for extension but closed for modification. Liskov Substitution Principle (LSP): Subtypes must be substitutable for their base types. Interface Segregation Principle (ISP): A client should not be forced to implement interfaces they do not use. Dependency Inversion Principle (DIP): High-level modules should not depend on low-level modules. Both should depend on abstractions.Creating a Generic Repository Create a Generic Repository Interface: public interface IRepository&lt;T&gt; where T : class{ Task&lt;T&gt; GetByIdAsync(int id); Task&lt;IEnumerable&lt;T&gt;&gt; GetAllAsync(); Task AddAsync(T entity); Task UpdateAsync(T entity); Task DeleteAsync(T entity);} Implement the Generic Repository: public class Repository&lt;T&gt; : IRepository&lt;T&gt; where T : class{ private readonly DbContext _context; public Repository(DbContext context) { _context = context; } public async Task&lt;T&gt; GetByIdAsync(int id) { return await _context.Set&lt;T&gt;().FindAsync(id); } public async Task&lt;IEnumerable&lt;T&gt;&gt; GetAllAsync() { return await _context.Set&lt;T&gt;().ToListAsync(); } public async Task AddAsync(T entity) { await _context.Set&lt;T&gt;().AddAsync(entity); } public async Task UpdateAsync(T entity) { _context.Set&lt;T&gt;().Update(entity); } public async Task DeleteAsync(T entity) { _context.Set&lt;T&gt;().Remove(entity); }} Using the Generic Repository Dependency Injection:In your application’s startup or configuration, inject the repository into your services. services.AddScoped(typeof(IRepository&lt;&gt;), typeof(Repository&lt;&gt;)); Using the Repository:In your services or controllers, use the generic repository to interact with the database. public class MyService{ private readonly IRepository&lt;MyEntity&gt; _repository; public MyService(IRepository&lt;MyEntity&gt; repository) { _repository = repository; } public async Task&lt;MyEntity&gt; GetEntityById(int id) { return await _repository.GetByIdAsync(id); } // Implement other methods as needed} Apply SOLID Principles:Ensure your services adhere to SOLID principles, like separating concerns and following the Single Responsibility Principle, when implementing business logic. What Next?Creating a generic repository using Entity Framework and adhering to SOLID design principles can make your C# application more maintainable and scalable. By using this pattern, you can easily extend and modify your data access layer without affecting the rest of your application.Remember to adapt the code and principles to your specific project’s needs and requirements." }, { "title": "C# | Using the yield Keyword", "url": "/posts/C-Using-the-yield-Keyword/", "categories": "C#, Best Practices", "tags": "microsoft, csharp, c#, bestpractices", "date": "2023-07-08 00:00:00 +0200", "snippet": "Using the yield Keyword in C#In C#, the yield keyword is used to create an iterator. It enables you to efficiently process a sequence of data one element at a time, without having to generate the e...", "content": "Using the yield Keyword in C#In C#, the yield keyword is used to create an iterator. It enables you to efficiently process a sequence of data one element at a time, without having to generate the entire sequence in memory. This can be particularly useful for working with large data sets or when you want to generate elements on-the-fly.How yield WorksWhen you use yield in a method, it signals to the C# compiler that this method should be treated as an iterator. The method can then produce a sequence of values using the yield return statement. The iterator method, when called, will execute up to the first yield return statement and then pause. When the consumer requests the next element, the method continues execution from where it left off, generating the next value. This process continues until all values have been generated, and the iterator method is considered exhausted.Here is a simple example of how to use the yield keyword to create an iterator:using System;using System.Collections.Generic;public class IteratorDemo{ public static IEnumerable&lt;int&gt; GenerateNumbers() { for (int i = 0; i &lt; 10; i++) { yield return i; } } public static void Main(string[] args) { foreach (var number in GenerateNumbers()) { Console.WriteLine(number); } }}In this example, the GenerateNumbers method is an iterator that generates numbers from 0 to 9 using yield return. The foreach loop then iterates through the generated numbers.Benefits of yieldUsing yield provides several benefits: Memory Efficiency: yield allows you to work with large data sets without loading the entire set into memory, which can save memory and improve performance. Lazy Evaluation: Elements are generated on-the-fly as they are needed, enabling lazy evaluation and efficient resource utilization. Simplified Code: yield can simplify the code for generating sequences, making it more readable and maintainable. Deferred Execution: The execution of the iterator is deferred until you request elements, making it possible to generate data progressively. Common Use Cases File Parsing: When parsing large log files, you can use yield to read and process one line at a time. Database Queries: yield can be used with databases to fetch and process records as needed. Infinite Sequences: Create iterators for sequences that are theoretically infinite, such as a sequence of prime numbers. Custom Collections: Implement custom collections with yield to provide efficient enumeration. What Next?The yield keyword is a powerful feature in C# that allows you to work with sequences of data in a memory-efficient and convenient way. It is especially useful when dealing with large data sets or when you want to defer the execution of data generation until it is needed." }, { "title": "C# | Using yield with Entity Framework", "url": "/posts/C-Using-yield-with-Entity-Framework/", "categories": "C#, Best Practices", "tags": "microsoft, csharp, c#, bestpractices", "date": "2023-07-07 00:00:00 +0200", "snippet": "Using yield in C# with Entity FrameworkIn C#, the yield keyword is used to create an iterator. It allows you to return a sequence of values one at a time, which is particularly useful when working ...", "content": "Using yield in C# with Entity FrameworkIn C#, the yield keyword is used to create an iterator. It allows you to return a sequence of values one at a time, which is particularly useful when working with large data sets or when you want to generate values lazily. In this guide, we’ll explore how to use yield with Entity Framework to retrieve and manipulate data efficiently.Understanding yieldThe yield keyword is often used in C# when defining an iterator method. It allows you to return a sequence of values without having to load the entire set into memory at once. Instead, it generates each value on-the-fly as requested. An iterator method uses the yield return statement to produce each item in the sequence. The calling code can iterate over the sequence using a foreach loop or other enumeration methods. The method’s state is preserved between calls, so it continues where it left off.Using yield with Entity FrameworkEntity Framework is an Object-Relational Mapping (ORM) framework that allows you to work with databases using C#. You can combine yield with Entity Framework to efficiently retrieve and process data from a database.Here’s how to use yield with Entity Framework: Create an Entity Framework Data Context: Define an Entity Framework data context that connects to your database. Define a Query Method: Create a method that returns an IEnumerable&lt;T&gt; using the yield keyword. This method will represent your query. Use the Query Method: Call the query method to retrieve data. Since it uses yield, the data will be streamed one item at a time, reducing memory usage. Example: Retrieving Data with yield and Entity FrameworkLet’s see an example of how to use yield with Entity Framework to retrieve a list of products from a database.public class Product{ public int ProductId { get; set; } public string Name { get; set; } public decimal Price { get; set; }}public class MyDbContext : DbContext{ public DbSet&lt;Product&gt; Products { get; set; }}public class ProductRepository{ private readonly MyDbContext dbContext; public ProductRepository(MyDbContext context) { dbContext = context; } public IEnumerable&lt;Product&gt; GetProducts() { foreach (var product in dbContext.Products) { yield return product; } }}In this example, the GetProducts method uses yield to stream the products from the database one at a time, reducing memory consumption.What Next?Using the yield keyword with Entity Framework can help you efficiently work with large data sets from a database by streaming data one item at a time. This approach can lead to improved performance and reduced memory usage when working with data in C# applications." }, { "title": "Azure devops | Creating free account", "url": "/posts/Creating-free-account/", "categories": "Azure, Azure devops", "tags": "microsoft, azuredevops", "date": "2023-07-05 00:00:00 +0200", "snippet": "Create Azure devops free accountAs of my last knowledge update in January 2022, Azure DevOps Services, which provides a set of development tools, including version control, build automation, releas...", "content": "Create Azure devops free accountAs of my last knowledge update in January 2022, Azure DevOps Services, which provides a set of development tools, including version control, build automation, release management, and more, offers a free tier that you can use to get started. Follow below steps in order to create a free Azure DevOps account.1. Visit the Azure DevOps Services Website: Open your web browser and go to the Azure DevOps Services website at https://azure.com/devops.2. Sign Up for Azure DevOps: Click on the “Get started for free” or “Start free” button on the homepage. You’ll be prompted to sign in with your Microsoft account (formerly known as Live ID). If you don’t have one, you can create a new Microsoft account. Follow the sign-up process, which may include verifying your email and providing some basic information. 3. Set Up Your Organization: Once you’ve signed in, you’ll need to create an organization. An organization in Azure DevOps is a way to group your projects and manage access. Provide a unique name for your organization, and choose your region and time zone. Click “Continue.” 4. Choose a Free Plan: Azure DevOps offers a free plan that includes up to 5 users and unlimited stakeholders. You can select this free plan during the setup process. You can also choose to start a free trial of Azure DevOps, which may include additional features beyond the free tier. 5. Configure Additional Settings: Follow the setup wizard to configure additional settings for your organization. This may include choosing a project management process (Agile, Scrum, etc.), and you can also set up repositories, pipelines, boards, and other services.6. Start Using Azure DevOps: After completing the setup, you’ll be directed to your Azure DevOps organization. From there, you can create projects, repositories, and start managing your software development process.What Next? Please note that while Azure DevOps offers a free tier, there may be some limitations on the number of users and resources in the free plan. Be sure to check the Azure DevOps pricing and documentation for the most up-to-date information on what’s included in the free tier.Keep in mind that the process or offerings might have changed since my last update in January 2022, so I recommend visiting the Azure DevOps website for the most current information and to set up your free account." }, { "title": "C# | Best Practices", "url": "/posts/C-Best-Practices/", "categories": "C#, Best Practices", "tags": "microsoft, csharp, c#, bestpractices, tips&tricks", "date": "2023-07-02 00:00:00 +0200", "snippet": "C# Best PracticesThese best practices are designed to help you write clean, efficient, and maintainable C# code.1. Follow Naming Conventions Use PascalCase for class names, method names, and prope...", "content": "C# Best PracticesThese best practices are designed to help you write clean, efficient, and maintainable C# code.1. Follow Naming Conventions Use PascalCase for class names, method names, and properties (e.g., MyClass, MyMethod, MyProperty). Use camelCase for local variables and method parameters (e.g., myVariable, myParameter). Use ALL_CAPS for constants (e.g., MY_CONSTANT).2. Use Meaningful Names Choose descriptive and meaningful names for your variables, classes, and methods. Avoid abbreviations and single-letter variable names unless they are widely accepted (e.g., i, j, k for loop counters).3. Organize Your Code Use regions and comments to clearly structure your code into logical sections. Organize your files into namespaces that reflect the functionality of your code.4. Follow the DRY Principle (Don’t Repeat Yourself) Refactor code to eliminate duplication. If you find the same code in multiple places, create a reusable method or class.5. Use Exception Handling Wisely Only catch exceptions when you can handle them appropriately. Use specific exception types rather than catching Exception for better error handling.6. Use Code Documentation Document your code using XML comments for classes, methods, and properties. Provide clear and concise explanations of what the code does and how to use it.7. Keep Methods Small and Focused Aim for methods that do one thing and do it well. If a method is too long, consider breaking it into smaller, more focused methods.8. Use Dependency Injection Favor dependency injection over hardcoding dependencies in your classes. Use interfaces to define contracts and make your code more testable.9. Write Unit Tests Create unit tests for your code to ensure it functions as expected. Use a testing framework like MSTest, NUnit, or xUnit.10. Use Source Control Use a version control system like Git to track changes to your code. Commit and push code regularly to ensure a history of changes.11. Optimize Performance Profile your code to identify performance bottlenecks. Use appropriate data structures and algorithms for efficient processing.12. Keep an Eye on Security Avoid raw SQL queries and use parameterized queries to prevent SQL injection. Validate and sanitize user inputs to protect against security vulnerabilities.13. Follow SOLID Principles Strive to adhere to the SOLID principles: Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, and Dependency Inversion.14. Review Code Conduct code reviews with peers to catch issues early and share knowledge. Use code analysis tools and linters to automate code review processes.15. Stay Up-to-Date Keep up with the latest C# features and best practices by reading blogs, books, and attending conferences.What Next? By following these best practices, you can write C# code that is easier to read, maintain, and extend.Remember that good coding practices evolve, so always be open to learning and adapting to new techniques and tools." }, { "title": "C# | Asynchronous programming with [async | await | Task]", "url": "/posts/C-Asynchronous-programming-with-async-await-and-Task/", "categories": "C#, Best Practices", "tags": "microsoft, csharp, c#, bestpractices", "date": "2023-07-01 00:00:00 +0200", "snippet": "Using await and async Task in C#In C#, asynchronous programming is used to improve the responsiveness of applications by allowing tasks to run concurrently without blocking the main thread. The awa...", "content": "Using await and async Task in C#In C#, asynchronous programming is used to improve the responsiveness of applications by allowing tasks to run concurrently without blocking the main thread. The await and async keywords play a crucial role in achieving this. This guide will show you how to use await and async Task effectively.IntroductionAsynchronous programming in C# is essential for tasks that may take a significant amount of time, such as I/O-bound operations or network requests. By using await and async, you can ensure that your application remains responsive while waiting for these tasks to complete.Using async Task Define an async Method: To use await, define an async method within a class, typically returning a Task or Task&lt;T&gt;. public async Task MyAsyncMethod(){ // Asynchronous code here} Call the async Method: In another method, you can call your async method with the await keyword. await MyAsyncMethod(); The await KeywordThe await keyword is used within an async method to pause execution until the awaited task completes. It allows the calling thread to continue other work without blocking.async Task MyAsyncMethod(){ var result = await SomeAsyncTask(); // Code after the await will execute when SomeAsyncTask is completed.}Exception HandlingTo handle exceptions in asynchronous methods, you can use standard try-catch blocks. When an exception is thrown in an async method, it’s captured and propagated as part of the Task.try{ await SomeAsyncMethod();}catch (Exception ex){ // Handle the exception}Cancellation TokensTo cancel an asynchronous operation, you can use CancellationToken. Pass a CancellationToken to the asynchronous method, and check for cancellation within the method.async Task MyAsyncMethod(CancellationToken cancellationToken){ // Check for cancellation cancellationToken.ThrowIfCancellationRequested(); // Continue with the operation}Real-world ExampleHere’s an example of a common real-world scenario: making an HTTP request asynchronously.public async Task&lt;string&gt; FetchDataAsync(string url){ using (var httpClient = new HttpClient()) { var response = await httpClient.GetAsync(url); response.EnsureSuccessStatusCode(); return await response.Content.ReadAsStringAsync(); }}What Next?Using await and async Task in C# allows you to write responsive and efficient applications, especially when dealing with I/O-bound or long-running tasks. It enables you to keep your application responsive and improve the overall user experience by allowing multiple tasks to run concurrently without blocking the main thread." }, { "title": "C# | Web Api's Tips and Tricks", "url": "/posts/C-Web-Api's-Tips-and-Tricks/", "categories": "C#, Tips And Tricks", "tags": "microsoft, csharp, c#, webapi, tips&tricks", "date": "2023-06-22 00:00:00 +0200", "snippet": "Web Api’s Tips and TricksBuilding Web APIs in C# is a powerful way to create scalable and efficient backend services. Here are some tips and tricks to help you get the most out of your C# Web API d...", "content": "Web Api’s Tips and TricksBuilding Web APIs in C# is a powerful way to create scalable and efficient backend services. Here are some tips and tricks to help you get the most out of your C# Web API development.1. Use ASP.NET Core Start with ASP.NET Core for building Web APIs. It’s a cross-platform, high-performance framework with excellent support for RESTful services.2. RESTful Routes Follow RESTful principles when designing your API endpoints. Use HTTP methods (GET, POST, PUT, DELETE) for CRUD operations and use nouns to represent resources.3. Model Validation Leverage model validation attributes (e.g., [Required], [MaxLength], [RegularExpression]) to validate incoming data, ensuring data integrity and security.4. Versioning Consider versioning your API from the beginning to maintain backward compatibility as your API evolves. You can use URL versioning, header versioning, or content negotiation for versioning.5. Use DTOs Data Transfer Objects (DTOs) are essential for decoupling your API from your database models. They allow you to control what data is exposed and simplify data transformation.6. Dependency Injection Leverage ASP.NET Core’s built-in dependency injection to manage the lifecycle of your services. This promotes loose coupling and testability.7. Middleware ASP.NET Core middleware allows you to insert custom processing logic into the request/response pipeline. You can use it for tasks like authentication, logging, and exception handling.8. Authentication and Authorization Implement secure authentication and authorization mechanisms, such as JWT (JSON Web Tokens) or OAuth, to protect your API endpoints.9. Pagination For endpoints that return large datasets, implement pagination to improve performance and usability. Use query parameters like page and pageSize to control data retrieval.10. Logging and Error Handling Set up comprehensive logging to track API usage and errors. Implement global exception handling to provide meaningful error responses to clients.11. Caching Use response caching and distributed caching to reduce server load and improve response times for frequently accessed data.12. API Documentation Create clear and comprehensive API documentation using tools like Swagger or OpenAPI to help clients understand how to interact with your API.13. Testing Adopt a testing strategy that includes unit tests and integration tests to ensure the reliability and correctness of your API.14. Security Protect your API from common security threats, such as SQL injection and cross-site scripting (XSS), by validating and sanitizing user inputs.15. Performance Optimization Optimize your API for performance by using techniques like asynchronous programming, minimizing database queries, and reducing unnecessary data transfer.16. Rate Limiting Implement rate limiting to prevent abuse of your API by limiting the number of requests a client can make in a given time frame.17. Continuous Integration and Deployment (CI/CD) Set up CI/CD pipelines to automate the build, testing, and deployment of your Web API, ensuring a smooth release process." }, { "title": "C# | Common Errors", "url": "/posts/C-Common-Errors/", "categories": "C#, Best Practices", "tags": "microsoft, csharp, c#, bestpractices", "date": "2023-06-18 00:00:00 +0200", "snippet": "Common Errors in C#C# is a powerful programming language, but like any language, it has its share of common errors that developers may encounter. Understanding these errors and their solutions can ...", "content": "Common Errors in C#C# is a powerful programming language, but like any language, it has its share of common errors that developers may encounter. Understanding these errors and their solutions can help improve your coding skills and productivity.NullReferenceExceptionDescription: This error occurs when you try to access a member (method or property) of an object that is currently set to null.Common Causes: Accessing an uninitialized object. Accessing a property or method of an object after it has been set to null. Solution: Ensure that the object is properly initialized before accessing its members. Use null checks (if (obj != null)) or use the null-conditional operator (obj?.Method()).IndexOutOfRangeExceptionDescription: This error occurs when you attempt to access an element of an array or collection using an index that is out of its bounds.Common Causes: Accessing an array or collection with an index that is too large or too small. Solution: Check the length of the array or collection before accessing elements and make sure the index is within the valid range.ArgumentExceptionDescription: This error is thrown when an argument provided to a method is not valid.Common Causes: Passing invalid or unexpected arguments to a method. Using incorrect argument types or values. Solution: Ensure that you are passing valid arguments to methods. Read documentation and method signatures to understand the expected arguments.FileNotFoundExceptionDescription: This error is raised when an attempt to access a file fails because the specified file does not exist.Common Causes: Providing an incorrect or non-existent file path. Solution: Verify that the file exists at the specified path or handle the exception to provide appropriate feedback to the user.Syntax ErrorsDescription: Syntax errors occur when your code does not conform to the C# language syntax rules.Common Causes: Mismatched parentheses, brackets, or curly braces. Misspelled keywords or identifiers. Incorrect use of operators. Solution: Carefully review the code and correct the syntax errors indicated by the compiler.Unhandled ExceptionsDescription: Unhandled exceptions cause the application to crash when they are not properly caught and handled in your code.Common Causes: Failing to use try-catch blocks to handle exceptions. Not anticipating and handling specific exceptions that can occur in your code. Solution: Use try-catch blocks to catch and handle exceptions or use higher-level exception handling mechanisms to gracefully handle errors.Resource LeaksDescription: Resource leaks occur when you do not properly release resources like file handles, database connections, or memory.Common Causes: Failing to close or dispose of resources when they are no longer needed. Not using using statements for disposable objects. Solution: Always release resources explicitly or use using statements to ensure resources are properly cleaned up.What Next?Understanding and addressing these common C# errors will help you write more robust and reliable code. Learning to diagnose and fix errors is an essential skill for any C# developer." }, { "title": "C# | Tips and tricks", "url": "/posts/C-Tips-and-tricks/", "categories": "C#, Tips And Tricks", "tags": "microsoft, csharp, c#, bestpractices, tips&tricks", "date": "2023-06-16 00:00:00 +0200", "snippet": "C# tips and tricksC# is a versatile programming language that offers many features and techniques to make your coding more efficient and maintainable. In this document, we’ll explore some useful ti...", "content": "C# tips and tricksC# is a versatile programming language that offers many features and techniques to make your coding more efficient and maintainable. In this document, we’ll explore some useful tips and tricks for C# development.1. String InterpolationString interpolation allows you to embed expressions directly within string literals. It’s a cleaner and more readable way to concatenate strings and variables.string name = \"Hassan\";int age = 35;string message = $\"Hello, {name}! You are {age} years old.\";2. Null Conditional OperatorThe null-conditional operator (?.) simplifies null checks, making your code more concise and less error-prone.int? length = text?.Length;3. DeconstructionDeconstruction allows you to assign values from a tuple or an object to separate variables in a single line.var (x, y) = GetCoordinates();4. Pattern MatchingPattern matching simplifies conditional statements by checking for specific patterns in data, making your code more readable.if (obj is int number){ // Use 'number' as an int}5. Local FunctionsLocal functions are functions defined within another method, making your code more modular and improving encapsulation.int Calculate(int a, int b){ int Add(int x, int y) =&gt; x + y; return Add(a, b);}6. LINQ (Language Integrated Query)LINQ allows for elegant and efficient querying of collections and databases.var result = from person in people where person.Age &gt; 35 select person.Name;7. Ternary OperatorThe ternary operator is a concise way to write simple conditional expressions.string result = (condition) ? \"True\" : \"False\";8. Using StatementThe using statement simplifies resource management, ensuring that disposable objects are properly disposed of when no longer needed.using (var stream = new FileStream(\"file.txt\", FileMode.Open)){ // Work with the file stream}9. Async/AwaitAsync and await make asynchronous programming more readable and maintainable.async Task&lt;string&gt; DownloadAsync(string url){ var data = await DownloadDataAsync(url); return Encoding.UTF8.GetString(data);}10. Extension MethodsYou can add new methods to existing types using extension methods, enhancing code reusability.public static class StringExtensions{ public static bool IsNullOrEmpty(this string value) { return string.IsNullOrEmpty(value); }}What Next? These are just a few of the many tips and tricks that can help you become a more proficient C# developer. As you continue to work with C#, explore its vast ecosystem to improve your skills and productivity." }, { "title": "C# | Entity Framework Issues and Troubleshooting", "url": "/posts/C-Entity-Framework-Issues-and-Troubleshooting/", "categories": "C#, Entity Framework", "tags": "microsoft, csharp, c#, entityframework, bestpractices", "date": "2023-06-11 00:00:00 +0200", "snippet": "Entity Framework Issues and TroubleshootingEntity Framework is a powerful ORM (Object-Relational Mapping) framework for C# applications, but it can sometimes lead to issues that developers need to ...", "content": "Entity Framework Issues and TroubleshootingEntity Framework is a powerful ORM (Object-Relational Mapping) framework for C# applications, but it can sometimes lead to issues that developers need to address. Here are some common Entity Framework issues and tips on how to troubleshoot them.1. Connection and Configuration Issues1.1 Unable to Connect to Database Issue: Entity Framework cannot establish a connection to the database. Troubleshooting: Check the connection string in your appsettings.json or web.config file. Ensure the database server is running. Verify your database server’s firewall rules. 1.2 Configuration Problems Issue: Incorrect configuration of Entity Framework. Troubleshooting: Double-check your OnConfiguring method in the DbContext class. Verify that the Entity Framework version matches your project. 2. Query and Performance Issues2.1 Slow Queries Issue: Queries executed by Entity Framework are slow. Troubleshooting: Use SQL Server Profiler to analyze generated SQL queries. Consider using indexing on database columns. Optimize your LINQ queries for better performance. 2.2 N+1 Query Problem Issue: The N+1 query problem occurs when Entity Framework generates too many SQL queries. Troubleshooting: Use eager loading with .Include() or .ThenInclude() to fetch related data. Avoid lazy loading when it’s not necessary. 3. Data Migrations and Schema Changes3.1 Migration Failures Issue: Data migration or schema update fails. Troubleshooting: Check the migration script for errors. Ensure that the data model matches the database schema. 3.2 Model Changes Not Reflected Issue: Changes to your data model are not reflected in the database. Troubleshooting: Create a new migration using Add-Migration in the Package Manager Console. Update the database using Update-Database. 4. DbContext and Entity State Management4.1 DbContext Lifetime Issue: DbContext lifetime management problems. Troubleshooting: Use a scoped or request-specific DbContext for web applications. Dispose of DbContext instances properly. 4.2 Entity State Not Updating Issue: Entity state is not being updated after changes. Troubleshooting: Call SaveChanges() after modifying entities. Check for validation errors that may prevent changes from being saved. 5. Miscellaneous Issues5.1 Exception Handling Issue: Unhandled exceptions during Entity Framework operations. Troubleshooting: Implement exception handling in your code to gracefully handle errors.5.2 Asynchronous Code Issue: Incorrect use of asynchronous code with Entity Framework. Troubleshooting: Ensure you are using asynchronous methods properly, and don’t mix synchronous and asynchronous calls.What Next?Remember that Entity Framework issues can vary depending on the project, database, and configuration. When facing problems, check documentation, online forums, and communities for more specific guidance.This guide should help you identify and troubleshoot common issues when working with Entity Framework in C# applications. If you encounter any other issues, don’t hesitate to seek assistance from the Entity Framework community or forums." }, { "title": "SQL | Database Query Optimization with Examples", "url": "/posts/SQL-Database-Query-Optimization-with-Examples/", "categories": "SQL, Optimization", "tags": "microsoft, sql, database, optimization", "date": "2023-06-03 00:00:00 +0200", "snippet": "Database Query Optimization with ExamplesDatabase query optimization is crucial for improving the performance of your application and reducing the response time of database queries. In this guide, ...", "content": "Database Query Optimization with ExamplesDatabase query optimization is crucial for improving the performance of your application and reducing the response time of database queries. In this guide, we’ll explore various optimization techniques with practical examples.IndexingIndexes help the database engine locate and retrieve rows from tables more efficiently. Using indexes can significantly speed up queries.Example:-- Create an index on the 'email' column of the 'users' tableCREATE INDEX idx_users_email ON users(email);-- Query using the indexed columnSELECT * FROM users WHERE email = 'user@example.com';Query RewritingRewriting queries can lead to more optimized execution plans, reducing the query’s execution time.Example:-- Original querySELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-12-31';-- Optimized query using inequalitiesSELECT * FROM orders WHERE order_date &gt;= '2023-01-01' AND order_date &lt;= '2023-12-31';Avoiding SELECT *Avoiding SELECT * and specifying only the necessary columns reduces the amount of data transferred from the database to the application, improving query performance.Example:-- Avoid SELECT *SELECT order_id, order_date, total_amount FROM orders WHERE customer_id = 123;Use of LIMIT and OFFSETWhen you don’t need to retrieve all matching rows, using LIMIT and OFFSET can reduce the amount of data processed and improve query performance.Example:-- Retrieve 10 results starting from the 20th rowSELECT * FROM products LIMIT 10 OFFSET 20;NormalizationNormalization is the process of organizing data in a database to eliminate redundancy and improve data integrity. It can help reduce the size of the data and optimize query performance.Example:Consider a denormalized table:-- Denormalized tableCREATE TABLE orders ( order_id INT PRIMARY KEY, customer_name VARCHAR(255), product_name VARCHAR(255), total_amount DECIMAL);Normalize the table to separate customers and products:-- Customers tableCREATE TABLE customers ( customer_id INT PRIMARY KEY, customer_name VARCHAR(255));-- Products tableCREATE TABLE products ( product_id INT PRIMARY KEY, product_name VARCHAR(255));-- Normalized orders tableCREATE TABLE orders ( order_id INT PRIMARY KEY, customer_id INT, product_id INT, total_amount DECIMAL);What Next?By following normalization principles, you can improve data integrity and reduce data redundancy.Optimizing database queries is an ongoing process that depends on the specific database system and application requirements. These techniques and examples provide a foundation for improving query performance, but it’s essential to analyze query execution plans and monitor database performance to achieve the best results." }, { "title": "Kubernetes | Helm Commands with YAML File Examples", "url": "/posts/Kubernetes-Helm-Commands-with-YAML-File-Examples/", "categories": "Kubernetes, Helm", "tags": "microsoft, kubernetes, helm, k8s", "date": "2023-05-25 00:00:00 +0200", "snippet": "Helm Commands with YAML File ExamplesHelm is a powerful package manager for Kubernetes that simplifies the deployment and management of containerized applications. In this guide, we’ll explore some...", "content": "Helm Commands with YAML File ExamplesHelm is a powerful package manager for Kubernetes that simplifies the deployment and management of containerized applications. In this guide, we’ll explore some common Helm commands and provide detailed examples of Helm Chart YAML files.Helm CommandsInitialize a Helm ChartTo create a new Helm chart, you can use the following command:helm create mychartThis command generates the necessary directory structure and files for your chart.Installing a ChartTo install a Helm chart, you can use the following command:helm install my-release ./mychartHere, my-release is the name you give to the release, and ./mychart is the path to your Helm chart.Upgrading a ChartTo upgrade a Helm release, you can use the following command:helm upgrade my-release ./mychartThis command is used to apply changes to a deployed release.Uninstalling a ChartTo uninstall a Helm release, you can use the following command:helm uninstall my-releaseThis command removes the release and associated resources.Helm Chart YAML ExamplesChart.yamlThe Chart.yaml file provides metadata about your Helm chart. Here’s an example:apiVersion: v2name: mychartdescription: A Helm chart for my applicationversion: 0.1.0appVersion: 1.0.0values.yamlThe values.yaml file contains configuration values for your Helm chart. Here’s an example:replicaCount: 1image: repository: nginx tag: stable pullPolicy: IfNotPresentservice: name: mychart-service type: ClusterIP port: 80Deployment.yamlThe Deployment.yaml file is part of your Helm chart’s templates and defines a Kubernetes Deployment. Here’s an example:apiVersion: apps/v1kind: Deploymentmetadata: name: spec: replicas: template: spec: containers: - name: image: \":\" ports: - containerPort: 80Service.yamlThe Service.yaml file defines a Kubernetes Service for your application. Here’s an example:apiVersion: v1kind: Servicemetadata: name: spec: selector: app: ports: - port: targetPort: 80What Next?These are just a few examples of Helm commands and YAML files used in Helm charts. Helm makes it easier to package, deploy, and manage Kubernetes applications, allowing you to define and version your application configurations in a structured way." }, { "title": "SQL | Database Query Optimization with Common Table Expressions", "url": "/posts/SQL-Database-Query-Optimization-with-Common-Table-Expressions/", "categories": "SQL, Optimization", "tags": "microsoft, sql, database, optimization, cte", "date": "2023-05-18 00:00:00 +0200", "snippet": "Database Query Optimization with Common Table Expressions (CTE)Common Table Expressions (CTEs) are a valuable tool for optimizing database queries, particularly when dealing with complex queries or...", "content": "Database Query Optimization with Common Table Expressions (CTE)Common Table Expressions (CTEs) are a valuable tool for optimizing database queries, particularly when dealing with complex queries or large datasets. They allow you to break down your query into smaller, more manageable parts. In this guide, we’ll explore how to use CTEs for query optimization with examples.What is a Common Table Expression (CTE)?A Common Table Expression is a temporary result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. CTEs help improve query readability and maintainability by breaking down complex queries into smaller logical units.Query Optimization with CTEsCTEs can significantly enhance query performance by allowing the database optimizer to better understand and optimize your query. They can eliminate redundancy and make your SQL code more elegant.To optimize your queries with CTEs: Use them for recursive queries. Organize and structure your SQL code for clarity. Reduce duplicated subqueries.ExamplesExample 1: Recursive CTERecursive CTEs are particularly useful when working with hierarchical data structures, such as organizational charts or comment threads.Suppose you have a table named Employee with columns EmployeeID and ManagerID. You can use a CTE to retrieve all employees reporting to a specific manager, including their subordinates.WITH RecursiveEmployeeCTE AS ( SELECT EmployeeID, EmployeeName, ManagerID FROM Employee WHERE EmployeeID = @ManagerID UNION ALL SELECT e.EmployeeID, e.EmployeeName, e.ManagerID FROM Employee AS e INNER JOIN RecursiveEmployeeCTE AS r ON e.ManagerID = r.EmployeeID)SELECT EmployeeID, EmployeeNameFROM RecursiveEmployeeCTE;Example 2: Hierarchical DataConsider a table named Category with columns CategoryID and ParentCategoryID. You can use a CTE to retrieve all categories in a hierarchical structure.WITH RecursiveCategoryCTE AS ( SELECT CategoryID, CategoryName, ParentCategoryID FROM Category WHERE ParentCategoryID IS NULL UNION ALL SELECT c.CategoryID, c.CategoryName, c.ParentCategoryID FROM Category AS c INNER JOIN RecursiveCategoryCTE AS r ON c.ParentCategoryID = r.CategoryID)SELECT CategoryID, CategoryNameFROM RecursiveCategoryCTE;What Next?Common Table Expressions (CTEs) are a powerful tool for database query optimization. They enhance the readability and maintainability of your SQL code while improving performance, especially in cases involving recursive or hierarchical data. By breaking down complex queries into smaller, manageable units, CTEs can make your database queries more efficient and elegant." } ]
